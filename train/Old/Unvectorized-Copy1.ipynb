{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unexpected-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import YouCookII\n",
    "from dataset import YouCookIICollate\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from model import Model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loaded-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advisory-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = '[unused3]'\n",
    "\n",
    "lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [ACTION]})\n",
    "lxmert_tokenizer.encode([ACTION])\n",
    "\n",
    "lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "lxmert.to(device)\n",
    "\n",
    "ACTION_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(ACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "noticed-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 8\n",
    "MAX_DETECTIONS=20\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DETECTION_EMBEDDING_SIZE = 2048\n",
    "OUTPUT_EMBEDDING_SIZE = 768\n",
    "NUM_FRAMES_PER_STEP=5\n",
    "MAX_DETECTIONS=20\n",
    "CANDIDATES = NUM_FRAMES_PER_STEP * MAX_DETECTIONS\n",
    "\n",
    "dataset = YouCookII(NUM_ACTIONS, \"/h/sagar/ece496-capstone/datasets/ycii\")\n",
    "collate = YouCookIICollate(MAX_DETECTIONS=MAX_DETECTIONS)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minor-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(device, MAX_DETECTIONS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, boxes, features, steps, entities, entity_count, _, _ = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dynamic-receiver",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.0554)\n",
      "Loss: 1411.52197265625, Accuracy: 0.4489795918367347\n",
      "tensor(-1.0769)\n",
      "Loss: 1218.94677734375, Accuracy: 0.5523809523809524\n",
      "tensor(4.6980)\n",
      "Loss: 1309.8203125, Accuracy: 0.45918367346938777\n",
      "tensor(1.7204)\n",
      "Loss: 1305.0159912109375, Accuracy: 0.5142857142857142\n",
      "tensor(14.7055)\n",
      "Loss: 1380.8665771484375, Accuracy: 0.4857142857142857\n",
      "tensor(7.7888)\n",
      "Loss: 1084.072509765625, Accuracy: 0.5803571428571429\n",
      "tensor(5.3244)\n",
      "Loss: 1290.072998046875, Accuracy: 0.45918367346938777\n",
      "tensor(-2.6532)\n",
      "Loss: 1280.8931884765625, Accuracy: 0.5523809523809524\n",
      "tensor(15.9109)\n",
      "Loss: 1337.2987060546875, Accuracy: 0.43956043956043955\n",
      "tensor(6.6066)\n",
      "Loss: 1459.7196044921875, Accuracy: 0.36507936507936506\n",
      "tensor(0.7305)\n",
      "Loss: 1435.6927490234375, Accuracy: 0.48739495798319327\n",
      "tensor(6.8205)\n",
      "Loss: 1409.686767578125, Accuracy: 0.3673469387755102\n",
      "tensor(14.0222)\n",
      "Loss: 1453.2303466796875, Accuracy: 0.45714285714285713\n",
      "tensor(6.5387)\n",
      "Loss: 1272.58984375, Accuracy: 0.5\n",
      "tensor(11.9544)\n",
      "Loss: 1462.01123046875, Accuracy: 0.4642857142857143\n",
      "tensor(16.6300)\n",
      "Loss: 1284.8497314453125, Accuracy: 0.6373626373626373\n",
      "tensor(3.1920)\n",
      "Loss: 1149.4879150390625, Accuracy: 0.5142857142857142\n",
      "tensor(7.1702)\n",
      "Loss: 1473.3348388671875, Accuracy: 0.4380952380952381\n",
      "tensor(1.6539)\n",
      "Loss: 1450.5374755859375, Accuracy: 0.36363636363636365\n",
      "tensor(17.4796)\n",
      "Loss: 1798.866943359375, Accuracy: 0.40714285714285714\n",
      "tensor(1.8695)\n",
      "Loss: 1137.581298828125, Accuracy: 0.6142857142857143\n",
      "tensor(11.6127)\n",
      "Loss: 1366.2352294921875, Accuracy: 0.5\n",
      "tensor(6.2466)\n",
      "Loss: 1361.1143798828125, Accuracy: 0.45714285714285713\n",
      "tensor(4.8167)\n",
      "Loss: 1159.6068115234375, Accuracy: 0.5918367346938775\n",
      "tensor(4.0358)\n",
      "Loss: 1226.0396728515625, Accuracy: 0.49206349206349204\n",
      "tensor(4.9260)\n",
      "Loss: 1251.953857421875, Accuracy: 0.5164835164835165\n",
      "tensor(6.3592)\n",
      "Loss: 1530.9920654296875, Accuracy: 0.49107142857142855\n",
      "tensor(14.5441)\n",
      "Loss: 1525.1766357421875, Accuracy: 0.3116883116883117\n",
      "tensor(12.3848)\n",
      "Loss: 1164.7073974609375, Accuracy: 0.6134453781512605\n",
      "tensor(-0.2136)\n",
      "Loss: 1160.85107421875, Accuracy: 0.6190476190476191\n",
      "tensor(6.7438)\n",
      "Loss: 1318.6837158203125, Accuracy: 0.5238095238095238\n",
      "tensor(13.0596)\n",
      "Loss: 1358.7542724609375, Accuracy: 0.4857142857142857\n",
      "tensor(8.4791)\n",
      "Loss: 1517.7906494140625, Accuracy: 0.43609022556390975\n",
      "tensor(-0.1919)\n",
      "Loss: 1239.03125, Accuracy: 0.5824175824175825\n",
      "tensor(5.4923)\n",
      "Loss: 1608.4207763671875, Accuracy: 0.25892857142857145\n",
      "tensor(9.6640)\n",
      "Loss: 1304.871337890625, Accuracy: 0.4957983193277311\n",
      "tensor(8.0742)\n",
      "Loss: 1429.71240234375, Accuracy: 0.42857142857142855\n",
      "tensor(4.7419)\n",
      "Loss: 1248.1943359375, Accuracy: 0.5164835164835165\n",
      "tensor(5.1157)\n",
      "Loss: 1388.4715576171875, Accuracy: 0.4523809523809524\n",
      "tensor(10.3826)\n",
      "Loss: 1317.3515625, Accuracy: 0.4957983193277311\n",
      "tensor(14.5721)\n",
      "Loss: 1272.8775634765625, Accuracy: 0.5164835164835165\n",
      "tensor(11.7830)\n",
      "Loss: 1277.3624267578125, Accuracy: 0.49206349206349204\n",
      "tensor(28.4830)\n",
      "Loss: 1226.007568359375, Accuracy: 0.5238095238095238\n",
      "tensor(9.4277)\n",
      "Loss: 1406.3734130859375, Accuracy: 0.4523809523809524\n",
      "tensor(3.8926)\n",
      "Loss: 1470.6541748046875, Accuracy: 0.5064935064935064\n",
      "tensor(5.0061)\n",
      "Loss: 1652.2293701171875, Accuracy: 0.42016806722689076\n",
      "tensor(1.7801)\n",
      "Loss: 1398.55126953125, Accuracy: 0.4126984126984127\n",
      "tensor(2.4108)\n",
      "Loss: 1186.3646240234375, Accuracy: 0.5714285714285714\n",
      "tensor(2.9297)\n",
      "Loss: 1296.5693359375, Accuracy: 0.5178571428571429\n",
      "tensor(5.3268)\n",
      "Loss: 1355.3626708984375, Accuracy: 0.35714285714285715\n",
      "tensor(4.8919)\n",
      "Loss: 1293.990966796875, Accuracy: 0.5238095238095238\n",
      "tensor(8.9574)\n",
      "Loss: 1189.09130859375, Accuracy: 0.6493506493506493\n",
      "tensor(1.7432)\n",
      "Loss: 1335.8414306640625, Accuracy: 0.5306122448979592\n",
      "tensor(-2.7329)\n",
      "Loss: 1368.8507080078125, Accuracy: 0.453781512605042\n",
      "tensor(14.6691)\n",
      "Loss: 1450.6243896484375, Accuracy: 0.49107142857142855\n",
      "tensor(6.4525)\n",
      "Loss: 1356.725830078125, Accuracy: 0.44047619047619047\n",
      "tensor(7.0723)\n",
      "Loss: 1382.2440185546875, Accuracy: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for data in dataloader:\n",
    "        _, boxes, features, steps, entities, entity_count, _, _ = data\n",
    "        loss_data, VG, RR = model(BATCH_SIZE, NUM_ACTIONS, steps, features, boxes, entities, entity_count)\n",
    "        \n",
    "        loss = compute_loss_batched(loss_data)\n",
    "        accuracy = compute_alignment_accuracy_batched(loss_data)\n",
    "        \n",
    "        print(\"Loss: {}, Accuracy: {}\".format(float(loss), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "lesbian-arctic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[76803.7812]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_alignment_loss(model, dataloader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "overall-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_loss(model, dataloader, batch_size):\n",
    "    loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            _, boxes, features, steps, entities, entity_count, _, _ = data\n",
    "            loss_data, VG, RR = model(BATCH_SIZE, NUM_ACTIONS, steps, features, boxes, entities, entity_count)\n",
    "            \n",
    "            loss = loss + compute_loss_batched(loss_data)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "meaning-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alignment_accuracy_batched(loss_data):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    alignment_scores, entity_count, BATCH_SIZE, NUM_ACTIONS, MAX_ENTITIES = loss_data\n",
    "    \n",
    "    for batch_idx in range(BATCH_SIZE):\n",
    "        _alignment_scores = alignment_scores[batch_idx]\n",
    "        _entity_count = entity_count[batch_idx]\n",
    "        \n",
    "        _total, _correct = compute_alignment_accuracy((_alignment_scores, _entity_count, NUM_ACTIONS, MAX_ENTITIES))\n",
    "        \n",
    "        total = total + _total\n",
    "        correct = correct + _correct\n",
    "            \n",
    "    return (total, correct)\n",
    "\n",
    "def compute_alignment_accuracy(loss_data):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    alignment_scores, entity_count, NUM_ACTIONS, MAX_ENTITIES = loss_data\n",
    "    \n",
    "    # l: ENTITY_ACTION_ID\n",
    "    # e: ENTITY_ID\n",
    "    # m: CANDIDATE_ACTION_ID\n",
    "    \n",
    "    for m in range(NUM_ACTIONS):\n",
    "        for e in range(entity_count[m]):\n",
    "            for l in range(NUM_ACTIONS):\n",
    "                if m == l:\n",
    "                    continue\n",
    "                    \n",
    "                aligned = compute_alignment(alignment_scores[m, e, m, :], alignment_scores[m, e, l, :])\n",
    "                \n",
    "                if aligned:\n",
    "                    correct = correct + 1\n",
    "                    \n",
    "                total = total + 1\n",
    "                \n",
    "    return total, correct\n",
    "    \n",
    "def compute_alignment(score_m, score_l):\n",
    "    '''\n",
    "        score_m: score between entity from STEP M and candidates from STEP M\n",
    "        score_l: score between entity from STEP M and candidates from STEP L\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    m_max = score_m.max()\n",
    "    l_max = score_l.max()\n",
    "    \n",
    "    return (m_max > l_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "logical-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_batched(loss_data, margin=10):\n",
    "    loss = 0.\n",
    "    \n",
    "    alignment_scores, entity_count, BATCH_SIZE, NUM_ACTIONS, MAX_ENTITIES = loss_data\n",
    "    \n",
    "    for batch_idx in range(BATCH_SIZE):\n",
    "        _alignment_scores = alignment_scores[batch_idx]\n",
    "        _entity_count = entity_count[batch_idx]\n",
    "        \n",
    "        loss = loss + compute_loss((_alignment_scores, _entity_count, NUM_ACTIONS, MAX_ENTITIES), margin)\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def compute_loss(loss_data, margin):    \n",
    "    alignment_scores, entity_count, NUM_ACTIONS, MAX_ENTITIES = loss_data\n",
    "    \n",
    "    # Recall the shape of the alignment scores tensor:\n",
    "    # ENTITY_ACTION_ID * ENTITY * CANDIDATE_ACTION_ID * CANDIDATE.\n",
    "    \n",
    "    S = torch.zeros((NUM_ACTIONS, NUM_ACTIONS))\n",
    "    zero = torch.zeros((1, 1))\n",
    "    \n",
    "    # l: ENTITY_ACTION_ID\n",
    "    # m: CANDIDATE_ACTION_ID\n",
    "    \n",
    "    for l in range(NUM_ACTIONS):\n",
    "        for m in range(NUM_ACTIONS):\n",
    "            S[l][m] = compute_S(alignment_scores[m, :, l, :], entity_count[m])\n",
    "                \n",
    "    loss = 0.\n",
    "    \n",
    "    for l in range(NUM_ACTIONS):\n",
    "        for m in range(NUM_ACTIONS):\n",
    "            loss = loss + torch.max(S[l][m] - S[l][l] + margin, zero) + torch.max(S[m][l] - S[l][l] + margin, zero)\n",
    "    \n",
    "    return loss\n",
    "        \n",
    "def compute_S(scores, entity_count):\n",
    "    '''\n",
    "        scores: Alignment scores between entities from STEP M and candidates (boxes) from step STEP L.\n",
    "    '''\n",
    "    if entity_count == 0:\n",
    "        return 0.\n",
    "    \n",
    "    # Remove padded dimension.\n",
    "    scores = scores[:entity_count]\n",
    "    \n",
    "    S = scores.max(dim=-1)[0].sum()\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-jersey",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.to(device)\n",
    "boxes = boxes.to(device)\n",
    "\n",
    "steps = remove_unused2(steps)\n",
    "\n",
    "inputs = lxmert_tokenizer(\n",
    "    steps,\n",
    "    padding=\"longest\",\n",
    "    truncation=False,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inputs.input_ids = inputs.input_ids.to(device)\n",
    "inputs.attention_mask = inputs.attention_mask.to(device)\n",
    "inputs.token_type_ids = inputs.token_type_ids.to(device)\n",
    "\n",
    "output = lxmert(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    visual_feats=features,\n",
    "    visual_pos=boxes,\n",
    "    token_type_ids=inputs.token_type_ids,\n",
    "    return_dict=True,\n",
    "    output_attentions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_idx = get_ent_inds(model, entities, steps)\n",
    "\n",
    "linguistic_embeddings = get_entity_embeddings(output['language_output'], entity_idx)\n",
    "vision_embeddings = output['vision_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sizes = torch.tensor(entity_count).flatten().tolist()\n",
    "entity_embeddings = linguistic_embeddings.split(split_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = num_actions \n",
    "\n",
    "E = pad_sequence(entity_embeddings, batch_first=True)\n",
    "E = E.reshape(-1, NUM_ACTIONS, E.shape[1], E.shape[2])\n",
    "\n",
    "max_entities = E.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = vision_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, CANDIDATES, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "VG_scores = torch.einsum('bacs, baes -> baec', V, E)\n",
    "VG_scores_max, VG_scores_index = VG_scores.max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_scores = torch.einsum('bqcs, bwes -> bweqc', V, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-spirit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
