{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import YouCookII\n",
    "from dataset import YouCookIICollate\n",
    "from torch.utils.data import DataLoader\n",
    "from loss import loss_RA_MIL\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from model import Model\n",
    "from model_FC import ModelFC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from loss import loss_RA_MIL\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = '[unused3]'\n",
    "\n",
    "lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [ACTION]})\n",
    "lxmert_tokenizer.encode([ACTION])\n",
    "\n",
    "lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "lxmert.to(device)\n",
    "\n",
    "ACTION_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(ACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 8\n",
    "MAX_DETECTIONS=20\n",
    "batch_size = 1\n",
    "\n",
    "DETECTION_EMBEDDING_SIZE = 2048\n",
    "OUTPUT_EMBEDDING_SIZE = 768\n",
    "NUM_FRAMES_PER_STEP=5\n",
    "MAX_DETECTIONS=20\n",
    "CANDIDATES = NUM_FRAMES_PER_STEP * MAX_DETECTIONS\n",
    "\n",
    "dataset = YouCookII(num_actions, \"/h/sagar/ece496-capstone/datasets/ycii\")\n",
    "collate = YouCookIICollate(MAX_DETECTIONS=MAX_DETECTIONS)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(device, MAX_DETECTIONS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, boxes, features, steps_list, entity_list, entity_count_list, _, _ = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_list = remove_unused2(steps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_count = entity_count_list[0]\n",
    "entities = entity_list[0]\n",
    "steps = steps_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [step.strip() for step in steps.split(ACTION)[:-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = boxes.squeeze(0)\n",
    "boxes = boxes.reshape(num_actions, CANDIDATES, -1)\n",
    "boxes = boxes.to(device)\n",
    "\n",
    "features = features.squeeze(0)\n",
    "features = features.reshape(num_actions, CANDIDATES, -1)\n",
    "features = features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from random import seed\n",
    "\n",
    "seed(0)\n",
    "\n",
    "steps_pairs = []\n",
    "bboxes_pairs = torch.zeros(num_actions, 2, CANDIDATES, 4)\n",
    "features_pairs = torch.zeros(num_actions, 2, CANDIDATES, DETECTION_EMBEDDING_SIZE)\n",
    "entity_list_pairs = []\n",
    "num_steps = len(steps)\n",
    "pairs = []\n",
    "\n",
    "for idx, step in enumerate(steps):\n",
    "    idx_1 = idx\n",
    "    idx_2 = choice([idx_2 for idx_2 in range(num_steps) if idx_2 != idx_1])\n",
    "    \n",
    "    pairs.append((idx_1, idx_2))\n",
    "    \n",
    "    steps_pairs.append(steps[idx_1] + \" \" + ACTION + \" \" + steps[idx_2] + \" \" + ACTION + \" \" + ACTION)\n",
    "    \n",
    "    bboxes_pairs[idx_1][0] = boxes[idx_1]\n",
    "    bboxes_pairs[idx_1][1] = boxes[idx_2]\n",
    "    \n",
    "    features_pairs[idx_1][0] = features[idx_1]\n",
    "    features_pairs[idx_1][1] = features[idx_2]\n",
    "    \n",
    "    entity_list_pairs.append([entities[idx_1], entities[idx_2]])\n",
    "    \n",
    "bboxes_pairs = bboxes_pairs.reshape(num_actions, 2 * CANDIDATES, -1)\n",
    "bboxes_pairs = bboxes_pairs.to(device)\n",
    "\n",
    "features_pairs = features_pairs.reshape(num_actions, 2 * CANDIDATES, DETECTION_EMBEDDING_SIZE)\n",
    "features_pairs = features_pairs.to(device)\n",
    "\n",
    "entity_count = [len(action) for action in sum(entity_list_pairs, [])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bboxes_pairs.shape)\n",
    "print(features_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = lxmert_tokenizer(\n",
    "            steps_pairs,\n",
    "            padding=\"longest\",\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "inputs.input_ids = inputs.input_ids.to(device)\n",
    "inputs.attention_mask = inputs.attention_mask.to(device)\n",
    "inputs.token_type_ids = inputs.token_type_ids.to(device)\n",
    "\n",
    "output = lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features_pairs,\n",
    "            visual_pos=bboxes_pairs,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_idx = get_ent_inds(model, entity_list_pairs, steps_pairs)\n",
    "entity_embeddings = get_entity_embeddings(output['language_output'], entity_idx).split(entity_count)\n",
    "\n",
    "action_idx = (inputs.input_ids == ACTION_TOKEN)\n",
    "A = output['language_output'][action_idx]\n",
    "\n",
    "V = output['vision_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = []\n",
    "\n",
    "for i in range(num_actions):\n",
    "    idx_1 = 2 * i\n",
    "    idx_2 = idx_1 + 1\n",
    "    E.append([entity_embeddings[idx_1], entity_embeddings[idx_2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "VG = []\n",
    "\n",
    "for i in range(num_actions):    \n",
    "    pair_1 = []\n",
    "    pair_2 = []\n",
    "    \n",
    "    for ent_1 in E[i][0]:\n",
    "        alignment_scores = (ent_1 * V[i][0:100]).sum(dim=-1)\n",
    "        pair_1.append(int(alignment_scores.argmax()))\n",
    "                \n",
    "    for ent_2 in E[i][1]:\n",
    "        alignment_scores = (ent_2 * V[i][100:]).sum(dim=-1)\n",
    "        pair_2.append(int(alignment_scores.argmax()))\n",
    "        \n",
    "    VG.append([pair_1, pair_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "VG_V = []\n",
    "\n",
    "for i, pair in enumerate(VG):\n",
    "    VG_V.append([])\n",
    "    for j, action in enumerate(pair):\n",
    "        VG_V[i].append([])\n",
    "        for k, entity in enumerate(action):\n",
    "            if j == 0:\n",
    "                VG_V[i][j].append(V[i][0:100][VG[i][j][k]])\n",
    "            else:\n",
    "                VG_V[i][j].append(V[i][100:][VG[i][j][k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(E, VG_V):\n",
    "    loss = 0\n",
    "\n",
    "    for entity, box in zip(E, VG_V):\n",
    "        loss = loss + loss_pair(entity, box)\n",
    "        \n",
    "    print(int(loss))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def S_lm(l, m, E, VG_V):\n",
    "    #print(\"Computing S_{}{}\".format(l, m))\n",
    "    \n",
    "    entities = E[m]\n",
    "    scores = 0\n",
    "    \n",
    "    if (len(VG_V[l]) == 0):\n",
    "        return 0\n",
    "    \n",
    "    boxes = torch.stack(VG_V[l])\n",
    "    \n",
    "    for j, ent in enumerate(entities):\n",
    "        scores = scores + (ent * boxes).sum(dim=-1).max()\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_pair(E, VG_V):\n",
    "    delta = torch.full((1, 1), 1000.0).to(device)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    assert(len(VG_V) == len(E))\n",
    "\n",
    "    num_actions = len(E)\n",
    "    \n",
    "    zero = torch.zeros((1)).to(device)\n",
    "\n",
    "    for l in range(num_actions):\n",
    "        for m in range(num_actions):\n",
    "            before_delta = S_lm(l, m, E, VG_V) - S_lm(l, l, E, VG_V)\n",
    "            loss = loss + torch.max(zero, (before_delta + delta))[0]\n",
    "\n",
    "        for m in range(num_actions):\n",
    "            before_delta = S_lm(m, l, E, VG_V) - S_lm(l, l, E, VG_V)\n",
    "            loss = loss + torch.max(zero, (before_delta + delta))[0]\n",
    "            \n",
    "    #print(\"Loss: {}\".format(loss))\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-probe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "figured-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import YouCookII\n",
    "from dataset import YouCookIICollate\n",
    "from torch.utils.data import DataLoader\n",
    "from loss import loss_RA_MIL\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from model import Model\n",
    "from model_FC import ModelFC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, num_actions, batch_size, epochs=25, lr=0.001, y=0.5, MAX_DETECTIONS=20):\n",
    "    dataset = YouCookII(num_actions, \"/h/sagar/ece496-capstone/datasets/ycii\")\n",
    "    train_size = int(len(dataset) * (2/3))\n",
    "    valid_size = int(len(dataset) - train_size)\n",
    "    \n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "    \n",
    "    collate = YouCookIICollate(MAX_DETECTIONS=MAX_DETECTIONS)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.2*epochs), epochs)\n",
    "\n",
    "    train_loss = np.zeros(epochs)\n",
    "    valid_loss = np.zeros(epochs)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for data in train_dataloader:\n",
    "            _, bboxes_tensor, features_tensor, steps_list, entity_list, entity_count_list, _, _ = data\n",
    "            batch_size = len(data[0])\n",
    "            \n",
    "            # Zero out any gradients.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Run inference (forward pass).\n",
    "            E, VG_V = model(batch_size, num_actions + 1, steps_list, features_tensor, bboxes_tensor, entity_count_list, entity_list)            \n",
    "            \n",
    "            # Loss from alignment.\n",
    "            loss_ = get_loss(E, VG_V)\n",
    "\n",
    "            # Backpropagation (backward pass).\n",
    "            loss_.backward()\n",
    "\n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss_\n",
    "            num_batches += 1\n",
    "        \n",
    "        # learning rate schedule\n",
    "        # update after each epoch\n",
    "        scheduler.step()\n",
    "        epoch_loss = epoch_loss / num_batches\n",
    "        \n",
    "        # Save loss and accuracy at each epoch, plot (and checkpoint).\n",
    "        train_loss[epoch] = epoch_loss\n",
    "        valid_loss[epoch] = get_validation_loss(model, num_actions, y, valid_dataloader)\n",
    "        \n",
    "        # after epoch completes\n",
    "        print(\"Epoch {} - Train Loss: {}, Validation Loss: {}\".format(epoch + 1, train_loss[epoch], valid_loss[epoch]))\n",
    "    \n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(valid_loss, label='valid loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    return train_loss, valid_loss\n",
    "\n",
    "def get_validation_loss(model, num_actions, y, valid_dataloader):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data in valid_dataloader:\n",
    "            _, bboxes_tensor, features_tensor, steps_list, entity_list, entity_count_list, _, _ = data\n",
    "            batch_size = len(data[0])\n",
    "\n",
    "            # Run inference (forward pass).\n",
    "            E, VG_V = model(batch_size, num_actions + 1, steps_list, features_tensor, bboxes_tensor, entity_count_list, entity_list)\n",
    "\n",
    "            # Loss from alignment.\n",
    "            loss_ = get_loss(E, VG_V)\n",
    "            \n",
    "            epoch_loss += loss_\n",
    "            num_batches += 1\n",
    "            \n",
    "    epoch_loss = epoch_loss / num_batches\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ideal-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from loss import loss_RA_MIL\n",
    "\n",
    "from model import *\n",
    "\n",
    "from random import choice\n",
    "from random import seed\n",
    "\n",
    "class ModelTesting(nn.Module):\n",
    "    ACTION = '[unused3]'\n",
    "\n",
    "    DETECTION_EMBEDDING_SIZE = 2048\n",
    "    OUTPUT_EMBEDDING_SIZE = 768\n",
    "\n",
    "    def __init__(self, device, NUM_FRAMES_PER_STEP=5, MAX_DETECTIONS=20):\n",
    "        super(ModelTesting, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.NUM_FRAMES_PER_STEP = NUM_FRAMES_PER_STEP\n",
    "        self.MAX_DETECTIONS = MAX_DETECTIONS\n",
    "        self.CANDIDATES = self.NUM_FRAMES_PER_STEP * self.MAX_DETECTIONS\n",
    "\n",
    "        self.lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [self.ACTION]})\n",
    "        self.lxmert_tokenizer.encode([self.ACTION])\n",
    "\n",
    "        self.ACTION_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(self.ACTION)\n",
    "\n",
    "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.lxmert.to(device)\n",
    "\n",
    "    def forward(self, BATCH_SIZE, NUM_ACTIONS, steps_list, features, boxes, entity_count_list, entity_list):\n",
    "        assert(BATCH_SIZE == 1)\n",
    "        \n",
    "        NUM_ACTIONS = NUM_ACTIONS - 1\n",
    "        \n",
    "        steps_list = remove_unused2(steps_list)\n",
    "        \n",
    "        entities_count = entity_count_list[0]\n",
    "        entities = entity_list[0]\n",
    "        steps = steps_list[0]\n",
    "        steps = [step.strip() for step in steps.split(self.ACTION)[:-2]]\n",
    "        \n",
    "        boxes = boxes.squeeze(0)\n",
    "        boxes = boxes.to(self.device)\n",
    "        boxes = boxes.reshape(NUM_ACTIONS, self.CANDIDATES, -1)\n",
    "        \n",
    "        features = features.squeeze(0)\n",
    "        features = features.to(self.device)\n",
    "        features = features.reshape(NUM_ACTIONS, self.CANDIDATES, -1)\n",
    "\n",
    "        steps_pairs = []\n",
    "        \n",
    "        bboxes_pairs = torch.zeros(NUM_ACTIONS, 2, self.CANDIDATES, 4)\n",
    "        bboxes_pairs = bboxes_pairs.to(self.device)\n",
    "        \n",
    "        features_pairs = torch.zeros(NUM_ACTIONS, 2, self.CANDIDATES, self.DETECTION_EMBEDDING_SIZE)\n",
    "        features_pairs = features_pairs.to(self.device)\n",
    "        \n",
    "        entity_list_pairs = []\n",
    "        num_steps = len(steps)\n",
    "        pairs = []\n",
    "        \n",
    "        for idx, step in enumerate(steps):\n",
    "            idx_1 = idx\n",
    "            idx_2 = choice([idx_2 for idx_2 in range(num_steps) if idx_2 != idx_1])\n",
    "\n",
    "            pairs.append((idx_1, idx_2))\n",
    "\n",
    "            steps_pairs.append(steps[idx_1] + \" \" + self.ACTION + \" \" + steps[idx_2] + \" \" + self.ACTION + \" \" + self.ACTION)\n",
    "\n",
    "            bboxes_pairs[idx_1][0] = boxes[idx_1]\n",
    "            bboxes_pairs[idx_1][1] = boxes[idx_2]\n",
    "\n",
    "            features_pairs[idx_1][0] = features[idx_1]\n",
    "            features_pairs[idx_1][1] = features[idx_2]\n",
    "\n",
    "            entity_list_pairs.append([entities[idx_1], entities[idx_2]])\n",
    "    \n",
    "        bboxes_pairs = bboxes_pairs.reshape(NUM_ACTIONS, 2 * self.CANDIDATES, -1)\n",
    "        bboxes_pairs = bboxes_pairs.to(self.device)\n",
    "\n",
    "        features_pairs = features_pairs.reshape(NUM_ACTIONS, 2 * self.CANDIDATES, self.DETECTION_EMBEDDING_SIZE)\n",
    "        features_pairs = features_pairs.to(self.device)\n",
    "\n",
    "        entity_count = [len(action) for action in sum(entity_list_pairs, [])]\n",
    "        \n",
    "        inputs = self.lxmert_tokenizer(\n",
    "            steps_pairs,\n",
    "            padding=\"longest\",\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs.input_ids = inputs.input_ids.to(self.device)\n",
    "        inputs.attention_mask = inputs.attention_mask.to(self.device)\n",
    "        inputs.token_type_ids = inputs.token_type_ids.to(self.device)\n",
    "\n",
    "        output = self.lxmert(\n",
    "                    input_ids=inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    visual_feats=features_pairs,\n",
    "                    visual_pos=bboxes_pairs,\n",
    "                    token_type_ids=inputs.token_type_ids,\n",
    "                    return_dict=True,\n",
    "                    output_attentions=True\n",
    "                )\n",
    "        \n",
    "        entity_idx = get_ent_inds(self, entity_list_pairs, steps_pairs)\n",
    "        entity_embeddings = get_entity_embeddings(output['language_output'], entity_idx).split(entity_count)\n",
    "\n",
    "        action_idx = (inputs.input_ids == self.ACTION_TOKEN)\n",
    "        A = output['language_output'][action_idx]\n",
    "\n",
    "        V = output['vision_output']\n",
    "        \n",
    "        E = []\n",
    "        for i in range(NUM_ACTIONS):\n",
    "            idx_1 = 2 * i\n",
    "            idx_2 = idx_1 + 1\n",
    "            E.append([entity_embeddings[idx_1], entity_embeddings[idx_2]])\n",
    "        \n",
    "        VG = []\n",
    "        \n",
    "        for i in range(NUM_ACTIONS):    \n",
    "            pair_1 = []\n",
    "            pair_2 = []\n",
    "\n",
    "            for ent_1 in E[i][0]:\n",
    "                alignment_scores = (ent_1 * V[i][0:100]).sum(dim=-1)\n",
    "                pair_1.append(alignment_scores.argmax())\n",
    "\n",
    "            for ent_2 in E[i][1]:\n",
    "                alignment_scores = (ent_2 * V[i][100:]).sum(dim=-1)\n",
    "                pair_2.append(alignment_scores.argmax())\n",
    "\n",
    "            VG.append([pair_1, pair_2])\n",
    "            \n",
    "        VG_V = []\n",
    "\n",
    "        for i, pair in enumerate(VG):\n",
    "            VG_V.append([])\n",
    "            for j, action in enumerate(pair):\n",
    "                VG_V[i].append([])\n",
    "                for k, entity in enumerate(action):\n",
    "                    if j == 0:\n",
    "                        VG_V[i][j].append(V[i][0:100][VG[i][j][k]])\n",
    "                    else:\n",
    "                        VG_V[i][j].append(V[i][100:][VG[i][j][k]])\n",
    "                        \n",
    "        VG_ind = torch.zeros((1, NUM_ACTIONS, max(entity_count))).to(self.device)\n",
    "\n",
    "        for i, pair in enumerate(VG):\n",
    "            for k, entity in enumerate(pair[0]):\n",
    "                VG_ind[0][i][k] = VG[i][0][k]\n",
    "                        \n",
    "        #return None, None, None, NUM_ACTIONS, None, None, VG_ind, None\n",
    "        return E, VG_V, output, inputs, E, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "earned-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_test = ModelTesting(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nervous-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YouCookII(8, \"/h/sagar/ece496-capstone/datasets/ycii\")\n",
    "collate = YouCookIICollate(MAX_DETECTIONS=20)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "religious-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4860661049902787\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for data in dataloader:\n",
    "    _, boxes, features, steps_list, entity_list, entity_count_list, _, _ = data\n",
    "    E, VG_V, outputs, inputs, E, V = model_test(1, 8 + 1, steps_list, features, boxes, entity_count_list, entity_list)\n",
    "    \n",
    "    for i in range(8):    \n",
    "        for ent_1 in E[i][0]:\n",
    "            aligned = (ent_1 * V[i][0:100]).sum(dim=-1)\n",
    "            aligned = aligned.argmax()\n",
    "\n",
    "            unaligned = (ent_1 * V[i][100:]).sum(dim=-1)\n",
    "            unaligned = unaligned.argmax()\n",
    "\n",
    "            #print(\"Aligned: {}, Unaligned: {}\".format(aligned, unaligned))\n",
    "\n",
    "            if (aligned > unaligned):\n",
    "                correct+=1\n",
    "            else:\n",
    "                incorrect+=1\n",
    "\n",
    "        for ent_2 in E[i][1]:\n",
    "            aligned = (ent_2 * V[i][100:]).sum(dim=-1)\n",
    "            aligned = aligned.argmax()\n",
    "\n",
    "            unaligned = (ent_2 * V[i][0:100]).sum(dim=-1)\n",
    "            unaligned = unaligned.argmax()\n",
    "\n",
    "            #print(\"Aligned: {}, Unaligned: {}\".format(aligned, unaligned))\n",
    "\n",
    "            if (aligned > unaligned):\n",
    "                correct+=1\n",
    "            else:\n",
    "                incorrect+=1\n",
    "            \n",
    "print(\"Accuracy: {}\".format(correct / (correct + incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "valid-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, boxes, features, steps_list, entity_list, entity_count_list, _, _ = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afraid-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "E, VG_V, outputs, inputs, E, V = model_test(1, 8 + 1, steps_list, features, boxes, entity_count_list, entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "western-calculator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        ...,\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "external-tolerance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        ...,\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352],\n",
       "        [-1.2660, -1.2161, -1.2227,  ...,  1.0998, -1.2849, -1.2352]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model_test.lxmert_tokenizer.convert_ids_to_tokens(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_1 = tokens\n",
    "tokens_2 = [\"C{}\".format(i + 1) for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 6\n",
    "\n",
    "attention_1 = [attn[action].unsqueeze(0) for attn in outputs.language_attentions]\n",
    "attention_2 = [attn[action].unsqueeze(0) for attn in outputs.vision_attentions]\n",
    "\n",
    "cross = [attn[action].unsqueeze(0) for attn in outputs.cross_encoder_attentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(cross).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(cross).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import model_view\n",
    "from bertviz import head_view\n",
    "\n",
    "head_view(\n",
    "    encoder_attention=attention_2,\n",
    "    decoder_attention=attention_1,\n",
    "    cross_attention=cross,\n",
    "    encoder_tokens=tokens_2,\n",
    "    decoder_tokens=tokens_1,\n",
    "    layer=0,\n",
    "    heads=[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, valid_loss = train(model_test, 8, 1, epochs=100, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_fi import eval_all_dataset\n",
    "eval_all_dataset(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_test.state_dict(), \"/h/sagar/ece496-capstone/weights/weights-nv-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "greater-maine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.load_state_dict(torch.load(\"/h/sagar/ece496-capstone/weights/weights-nv-5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-difficulty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
