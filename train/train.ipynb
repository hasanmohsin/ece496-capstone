{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video import Video\n",
    "\n",
    "video_path = \"/home/sagarpatel/Desktop/ece496-capstone/train/sample/video.mp4\"\n",
    "transcript_path = \"/home/sagarpatel/Desktop/ece496-capstone/train/sample/transcript.vtt\"\n",
    "transcript2_path = \"/home/sagarpatel/Desktop/ece496-capstone/train/sample/transcript2.vtt\"\n",
    "\n",
    "v = Video(video_path, transcript_path)\n",
    "v.align()\n",
    "v.generate_frames(\"sample\", swap=True)\n",
    "\n",
    "v2 = Video(video_path, transcript2_path)\n",
    "v2.align()\n",
    "v2.generate_frames(\"sample\", swap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /home/sagarpatel/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import itertools\n",
    "\n",
    "from loss import loss_RA_MIL\n",
    "from detector import Detector\n",
    "from parser import parse\n",
    "\n",
    "detector = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(\"/home/sagarpatel/Desktop/ece496-capstone/train/sample/*.png\"))\n",
    "candidates = [detector.inference(image, max_detections=5) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL = '[unused1]'\n",
    "PAD = '[unused2]'\n",
    "ENTITY = '[unused3]'\n",
    "ACTION = '[SEP]'\n",
    "\n",
    "MAX_STEP_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1 = [step.text.strip() for step in v.steps]\n",
    "steps2 = [step.text.strip() for step in v2.steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "\n",
    "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\", pad_token=PAD)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "tokenizer.encode([NULL, PAD, ENTITY], add_special_tokens=True)\n",
    "\n",
    "tokens_steps1 = tokenizer(\n",
    "                    steps1,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "tokens_steps2 = tokenizer(\n",
    "                    steps2,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1_flat = tokens_steps1['input_ids'].flatten()\n",
    "steps1_flat = steps1_flat[steps1_flat != 101]\n",
    "steps1_flat = tokenizer.decode(steps1_flat) + ' ' + NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps2_flat = tokens_steps2['input_ids'].flatten()\n",
    "steps2_flat = steps2_flat[steps2_flat != 101]\n",
    "steps2_flat = tokenizer.decode(steps2_flat) + ' ' + NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all of the bounding boxes for the detections and their features.\n",
    "boxes = torch.tensor([candidate[0].numpy() for candidate in candidates]).squeeze(1)\n",
    "features = torch.tensor([candidate[1].numpy() for candidate in candidates]).squeeze(1)\n",
    "\n",
    "boxes = boxes.flatten(start_dim=0, end_dim=1)\n",
    "features = features.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "boxes = torch.stack((boxes, boxes))\n",
    "features = torch.stack((features, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [steps1_flat, steps2_flat]\n",
    "\n",
    "ENTITIES_COUNT = [\n",
    "    [2, 2, 1, 2, 3, 2, 2, 2, 1, 1],\n",
    "    [1, 1, 1, 1, 3, 2, 1, 2, 1, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-research/bert/issues/635\n",
    "# https://colab.research.google.com/drive/18TyuMfZYlgQ_nXo-tr8LCnzUaoX0KS-h?usp=sharing#scrollTo=W4cZIVrg82ua.\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Model(nn.Module):\n",
    "    NULL = '[unused1]'\n",
    "    PAD = '[unused2]'\n",
    "    ENTITY = '[unused3]'\n",
    "    ACTION = '[SEP]'\n",
    "    \n",
    "    DETECTION_EMBEDDING_SIZE = 2048\n",
    "    OUTPUT_EMBEDDING_SIZE = 768\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "        self.lxmert_tokenizer.encode([NULL, PAD, ENTITY])\n",
    "        \n",
    "        self.NULL_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(NULL)\n",
    "        self.ENTITY_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(ENTITY)\n",
    "        self.ACTION_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(ACTION)\n",
    "        \n",
    "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        \n",
    "    def forward(self, MAX_INSTRUCTION_LENGTH, BATCH_SIZE, NUM_ACTIONS, CANDIDATES, steps, features, boxes, entity_count):\n",
    "        '''\n",
    "            MAX_INSTRUCTION_LENGTH\n",
    "                : maximum number of words in a combined string of steps for a video,\n",
    "                  this instruction string must contain the NULL step.\n",
    "                  \n",
    "                  ex. | E ... E . [SEP] [PAD] [PAD] | .... [SEP] [PAD] | NULL | \n",
    "                  \n",
    "            steps\n",
    "                : batched video steps of size (BATCH_SIZE, NUM_ACTIONS), list of lists,\n",
    "                  each instructional video must have the same number of steps = NUM_ACTIONS.\n",
    "                \n",
    "                  ex. [['step 1.1', 'step 1.2'], ['step 2.1', 'step 2.2']]\n",
    "                \n",
    "            features\n",
    "                : batched detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 2048)\n",
    "                \n",
    "            boxes\n",
    "                : batched bounding boxes of detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 4)\n",
    "                \n",
    "            entity_count:\n",
    "                : number of entities per action, size (BATCH_SIZE, NUM_ACTIONS), list of lists\n",
    "        '''\n",
    "\n",
    "        inputs = self.lxmert_tokenizer(\n",
    "            steps,\n",
    "            padding=\"max_length\",\n",
    "            max_length= MAX_INSTRUCTION_LENGTH + 2, # [CLS] and [SEP] token\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        output = self.lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features,\n",
    "            visual_pos=boxes,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        \n",
    "        token_ids = inputs.input_ids\n",
    "        \n",
    "        entity_idx = ((token_ids == self.ENTITY_TOKEN) | (token_ids == self.NULL_TOKEN))\n",
    "        action_idx = (token_ids == self.ACTION_TOKEN)\n",
    "        \n",
    "        entity_embeddings = output['language_output'][entity_idx]\n",
    "        action_embeddings = output['language_output'][action_idx]\n",
    "        vision_embeddings = output['vision_output']\n",
    "        \n",
    "        split_sizes = torch.tensor(entity_count).flatten().tolist()\n",
    "        entities = entity_embeddings.split(split_sizes)\n",
    "        \n",
    "        E = pad_sequence(entities, batch_first=True)\n",
    "        max_entities = E.shape[1]\n",
    "        E = E.reshape(-1, NUM_ACTIONS, E.shape[1], E.shape[2])\n",
    "\n",
    "        A = action_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, -1)\n",
    "        V = vision_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, CANDIDATES, -1)\n",
    "        \n",
    "        # Calculate loss_E.\n",
    "        loss_E = E\n",
    "        \n",
    "        # Calculate VG (VG_scores_index) and loss_V.\n",
    "        VG_scores = torch.einsum('bacs, baes -> baec', V, E)\n",
    "        VG_scores_max, VG_scores_index = VG_scores.max(dim=-1)\n",
    "\n",
    "        V_flat = V.reshape(-1, self.OUTPUT_EMBEDDING_SIZE)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index.flatten()\n",
    "        offsets = torch.arange(0, BATCH_SIZE * NUM_ACTIONS * CANDIDATES, CANDIDATES)\n",
    "        offsets = offsets.repeat_interleave(max_entities)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index_flat + offsets\n",
    "\n",
    "        loss_V = V_flat[VG_scores_index_flat, :].reshape(BATCH_SIZE, NUM_ACTIONS, max_entities, -1).shape\n",
    "        \n",
    "        # Calculate RR (RR_scores_index).\n",
    "        RR_scores = torch.einsum('baes, bcs -> baec', E, A)\n",
    "\n",
    "        edge_mask = torch.ones(NUM_ACTIONS, NUM_ACTIONS).tril(diagonal=-1)\n",
    "        edge_mask[-1, :] = 0\n",
    "        edge_mask[:, -1] = 1\n",
    "        edge_mask = einops.repeat(edge_mask, 'x y -> b x c y', b=BATCH_SIZE, c=max_entities)\n",
    "\n",
    "        RR_scores_max, RR_scores_index = (RR_scores * edge_mask).max(dim=-1)\n",
    "        \n",
    "        # Calculate loss_R.\n",
    "        loss_R = np.ones((BATCH_SIZE, NUM_ACTIONS, NUM_ACTIONS))\n",
    "\n",
    "        dim_1 = RR_scores_index.reshape(BATCH_SIZE, -1)\n",
    "        dim_2 = torch.arange(NUM_ACTIONS).repeat_interleave(max_entities)\n",
    "        dim_2 = einops.repeat(dim_2, 'd -> b d', b=BATCH_SIZE)\n",
    "\n",
    "        loss_R[:, dim_1, dim_2] = 0\n",
    "        \n",
    "        return loss_E, loss_V, loss_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=25, lr=0.001, batch_size=10, y=0.5):\n",
    "    '''\n",
    "    Training loop for the model.\n",
    "    '''\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Ouput losses.\n",
    "    train_loss = np.zeros(epochs)\n",
    "    \n",
    "    # Output accuracies.\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    \n",
    "    m_RR = None\n",
    "    m_VG = None\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Zero out any gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run inference (forward pass).\n",
    "        loss_E, loss_V, loss_R = model(265, 2, 10, 5, steps, features, boxes, ENTITIES_COUNT)\n",
    "\n",
    "        # Loss from alignment.\n",
    "        loss = loss_RA_MIL(y, loss_R, loss_E, loss_V)\n",
    "        print(loss)\n",
    "\n",
    "        # Backpropagation (backward pass).\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        m_RR = RR\n",
    "        m_VG = VG\n",
    "            \n",
    "        # TODO: save loss and accuracy at each epoch, plot (and checkpoint).\n",
    "        \n",
    "    return m_RR, m_VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 1, but got torch.Size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bd43fdef3b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-5f858fbf4929>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, lr, batch_size, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Loss from alignment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_RA_MIL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_R\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ece496-capstone/train/loss.py\u001b[0m in \u001b[0;36mloss_RA_MIL\u001b[0;34m(y, R, E, V)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Note that the alignment score for the padding matrices will be -inf due\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# to the way they're configured (1 * -inf).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mjd, lkd -> lmjk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m#scores[scores.isnan()] = float('-inf')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ece496-capstone/env/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 1, but got torch.Size"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "RR, VG = train(model, epochs=50, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
