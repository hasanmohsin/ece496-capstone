{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "PATH = \"/home/sagarpatel/Desktop/ece496-capstone/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video import Video\n",
    "\n",
    "video_path = PATH + \"train/sample/video.mp4\"\n",
    "transcript_path = PATH + \"train/sample/transcript.vtt\"\n",
    "transcript2_path = PATH + \"train/sample/transcript2.vtt\"\n",
    "\n",
    "v = Video(video_path, transcript_path)\n",
    "v.align()\n",
    "v.generate_frames(\"sample\", swap=True)\n",
    "\n",
    "v2 = Video(video_path, transcript2_path)\n",
    "v2.align()\n",
    "v2.generate_frames(\"sample\", swap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /home/sagarpatel/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import itertools\n",
    "\n",
    "from loss import loss_RA_MIL\n",
    "from detector import Detector\n",
    "from parser import parse\n",
    "\n",
    "detector = Detector(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'loss' from '/home/sagarpatel/Desktop/ece496-capstone/train/loss.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import loss\n",
    "importlib.reload(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(PATH + \"train/sample/*.png\"))\n",
    "candidates = [detector.inference(image, max_detections=5) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[0][0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL = '[unused1]'\n",
    "PAD = '[unused2]'\n",
    "ENTITY = '[unused3]'\n",
    "ACTION = '[SEP]'\n",
    "\n",
    "MAX_STEP_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1 = [step.text.strip() for step in v.steps]\n",
    "steps2 = [step.text.strip() for step in v2.steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1_flat = ' [SEP] '.join(steps1) + ' ' + NULL + ' [SEP]'\n",
    "steps2_flat = ' [SEP] '.join(steps2) + ' ' + NULL + ' [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all of the bounding boxes for the detections and their features.\n",
    "boxes = torch.tensor([candidate[0].numpy() for candidate in candidates]).squeeze(1)\n",
    "features = torch.tensor([candidate[1].numpy() for candidate in candidates]).squeeze(1)\n",
    "\n",
    "boxes = boxes.flatten(start_dim=0, end_dim=1)\n",
    "features = features.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "boxes = torch.stack((boxes, boxes))\n",
    "features = torch.stack((features, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-research/bert/issues/635\n",
    "# https://colab.research.google.com/drive/18TyuMfZYlgQ_nXo-tr8LCnzUaoX0KS-h?usp=sharing#scrollTo=W4cZIVrg82ua.\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Model(nn.Module):\n",
    "    NULL = '[unused1]'\n",
    "    PAD = '[unused2]'\n",
    "    ENTITY = '[unused3]'\n",
    "    ACTION = '[SEP]'\n",
    "    \n",
    "    DETECTION_EMBEDDING_SIZE = 2048\n",
    "    OUTPUT_EMBEDDING_SIZE = 768\n",
    "        \n",
    "    def __init__(self, device):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "        self.lxmert_tokenizer.encode([NULL, PAD, ENTITY])\n",
    "        \n",
    "        self.NULL_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(NULL)\n",
    "        self.ENTITY_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(ENTITY)\n",
    "        self.ACTION_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(ACTION)\n",
    "        \n",
    "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.lxmert.to(device)\n",
    "        \n",
    "    def forward(self, BATCH_SIZE, NUM_ACTIONS, CANDIDATES, steps, features, boxes, entity_count):\n",
    "        '''\n",
    "            MAX_INSTRUCTION_LENGTH\n",
    "                : maximum number of words in a combined string of steps for a video,\n",
    "                  this instruction string must contain the NULL step.\n",
    "                  \n",
    "                  ex. | E ... E . [SEP] [PAD] [PAD] | .... [SEP] [PAD] | NULL | \n",
    "                  \n",
    "            steps\n",
    "                : batched video steps of size (BATCH_SIZE, NUM_ACTIONS), list of lists,\n",
    "                  each instructional video must have the same number of steps = NUM_ACTIONS.\n",
    "                \n",
    "                  ex. [['step 1.1', 'step 1.2'], ['step 2.1', 'step 2.2']]\n",
    "                \n",
    "            features\n",
    "                : batched detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 2048)\n",
    "                \n",
    "            boxes\n",
    "                : batched bounding boxes of detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 4)\n",
    "                \n",
    "            entity_count:\n",
    "                : number of entities per action, size (BATCH_SIZE, NUM_ACTIONS), list of lists\n",
    "        '''\n",
    "\n",
    "        inputs = self.lxmert_tokenizer(\n",
    "            steps,\n",
    "            padding=\"longest\",\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        output = self.lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features,\n",
    "            visual_pos=boxes,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        \n",
    "        token_ids = inputs.input_ids\n",
    "        \n",
    "        entity_idx = ((token_ids == self.ENTITY_TOKEN) | (token_ids == self.NULL_TOKEN))\n",
    "        action_idx = (token_ids == self.ACTION_TOKEN)\n",
    "        \n",
    "        entity_embeddings = output['language_output'][entity_idx]\n",
    "        action_embeddings = output['language_output'][action_idx]\n",
    "        vision_embeddings = output['vision_output']\n",
    "        \n",
    "        split_sizes = torch.tensor(entity_count).flatten().tolist()\n",
    "        entities = entity_embeddings.split(split_sizes)\n",
    "        \n",
    "        E = pad_sequence(entities, batch_first=True)\n",
    "        max_entities = E.shape[1]\n",
    "        E = E.reshape(-1, NUM_ACTIONS, E.shape[1], E.shape[2])\n",
    "\n",
    "        A = action_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, -1)\n",
    "        V = vision_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, CANDIDATES, -1)\n",
    "        \n",
    "        # Calculate loss_E.\n",
    "        loss_E = E\n",
    "        \n",
    "        # Calculate VG (VG_scores_index) and loss_V.\n",
    "        VG_scores = torch.einsum('bacs, baes -> baec', V, E)\n",
    "        VG_scores_max, VG_scores_index = VG_scores.max(dim=-1)\n",
    "\n",
    "        V_flat = V.reshape(-1, self.OUTPUT_EMBEDDING_SIZE)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index.flatten()\n",
    "        offsets = torch.arange(0, BATCH_SIZE * NUM_ACTIONS * CANDIDATES, CANDIDATES)\n",
    "        offsets = offsets.repeat_interleave(max_entities)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index_flat + offsets\n",
    "\n",
    "        loss_V = V_flat[VG_scores_index_flat, :].reshape(BATCH_SIZE, NUM_ACTIONS, max_entities, -1)\n",
    "        \n",
    "        # Calculate RR (RR_scores_index).\n",
    "        #note: we write c here as number of actions as well\n",
    "        #since we don't wish to sum over a \n",
    "        RR_scores = torch.einsum('baes, bcs -> baec', E, A)\n",
    "\n",
    "        edge_mask = torch.ones(NUM_ACTIONS, NUM_ACTIONS).tril(diagonal=-1)\n",
    "        edge_mask[-1, :] = 0\n",
    "        edge_mask[:, -1] = 1\n",
    "        edge_mask = einops.repeat(edge_mask, 'x y -> b x c y', b=BATCH_SIZE, c=max_entities)\n",
    "\n",
    "        RR_scores_max, RR_scores_index = (RR_scores * edge_mask).max(dim=-1)\n",
    "        \n",
    "        # Calculate loss_R.\n",
    "        loss_R = torch.ones((BATCH_SIZE, NUM_ACTIONS, NUM_ACTIONS))\n",
    "\n",
    "        dim_1 = RR_scores_index.reshape(BATCH_SIZE, -1)\n",
    "        dim_2 = torch.arange(NUM_ACTIONS).repeat_interleave(max_entities)\n",
    "        dim_2 = einops.repeat(dim_2, 'd -> b d', b=BATCH_SIZE)\n",
    "\n",
    "        loss_R[:, dim_1, dim_2] = 0.0\n",
    "        \n",
    "        #entity embeddings, selected visual grounding embeddings, adjacency list for\n",
    "        #ref resolution\n",
    "        return loss_E, loss_V, loss_R, VG_scores_index, RR_scores_index, A, E, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [steps1_flat, steps2_flat]\n",
    "\n",
    "ENTITIES_COUNT = [\n",
    "    [2, 2, 1, 2, 3, 2, 2, 2, 1, 1],\n",
    "    [1, 1, 1, 1, 3, 2, 1, 2, 1, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "features.to(device)\n",
    "boxes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_E, loss_V, loss_R, VG, RR, A, E, V = model(2, 10, 5, steps, features, boxes, ENTITIES_COUNT)\n",
    "\n",
    "# https://stackoverflow.com/questions/14531346/how-to-add-a-text-into-a-rectangle\n",
    "\n",
    "# Visualization. Requires: \n",
    "# - list of entities and entity count\n",
    "# - list of steps\n",
    "# - VG, RR, bb -- Note: this is for the current video, ie. not batched\n",
    "# - directory with an image per step (0.png, 1.png, etc.)\n",
    "\n",
    "bb = boxes[0].reshape([10, 5, 4]) # Actions, candidates, bb\n",
    "\n",
    "VG = VG[0]\n",
    "RR = RR[0]\n",
    "\n",
    "entities = [\n",
    "    [\"the tomatoes\", \"a pan\"],\n",
    "    [\"oil\", \"the pan\"],\n",
    "    [\"the bacon\"],\n",
    "    [\"some mayonnaise\", \"the bread\"],\n",
    "    [\"a piece of\", \"lettuce\", \"it\"],\n",
    "    [\"the tomatoes\", \"it\"],\n",
    "    [\"some salt\", \"it\"],\n",
    "    [\"the bacon\", \"the top\"],\n",
    "    [\"piece of bread\"],\n",
    "    [\"NULL\"]\n",
    "]\n",
    "\n",
    "entity_count = ENTITIES_COUNT[0]\n",
    "\n",
    "steps_trimmed = [step.replace(ENTITY, \"\").replace(\"  \", \" \") for step in steps1]\n",
    "steps_trimmed.append(\"NULL\")\n",
    "\n",
    "directory = \"/home/sagarpatel/Desktop/ece496-capstone/train/sample/\"\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize(VG, RR, bb, entities, steps_trimmed, steps, directory):\n",
    "    # VG visualizer.\n",
    "    for step_idx, data in enumerate(zip(entity_count, VG, bb)):\n",
    "        step_entity_count, step_VG, step_bb = data\n",
    "        frame = cv2.imread('/home/sagarpatel/Desktop/ece496-capstone/train/sample/{}.png'.format(step_idx))\n",
    "        height = frame.shape[0]\n",
    "        width = frame.shape[1]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(frame)\n",
    "        axes = plt.gca()\n",
    "\n",
    "        print(\"\\nStep {}\\n---------\".format(step_idx))\n",
    "\n",
    "        for entity_idx, entity_VG in enumerate(step_VG):        \n",
    "            if entity_idx >= step_entity_count:\n",
    "                break\n",
    "\n",
    "            entity_text = entities[step_idx][entity_idx]\n",
    "            action_id = RR[step_idx][entity_idx]\n",
    "            action_text = steps_trimmed[action_id]\n",
    "            print(\"Entity: {} --> Action: {} ({})\".format(entity_text, action_id, action_text))\n",
    "\n",
    "            index = int(entity_VG)\n",
    "            entity_bb = step_bb[index].tolist()\n",
    "\n",
    "            x0, y0 = entity_bb[0] * width, entity_bb[1] * height\n",
    "            x1, y1 = entity_bb[2] * width, entity_bb[3] * height\n",
    "            \n",
    "            width = x1 - x0\n",
    "            height = y1 - y0\n",
    "\n",
    "            box = Rectangle((x0, y0), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            axes.add_patch(box)\n",
    "            axes.annotate(entity_text, (x0 + (width / 2), y0 + (height / 2)), color='white', \n",
    "                fontsize=10, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(VG, RR, bb, entities, entity_count, steps, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=25, lr=0.001, batch_size=10, y=0.5):\n",
    "    '''\n",
    "    Training loop for the model.\n",
    "    '''\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Ouput losses.\n",
    "    train_loss = np.zeros(epochs)\n",
    "    \n",
    "    # Output accuracies.\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    \n",
    "    m_RR = None\n",
    "    m_VG = None\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Zero out any gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run inference (forward pass).\n",
    "        loss_E, loss_V, loss_R = model(265, 2, 10, 5, steps, features, boxes, ENTITIES_COUNT)\n",
    "\n",
    "        # Loss from alignment.\n",
    "        loss_ = loss.loss_RA_MIL(y, loss_R, loss_E, loss_V)\n",
    "        print(loss_)\n",
    "\n",
    "        # Backpropagation (backward pass).\n",
    "        loss_.backward()\n",
    "\n",
    "        # Update parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        #m_RR = RR\n",
    "        #m_VG = VG\n",
    "            \n",
    "        # TODO: save loss and accuracy at each epoch, plot (and checkpoint).\n",
    "        train_loss[epoch] = loss_\n",
    "        \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "train_loss = train(model, epochs=50, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "loss_E, loss_V, loss_R = model(265, 2, 10, 5, steps, features, boxes, ENTITIES_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(loss_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " MAX_INSTRUCTION_LENGTH, BATCH_SIZE, NUM_ACTIONS, CANDIDATES, steps, features, boxes, entity_count):\n",
    "        '''\n",
    "            MAX_INSTRUCTION_LENGTH\n",
    "                : maximum number of words in a combined string of steps for a video,\n",
    "                  this instruction string must contain the NULL step.\n",
    "                  \n",
    "                  ex. | E ... E . [SEP] [PAD] [PAD] | .... [SEP] [PAD] | NULL | \n",
    "                  \n",
    "            steps\n",
    "                : batched video steps of size (BATCH_SIZE, NUM_ACTIONS), list of lists,\n",
    "                  each instructional video must have the same number of steps = NUM_ACTIONS.\n",
    "                \n",
    "                  ex. [['step 1.1', 'step 1.2'], ['step 2.1', 'step 2.2']]\n",
    "                \n",
    "            features\n",
    "                : batched detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 2048)\n",
    "                \n",
    "            boxes\n",
    "                : batched bounding boxes of detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 4)\n",
    "                \n",
    "            entity_count:\n",
    "                : number of entities per action, size (BATCH_SIZE, NUM_ACTIONS), list of lists\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        inputs = tokenizer(\n",
    "            steps,\n",
    "            padding=\"max_length\",\n",
    "            max_length= 265 + 2, # [CLS] and [SEP] token\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(steps[1].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_steps = tokenizer.batch_decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(decoded_steps[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        output = lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features,\n",
    "            visual_pos=boxes,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['language_output'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS=10\n",
    "BATCH_SIZE=2\n",
    "\n",
    "edge_mask = torch.ones(NUM_ACTIONS, NUM_ACTIONS).tril(diagonal=-1)\n",
    "edge_mask[-1, :] = 0\n",
    "edge_mask[:, -1] = 1\n",
    "edge_mask = einops.repeat(edge_mask, 'x y -> b x c y', b=BATCH_SIZE, c=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.diagonal(dim1 = 1, dim2= 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
