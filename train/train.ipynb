{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rural-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import YouCookII\n",
    "from dataset import collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from loss import loss_RA_MIL\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, num_actions, batch_size, epochs=25, lr=0.001, y=0.5):\n",
    "    dataset = YouCookII(num_actions, \"/h/sagar/ece496-capstone/datasets/ycii\")\n",
    "    train_size = int(len(dataset) * (2/3))\n",
    "    valid_size = int(len(dataset) - train_size)\n",
    "    \n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.2*epochs), epochs)\n",
    "\n",
    "    train_loss = np.zeros(epochs)\n",
    "    valid_loss = np.zeros(epochs)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for data in train_dataloader:\n",
    "            _, bboxes_tensor, features_tensor, steps_list, _, entity_count_list, _, _ = data\n",
    "            batch_size = len(data[0])\n",
    "            \n",
    "            # Zero out any gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Run inference (forward pass).\n",
    "            loss_E, loss_V, loss_R, _, _, _, _, _ = model(batch_size, num_actions + 1, steps_list, features_tensor, bboxes_tensor, entity_count_list)\n",
    "\n",
    "            # Loss from alignment.\n",
    "            loss_ = loss_RA_MIL(y, loss_R, loss_E, loss_V)\n",
    "\n",
    "            # Backpropagation (backward pass).\n",
    "            loss_.backward()\n",
    "\n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss_\n",
    "            num_batches += 1\n",
    "        \n",
    "        #learning rate schedule\n",
    "        #update after each epoch\n",
    "        scheduler.step()\n",
    "        epoch_loss = epoch_loss / num_batches\n",
    "        \n",
    "        # Save loss and accuracy at each epoch, plot (and checkpoint).\n",
    "        train_loss[epoch] = epoch_loss\n",
    "        valid_loss[epoch] = get_validation_loss(num_actions, y, valid_dataloader)\n",
    "        \n",
    "        #after epoch completes\n",
    "        print(\"Epoch {} - Train Loss: {}, Validation Loss: {}\".format(epoch, train_loss[epoch], valid_loss[epoch]))\n",
    "    \n",
    "    plt.plot(train_loss, valid_loss)\n",
    "        \n",
    "    return train_loss, valid_loss\n",
    "\n",
    "def get_validation_loss(num_actions, y, valid_dataloader):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data in valid_dataloader:\n",
    "            _, bboxes_tensor, features_tensor, steps_list, _, entity_count_list, _, _ = data\n",
    "            batch_size = len(data[0])\n",
    "\n",
    "            # Run inference (forward pass).\n",
    "            loss_E, loss_V, loss_R, _, _, _, _, _ = model(batch_size, num_actions + 1, steps_list, features_tensor, bboxes_tensor, entity_count_list)\n",
    "\n",
    "            # Loss from alignment.\n",
    "            loss_ = loss_RA_MIL(y, loss_R, loss_E, loss_V)\n",
    "            \n",
    "            epoch_loss += loss_\n",
    "            num_batches += 1\n",
    "            \n",
    "    epoch_loss = epoch_loss / num_batches\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 1984.5281982421875, Validation Loss: 1601.7069091796875\n",
      "Epoch 1 - Train Loss: 1756.751220703125, Validation Loss: 1400.0838623046875\n",
      "Epoch 2 - Train Loss: 1506.93994140625, Validation Loss: 1183.3402099609375\n",
      "Epoch 3 - Train Loss: 1374.0240478515625, Validation Loss: 1397.35498046875\n",
      "Epoch 4 - Train Loss: 1236.9931640625, Validation Loss: 1048.4273681640625\n",
      "Epoch 5 - Train Loss: 1381.4388427734375, Validation Loss: 1177.0594482421875\n",
      "Epoch 6 - Train Loss: 1465.8040771484375, Validation Loss: 1674.5927734375\n",
      "Epoch 7 - Train Loss: 1784.80712890625, Validation Loss: 1846.4990234375\n",
      "Epoch 8 - Train Loss: 1567.5465087890625, Validation Loss: 879.2543334960938\n",
      "Epoch 9 - Train Loss: 1180.1571044921875, Validation Loss: 785.67724609375\n",
      "Epoch 10 - Train Loss: 1000.3878784179688, Validation Loss: 735.6682739257812\n",
      "Epoch 11 - Train Loss: 1185.2210693359375, Validation Loss: 724.5114135742188\n",
      "Epoch 12 - Train Loss: 786.1958618164062, Validation Loss: 501.7469787597656\n",
      "Epoch 13 - Train Loss: 447.70062255859375, Validation Loss: 381.99847412109375\n",
      "Epoch 14 - Train Loss: 756.5067138671875, Validation Loss: 560.1227416992188\n",
      "Epoch 15 - Train Loss: 528.9818115234375, Validation Loss: 308.3778076171875\n",
      "Epoch 16 - Train Loss: 439.6409912109375, Validation Loss: 460.3456726074219\n",
      "Epoch 17 - Train Loss: 435.2046813964844, Validation Loss: 366.63763427734375\n",
      "Epoch 18 - Train Loss: 487.3797912597656, Validation Loss: 491.8629455566406\n",
      "Epoch 19 - Train Loss: 750.6215209960938, Validation Loss: 548.1610717773438\n",
      "Epoch 20 - Train Loss: 677.7605590820312, Validation Loss: 436.3793029785156\n",
      "Epoch 21 - Train Loss: 772.158935546875, Validation Loss: 479.1011657714844\n",
      "Epoch 22 - Train Loss: 607.2523803710938, Validation Loss: 1169.3970947265625\n",
      "Epoch 23 - Train Loss: 669.22021484375, Validation Loss: 470.8390197753906\n",
      "Epoch 24 - Train Loss: 398.4836730957031, Validation Loss: 357.35498046875\n",
      "Epoch 25 - Train Loss: 475.8791198730469, Validation Loss: 292.0034484863281\n",
      "Epoch 26 - Train Loss: 324.4643859863281, Validation Loss: 302.6119079589844\n",
      "Epoch 27 - Train Loss: 293.40478515625, Validation Loss: 236.61514282226562\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(device)\n",
    "train_loss = train(model, 8, 2, epochs=50, lr=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
