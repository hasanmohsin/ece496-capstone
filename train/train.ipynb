{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "literary-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets seeds for reproducability.\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fossil-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import YouCookII\n",
    "from dataset import YouCookIICollate\n",
    "from torch.utils.data import DataLoader\n",
    "from loss import *\n",
    "from accuracy import *\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from model import Model\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, num_actions_train=4, batch_size=4, epochs=25, lr=0.001, MAX_DETECTIONS=20):    \n",
    "    train_datasets = [YouCookII(num_actions_train, \"/h/sagar/ece496-capstone/datasets/ycii_{}\".format(num_actions_train))]\n",
    "    \n",
    "    # Validation set defaults to test set for now for diagnosing.\n",
    "    num_actions_valid = [4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 25, 27]\n",
    "    valid_datasets = [YouCookII(num_action, \"/h/sagar/ece496-capstone/datasets/fi\") for num_action in num_actions_valid]\n",
    "    \n",
    "    train_size = sum([len(train_dataset) for train_dataset in train_datasets])\n",
    "    valid_size = sum([len(valid_dataset) for valid_dataset in valid_datasets])\n",
    "    \n",
    "    print(\"Training Dataset Size: {}, Validation Dataset Size: {}\".format(train_size, valid_size))\n",
    "    print(\"Effective Batch Size: {} * {} = {}\".format(num_actions_train, batch_size, num_actions_train * batch_size))\n",
    "    print(\"Learning Rate: {}, Epochs: {}\".format(lr, epochs))\n",
    "    \n",
    "    collate = YouCookIICollate(MAX_DETECTIONS=MAX_DETECTIONS)\n",
    "    \n",
    "    train_dataloaders = [DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate, drop_last=True, worker_init_fn=seed_worker)\n",
    "                         for train_dataset in train_datasets]\n",
    "    valid_dataloaders = [DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate, drop_last=False, worker_init_fn=seed_worker)\n",
    "                         for valid_dataset in valid_datasets]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.2*epochs), epochs)\n",
    "\n",
    "    train_loss = np.zeros(epochs)\n",
    "    valid_loss = np.zeros(epochs)\n",
    "    \n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    valid_accuracy = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0.\n",
    "        datapoints = 0\n",
    "        \n",
    "        for train_dataloader in train_dataloaders:\n",
    "            for input_data in train_dataloader:\n",
    "                _, bboxes, features, actions, steps, entities, entity_count, _ = input_data\n",
    "                \n",
    "                # Zero out any gradients.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Run inference (forward pass).\n",
    "                loss_data, VG, RR = model(steps, features, bboxes, entities, entity_count)\n",
    "\n",
    "                # Loss from alignment.\n",
    "                loss_ = compute_loss_batched(loss_data)\n",
    "\n",
    "                # Backpropagation (backward pass).\n",
    "                loss_.backward()\n",
    "\n",
    "                # Update parameters.\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss_\n",
    "                datapoints += len(steps) * len(actions[0])\n",
    "                            \n",
    "        # Scheduler update.\n",
    "        scheduler.step()\n",
    "        epoch_loss = epoch_loss / datapoints\n",
    "        \n",
    "        # Save loss and accuracy at each epoch and plot.\n",
    "        train_loss[epoch] = float(epoch_loss)\n",
    "        train_accuracy[epoch] = get_alignment_accuracy(model, train_dataloaders) \n",
    "        \n",
    "        valid_loss[epoch] = get_alignment_loss(model, valid_dataloaders)\n",
    "        valid_accuracy[epoch] = get_alignment_accuracy(model, valid_dataloaders)\n",
    "        \n",
    "        print(\"Epoch {} - Train Loss: {:.2f}, Validation Loss: {:.2f}, Train Accuracy: {:.2f}, Validation Accuracy: {:.2f}\"\n",
    "              .format(epoch + 1, train_loss[epoch], valid_loss[epoch], train_accuracy[epoch], valid_accuracy[epoch]))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(valid_loss, label='valid loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_accuracy, label='train accuracy')\n",
    "    plt.plot(valid_accuracy, label='valid accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "    return train_loss, valid_loss, train_accuracy, valid_accuracy, VG, loss_data, input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "australian-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: 813, Validation Dataset Size: 54\n",
      "Effective Batch Size: 4 * 8 = 32\n",
      "Learning Rate: 1e-05, Epochs: 50\n",
      "Epoch 1 - Train Loss: 63.62, Validation Loss: 324.58, Train Accuracy: 0.49, Validation Accuracy: 0.52\n",
      "Epoch 2 - Train Loss: 62.10, Validation Loss: 305.97, Train Accuracy: 0.51, Validation Accuracy: 0.51\n",
      "Epoch 3 - Train Loss: 61.45, Validation Loss: 304.60, Train Accuracy: 0.52, Validation Accuracy: 0.51\n",
      "Epoch 4 - Train Loss: 60.17, Validation Loss: 299.99, Train Accuracy: 0.52, Validation Accuracy: 0.51\n",
      "Epoch 5 - Train Loss: 59.36, Validation Loss: 301.89, Train Accuracy: 0.53, Validation Accuracy: 0.51\n",
      "Epoch 6 - Train Loss: 58.80, Validation Loss: 297.70, Train Accuracy: 0.54, Validation Accuracy: 0.52\n",
      "Epoch 7 - Train Loss: 57.12, Validation Loss: 294.78, Train Accuracy: 0.55, Validation Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Trainer.\n",
    "\n",
    "num_actions_train = 4\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "lr = 1e-5\n",
    "\n",
    "train_loss, valid_loss, train_accuracy, valid_accuracy, VG, loss_data, input_data = train(\n",
    "    model, \n",
    "    num_actions_train=num_actions_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation.\n",
    "\n",
    "from eval_fi import eval_all_dataset\n",
    "eval_all_dataset(model, path=\"/h/sagar/ece496-capstone/datasets/fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizer.\n",
    "\n",
    "from visualizer import inference\n",
    "\n",
    "YCII = \"/h/sagar/ece496-capstone/datasets/ycii\"\n",
    "FI = \"/h/sagar/ece496-capstone/datasets/fi\"\n",
    "\n",
    "VG, RR = inference(model, 27, 0, FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading weights.\n",
    "\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "\n",
    "if SAVE:\n",
    "    torch.save(model.state_dict(), \"/h/sagar/ece496-capstone/weights/t3\")\n",
    "    \n",
    "if LOAD:\n",
    "    model.load_state_dict(torch.load(\"/h/sagar/ece496-capstone/weights/t1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules.\n",
    "\n",
    "import importlib\n",
    "import visualizer\n",
    "import eval_fi\n",
    "import model as mdl\n",
    "import loss\n",
    "\n",
    "importlib.reload(visualizer)\n",
    "importlib.reload(eval_fi)\n",
    "importlib.reload(mdl)\n",
    "importlib.reload(loss)\n",
    "importlib.reload(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-extreme",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
