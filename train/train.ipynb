{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video import Video\n",
    "\n",
    "video_path = \"/home/sagarpatel/Desktop/ece496-capstone/train/sample/video.mp4\"\n",
    "transcript_path = \"/home/sagarpatel/Desktop/ece496-capstone/train/sample/transcript.vtt\"\n",
    "transcript2_path = \"/home/sagarpatel/Desktop/ece496-capstone/train/sample/transcript2.vtt\"\n",
    "\n",
    "v = Video(video_path, transcript_path)\n",
    "v.align()\n",
    "v.generate_frames(\"sample\", swap=True)\n",
    "\n",
    "v2 = Video(video_path, transcript2_path)\n",
    "v2.align()\n",
    "v2.generate_frames(\"sample\", swap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /home/sagarpatel/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import itertools\n",
    "\n",
    "from loss import loss_RA_MIL\n",
    "from detector import Detector\n",
    "from parser import parse\n",
    "\n",
    "detector = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(\"/home/sagarpatel/Desktop/ece496-capstone/train/sample/*.png\"))\n",
    "candidates = [detector.inference(image, max_detections=5) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL = '[unused1]'\n",
    "PAD = '[unused2]'\n",
    "ENTITY = '[unused3]'\n",
    "ACTION = '[SEP]'\n",
    "\n",
    "MAX_STEP_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1 = [step.text.strip() for step in v.steps]\n",
    "steps2 = [step.text.strip() for step in v2.steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "\n",
    "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\", pad_token=PAD)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "tokenizer.encode([NULL, PAD, ENTITY], add_special_tokens=True)\n",
    "\n",
    "tokens_steps1 = tokenizer(\n",
    "                    steps1,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "tokens_steps2 = tokenizer(\n",
    "                    steps2,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 18651,     4,  1996, 12851,  1999,     4,  1037,  6090,  1012,\n",
       "          102,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,   101,  5587,     4,  3514,  2046,     4,  1996,  6090,\n",
       "         1012,   102,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,   101,  5660,     4,  1996, 11611,  1012,\n",
       "          102,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,   101,  3659,     4,  2070,\n",
       "        14415,  9516,  5562,  3031,     4,  1996,  7852,  1012,   102,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,   101,  2173,\n",
       "            4,  1037,  3538,  1997,     4,  2292,  8525,  3401,  3031,     4,\n",
       "         2009,  1012,   102,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "          101,  2173,     4,  1996, 12851,  2058,     4,  2009,  1012,   102,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,   101, 11867,  6657, 19099,     4,  2070,  5474,  1998,\n",
       "        11565,  3031,     4,  2009,  1012,   102,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,   101,  2173,     4,  1996, 11611,  2012,\n",
       "            4,  1996,  2327,  1012,   102,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,   101,  2173,  1996,     4,\n",
       "         3538,  1997,  7852,  2012,  1996,  2327,  1012,   102,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_steps1['input_ids'].flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1_flat = tokens_steps1['input_ids'].flatten()\n",
    "steps1_flat = steps1_flat[steps1_flat != 101]\n",
    "steps1_flat = tokenizer.decode(steps1_flat) + ' ' + NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps2_flat = tokens_steps2['input_ids'].flatten()\n",
    "steps2_flat = steps2_flat[steps2_flat != 101]\n",
    "steps2_flat = tokenizer.decode(steps2_flat) + ' ' + NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all of the bounding boxes for the detections and their features.\n",
    "boxes = torch.tensor([candidate[0].numpy() for candidate in candidates]).squeeze(1)\n",
    "features = torch.tensor([candidate[1].numpy() for candidate in candidates]).squeeze(1)\n",
    "\n",
    "# Find all of the entities in each of the sentences and their ordinal positions.\n",
    "# entities, indices = parse(steps, max_step_length=10)\n",
    "\n",
    "# We need to add a dummy step that can be referred to as NULL.\n",
    "# entities.append([\"[unused1]\"])\n",
    "# indices.append([len(steps) * 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 4])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = boxes.flatten(start_dim=0, end_dim=1)\n",
    "features = features.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "boxes = torch.stack((boxes, boxes))\n",
    "features = torch.stack((features, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 2048])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "lxmert_tokenizer.encode([NULL, PAD, ENTITY])\n",
    "\n",
    "lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = lxmert_tokenizer(\n",
    "            [steps1_flat, steps2_flat],\n",
    "            padding=\"max_length\",\n",
    "            max_length=(30 + 1) * 9 + 3,\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "output = lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features,\n",
    "            visual_pos=boxes,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(NULL)\n",
    "ENTITY_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(ENTITY)\n",
    "ACTION_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(ACTION)\n",
    "\n",
    "entity_idx = ((token_ids == ENTITY_TOKEN) | (token_ids == NULL_TOKEN))\n",
    "action_idx = (token_ids == ACTION_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = output['language_output'][entity_idx]\n",
    "action_embeddings = output['language_output'][action_idx]\n",
    "vision_embeddings = output['vision_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 10\n",
    "BATCH_SIZE = 2\n",
    "CANDIDATES = 5\n",
    "EMBEDDING_SIZE = 768\n",
    "\n",
    "ENTITIES_COUNT = [\n",
    "    [2, 2, 1, 2, 3, 2, 2, 2, 1, 1],\n",
    "    [1, 1, 1, 1, 3, 2, 1, 2, 1, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sizes = torch.tensor(ENTITIES_COUNT).flatten().tolist()\n",
    "entities = entity_embeddings.split(split_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "E = pad_sequence(entities, batch_first=True)\n",
    "E = E.reshape(-1, NUM_ACTIONS, E.shape[1], E.shape[2])\n",
    "\n",
    "A = action_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, -1)\n",
    "V = vision_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, CANDIDATES, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss_E.\n",
    "\n",
    "loss_E = E\n",
    "\n",
    "# Calculate VG (VG_scores_index) and loss_V.\n",
    "\n",
    "VG_scores = torch.einsum('bacs, baes -> baec', V, E)\n",
    "VG_scores_max, VG_scores_index = VG_scores.max(dim=-1)\n",
    "\n",
    "V_flat = V.reshape(-1, EMBEDDING_SIZE)\n",
    "\n",
    "VG_scores_index_flat = VG_scores_index.flatten()\n",
    "offsets = torch.arange(0, BATCH_SIZE * NUM_ACTIONS * CANDIDATES, CANDIDATES)\n",
    "offsets = offsets.repeat_interleave(max_entities)\n",
    "\n",
    "VG_scores_index_flat = VG_scores_index_flat + offsets\n",
    "\n",
    "loss_V = V_flat[VG_scores_index_flat, :].reshape(BATCH_SIZE, NUM_ACTIONS, max_entities, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RR (RR_scores_index).\n",
    "\n",
    "\n",
    "RR_scores = torch.einsum('baes, bcs -> baec', E, A)\n",
    "\n",
    "edge_mask = torch.ones(NUM_ACTIONS, NUM_ACTIONS).tril(diagonal=-1)\n",
    "edge_mask[-1, :] = 0\n",
    "edge_mask[:, -1] = 1\n",
    "edge_mask = einops.repeat(edge_mask, 'x y -> b x c y', b=BATCH_SIZE, c=max_entities)\n",
    "\n",
    "RR_scores_max, RR_scores_index = (RR_scores * edge_mask).max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss_Y.\n",
    "\n",
    "loss_Y = np.ones((BATCH_SIZE, NUM_ACTIONS, NUM_ACTIONS))\n",
    "\n",
    "dim_1 = RR_scores_index.reshape(BATCH_SIZE, -1)\n",
    "dim_2 = torch.arange(NUM_ACTIONS).repeat_interleave(max_entities)\n",
    "dim_2 = einops.repeat(dim_2, 'd -> b d', b=BATCH_SIZE)\n",
    "\n",
    "loss_Y[:, dim_1, dim_2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [steps1_flat, steps2_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(30, 265, 2, 10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-research/bert/issues/635\n",
    "# https://colab.research.google.com/drive/18TyuMfZYlgQ_nXo-tr8LCnzUaoX0KS-h?usp=sharing#scrollTo=W4cZIVrg82ua.\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Model(nn.Module):\n",
    "    NULL = '[unused1]'\n",
    "    PAD = '[unused2]'\n",
    "    ENTITY = '[unused3]'\n",
    "    ACTION = '[SEP]'\n",
    "    \n",
    "    DETECTION_EMBEDDING_SIZE = 2048\n",
    "    OUTPUT_EMBEDDING_SIZE = 768\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "        self.lxmert_tokenizer.encode([NULL, PAD, ENTITY])\n",
    "        \n",
    "        self.NULL_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(NULL)\n",
    "        self.ENTITY_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(ENTITY)\n",
    "        self.ACTION_TOKEN = lxmert_tokenizer.convert_tokens_to_ids(ACTION)\n",
    "        \n",
    "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        \n",
    "    def forward(self, MAX_INSTRUCTION_LENGTH, BATCH_SIZE, NUM_ACTIONS, CANDIDATES, steps, features, boxes, entity_count):\n",
    "        '''\n",
    "            MAX_INSTRUCTION_LENGTH\n",
    "                : maximum number of words in a combined string of steps for a video,\n",
    "                  this instruction string must contain the NULL step.\n",
    "                  \n",
    "                  ex. | E ... E . [SEP] [PAD] [PAD] | .... [SEP] [PAD] | NULL | \n",
    "                  \n",
    "            steps\n",
    "                : batched video steps of size (BATCH_SIZE, NUM_ACTIONS), list of lists,\n",
    "                  each instructional video must have the same number of steps = NUM_ACTIONS.\n",
    "                \n",
    "                  ex. [['step 1.1', 'step 1.2'], ['step 2.1', 'step 2.2']]\n",
    "                \n",
    "            features\n",
    "                : batched detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 2048)\n",
    "                \n",
    "            boxes\n",
    "                : batched bounding boxes of detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 4)\n",
    "                \n",
    "            entity_count:\n",
    "                : number of entities per action, size (BATCH_SIZE, NUM_ACTIONS), list of lists\n",
    "        '''\n",
    "\n",
    "        inputs = lxmert_tokenizer(\n",
    "            steps,\n",
    "            padding=\"max_length\",\n",
    "            max_length= MAX_INSTRUCTION_LENGTH + 2, # [CLS] and [SEP] token\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        output = lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features,\n",
    "            visual_pos=boxes,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        \n",
    "        entity_idx = ((token_ids == self.ENTITY_TOKEN) | (token_ids == self.NULL_TOKEN))\n",
    "        action_idx = (token_ids == self.ACTION_TOKEN)\n",
    "        \n",
    "        entity_embeddings = output['language_output'][entity_idx]\n",
    "        action_embeddings = output['language_output'][action_idx]\n",
    "        vision_embeddings = output['vision_output']\n",
    "        \n",
    "        split_sizes = torch.tensor(entity_count).flatten().tolist()\n",
    "        entities = entity_embeddings.split(split_sizes)\n",
    "        \n",
    "        E = pad_sequence(entities, batch_first=True)\n",
    "        E = E.reshape(-1, NUM_ACTIONS, E.shape[1], E.shape[2])\n",
    "\n",
    "        A = action_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, -1)\n",
    "        V = vision_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, CANDIDATES, -1)\n",
    "        \n",
    "        # Calculate loss_E.\n",
    "        loss_E = E\n",
    "        \n",
    "        # Calculate VG (VG_scores_index) and loss_V.\n",
    "        VG_scores = torch.einsum('bacs, baes -> baec', V, E)\n",
    "        VG_scores_max, VG_scores_index = VG_scores.max(dim=-1)\n",
    "\n",
    "        V_flat = V.reshape(-1, self.OUTPUT_EMBEDDING_SIZE)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index.flatten()\n",
    "        offsets = torch.arange(0, BATCH_SIZE * NUM_ACTIONS * CANDIDATES, CANDIDATES)\n",
    "        offsets = offsets.repeat_interleave(max_entities)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index_flat + offsets\n",
    "\n",
    "        loss_V = V_flat[VG_scores_index_flat, :].reshape(BATCH_SIZE, NUM_ACTIONS, max_entities, -1).shape\n",
    "        \n",
    "        # Calculate RR (RR_scores_index).\n",
    "        RR_scores = torch.einsum('baes, bcs -> baec', E, A)\n",
    "\n",
    "        edge_mask = torch.ones(NUM_ACTIONS, NUM_ACTIONS).tril(diagonal=-1)\n",
    "        edge_mask[-1, :] = 0\n",
    "        edge_mask[:, -1] = 1\n",
    "        edge_mask = einops.repeat(edge_mask, 'x y -> b x c y', b=BATCH_SIZE, c=max_entities)\n",
    "\n",
    "        RR_scores_max, RR_scores_index = (RR_scores * edge_mask).max(dim=-1)\n",
    "        \n",
    "        # Calculate loss_Y.\n",
    "        loss_Y = np.ones((BATCH_SIZE, NUM_ACTIONS, NUM_ACTIONS))\n",
    "\n",
    "        dim_1 = RR_scores_index.reshape(BATCH_SIZE, -1)\n",
    "        dim_2 = torch.arange(NUM_ACTIONS).repeat_interleave(max_entities)\n",
    "        dim_2 = einops.repeat(dim_2, 'd -> b d', b=BATCH_SIZE)\n",
    "\n",
    "        loss_Y[:, dim_1, dim_2] = 0\n",
    "        \n",
    "        return loss_E, loss_V, loss_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs=25, lr=0.001, batch_size=10, y=0.5):\n",
    "    '''\n",
    "    Training loop for the model.\n",
    "    '''\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Ouput losses.\n",
    "    train_loss = np.zeros(epochs)\n",
    "    \n",
    "    # Output accuracies.\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    \n",
    "    m_RR = None\n",
    "    m_VG = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for boxes, features, entities, indices in data:\n",
    "            # Zero out any gradients.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Run inference (forward pass).\n",
    "            _, RR, VG, _, loss_V, loss_E, loss_R, _, _, _  = model(steps, boxes, features, entities, indices)\n",
    "            \n",
    "            # Loss from alignment.\n",
    "            loss = loss_RA_MIL(y, loss_R, loss_E, loss_V)\n",
    "            print(loss)\n",
    "            \n",
    "            # Backpropagation (backward pass).\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters.\n",
    "            optimizer.step()\n",
    "            \n",
    "            m_RR = RR\n",
    "            m_VG = VG\n",
    "            \n",
    "        # TODO: save loss and accuracy at each epoch, plot (and checkpoint).\n",
    "    return m_RR, m_VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(max_step_length=10)\n",
    "data = [(boxes, features, entities, indices)]\n",
    "\n",
    "RR, VG = train(model, data, epochs=50, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
