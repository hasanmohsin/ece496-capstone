{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video import Video\n",
    "\n",
    "video_path = \"/h/mhasan/ece496-capstone/train/sample/video.mp4\"\n",
    "transcript_path = \"/h/mhasan/ece496-capstone/train/sample/transcript.vtt\"\n",
    "transcript2_path = \"/h/mhasan/ece496-capstone/train/sample/transcript2.vtt\"\n",
    "\n",
    "v = Video(video_path, transcript_path)\n",
    "v.align()\n",
    "v.generate_frames(\"sample\", swap=True)\n",
    "\n",
    "v2 = Video(video_path, transcript2_path)\n",
    "v2.align()\n",
    "v2.generate_frames(\"sample\", swap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /h/mhasan/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import itertools\n",
    "\n",
    "from loss import loss_RA_MIL\n",
    "from detector import Detector\n",
    "from parser import parse\n",
    "\n",
    "detector = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'loss' from '/scratch/ssd002/home/mhasan/ece496-capstone/train/loss.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import loss\n",
    "importlib.reload(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(296.7588)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.loss_RA_MIL(0.5, torch.randn(2,5,5), torch.randn(2,5,3,100), torch.randn(2,5,3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(\"/h/mhasan/ece496-capstone/train/sample/*.png\"))\n",
    "candidates = [detector.inference(image, max_detections=5) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL = '[unused1]'\n",
    "PAD = '[unused2]'\n",
    "ENTITY = '[unused3]'\n",
    "ACTION = '[SEP]'\n",
    "\n",
    "MAX_STEP_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1 = [step.text.strip() for step in v.steps]\n",
    "steps2 = [step.text.strip() for step in v2.steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "\n",
    "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\", pad_token=PAD)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "tokenizer.encode([NULL, PAD, ENTITY], add_special_tokens=True)\n",
    "\n",
    "tokens_steps1 = tokenizer(\n",
    "                    steps1,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "tokens_steps2 = tokenizer(\n",
    "                    steps2,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1_flat = tokens_steps1['input_ids'].flatten()\n",
    "steps1_flat = steps1_flat[steps1_flat != 101]\n",
    "steps1_flat = tokenizer.decode(steps1_flat) + ' ' + NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps2_flat = tokens_steps2['input_ids'].flatten()\n",
    "steps2_flat = steps2_flat[steps2_flat != 101]\n",
    "steps2_flat = tokenizer.decode(steps2_flat) + ' ' + NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all of the bounding boxes for the detections and their features.\n",
    "boxes = torch.tensor([candidate[0].numpy() for candidate in candidates]).squeeze(1)\n",
    "features = torch.tensor([candidate[1].numpy() for candidate in candidates]).squeeze(1)\n",
    "\n",
    "boxes = boxes.flatten(start_dim=0, end_dim=1)\n",
    "features = features.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "boxes = torch.stack((boxes, boxes))\n",
    "features = torch.stack((features, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e3d397de7fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [steps1_flat, steps2_flat]\n",
    "\n",
    "ENTITIES_COUNT = [\n",
    "    [2, 2, 1, 2, 3, 2, 2, 2, 1, 1],\n",
    "    [1, 1, 1, 1, 3, 2, 1, 2, 1, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grill [unused3] the tomatoes in [unused3] a pan.',\n",
       " 'Add [unused3] oil into [unused3] the pan.',\n",
       " 'Cook [unused3] the bacon.',\n",
       " 'Spread [unused3] some mayonnaise onto [unused3] the bread.',\n",
       " 'Place [unused3] a piece of [unused3] lettuce onto [unused3] it.',\n",
       " 'Place [unused3] the tomatoes over [unused3] it.',\n",
       " 'Sprinkle [unused3] some salt and pepper onto [unused3] it.',\n",
       " 'Place [unused3] the bacon at [unused3] the top.',\n",
       " 'Place the [unused3] piece of bread at the top.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grill [unused3] the tomatoes.',\n",
       " 'Add [unused3] oil.',\n",
       " 'Cook [unused3] the bacon.',\n",
       " 'Spread [unused3] some mayonnaise.',\n",
       " 'Place [unused3] a piece of [unused3] lettuce onto [unused3] it.',\n",
       " 'Place [unused3] the tomatoes over [unused3] it.',\n",
       " 'Sprinkle some pepper onto [unused3] it.',\n",
       " 'Place [unused3] the bacon at [unused3] the top.',\n",
       " 'Place the [unused3] piece of bread at the top.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-research/bert/issues/635\n",
    "# https://colab.research.google.com/drive/18TyuMfZYlgQ_nXo-tr8LCnzUaoX0KS-h?usp=sharing#scrollTo=W4cZIVrg82ua.\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Model(nn.Module):\n",
    "    NULL = '[unused1]'\n",
    "    PAD = '[unused2]'\n",
    "    ENTITY = '[unused3]'\n",
    "    ACTION = '[SEP]'\n",
    "    \n",
    "    DETECTION_EMBEDDING_SIZE = 2048\n",
    "    OUTPUT_EMBEDDING_SIZE = 768\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.lxmert_tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "        self.lxmert_tokenizer.encode([NULL, PAD, ENTITY])\n",
    "        \n",
    "        self.NULL_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(NULL)\n",
    "        self.ENTITY_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(ENTITY)\n",
    "        self.ACTION_TOKEN = self.lxmert_tokenizer.convert_tokens_to_ids(ACTION)\n",
    "        \n",
    "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        \n",
    "    def forward(self, MAX_INSTRUCTION_LENGTH, BATCH_SIZE, NUM_ACTIONS, CANDIDATES, steps, features, boxes, entity_count):\n",
    "        '''\n",
    "            MAX_INSTRUCTION_LENGTH\n",
    "                : maximum number of words in a combined string of steps for a video,\n",
    "                  this instruction string must contain the NULL step.\n",
    "                  \n",
    "                  ex. | E ... E . [SEP] [PAD] [PAD] | .... [SEP] [PAD] | NULL | \n",
    "                  \n",
    "            steps\n",
    "                : batched video steps of size (BATCH_SIZE, NUM_ACTIONS), list of lists,\n",
    "                  each instructional video must have the same number of steps = NUM_ACTIONS.\n",
    "                \n",
    "                  ex. [['step 1.1', 'step 1.2'], ['step 2.1', 'step 2.2']]\n",
    "                \n",
    "            features\n",
    "                : batched detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 2048)\n",
    "                \n",
    "            boxes\n",
    "                : batched bounding boxes of detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 4)\n",
    "                \n",
    "            entity_count:\n",
    "                : number of entities per action, size (BATCH_SIZE, NUM_ACTIONS), list of lists\n",
    "        '''\n",
    "\n",
    "        inputs = self.lxmert_tokenizer(\n",
    "            steps,\n",
    "            padding=\"max_length\",\n",
    "            max_length= MAX_INSTRUCTION_LENGTH + 2, # [CLS] and [SEP] token\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        output = self.lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features,\n",
    "            visual_pos=boxes,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        \n",
    "        token_ids = inputs.input_ids\n",
    "        \n",
    "        entity_idx = ((token_ids == self.ENTITY_TOKEN) | (token_ids == self.NULL_TOKEN))\n",
    "        action_idx = (token_ids == self.ACTION_TOKEN)\n",
    "        \n",
    "        entity_embeddings = output['language_output'][entity_idx]\n",
    "        action_embeddings = output['language_output'][action_idx]\n",
    "        vision_embeddings = output['vision_output']\n",
    "        \n",
    "        split_sizes = torch.tensor(entity_count).flatten().tolist()\n",
    "        entities = entity_embeddings.split(split_sizes)\n",
    "        \n",
    "        E = pad_sequence(entities, batch_first=True)\n",
    "        max_entities = E.shape[1]\n",
    "        E = E.reshape(-1, NUM_ACTIONS, E.shape[1], E.shape[2])\n",
    "\n",
    "        A = action_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, -1)\n",
    "        V = vision_embeddings.reshape(BATCH_SIZE, NUM_ACTIONS, CANDIDATES, -1)\n",
    "        \n",
    "        # Calculate loss_E.\n",
    "        loss_E = E\n",
    "        \n",
    "        # Calculate VG (VG_scores_index) and loss_V.\n",
    "        VG_scores = torch.einsum('bacs, baes -> baec', V, E)\n",
    "        VG_scores_max, VG_scores_index = VG_scores.max(dim=-1)\n",
    "\n",
    "        V_flat = V.reshape(-1, self.OUTPUT_EMBEDDING_SIZE)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index.flatten()\n",
    "        offsets = torch.arange(0, BATCH_SIZE * NUM_ACTIONS * CANDIDATES, CANDIDATES)\n",
    "        offsets = offsets.repeat_interleave(max_entities)\n",
    "\n",
    "        VG_scores_index_flat = VG_scores_index_flat + offsets\n",
    "\n",
    "        loss_V = V_flat[VG_scores_index_flat, :].reshape(BATCH_SIZE, NUM_ACTIONS, max_entities, -1)\n",
    "        \n",
    "        # Calculate RR (RR_scores_index).\n",
    "        #note: we write c here as number of actions as well\n",
    "        #since we don't wish to sum over a \n",
    "        RR_scores = torch.einsum('baes, bcs -> baec', E, A)\n",
    "\n",
    "        edge_mask = torch.ones(NUM_ACTIONS, NUM_ACTIONS).tril(diagonal=-1)\n",
    "        edge_mask[-1, :] = 0\n",
    "        edge_mask[:, -1] = 1\n",
    "        edge_mask = einops.repeat(edge_mask, 'x y -> b x c y', b=BATCH_SIZE, c=max_entities)\n",
    "\n",
    "        RR_scores_max, RR_scores_index = (RR_scores * edge_mask).max(dim=-1)\n",
    "        \n",
    "        # Calculate loss_R.\n",
    "        loss_R = torch.ones((BATCH_SIZE, NUM_ACTIONS, NUM_ACTIONS))\n",
    "\n",
    "        dim_1 = RR_scores_index.reshape(BATCH_SIZE, -1)\n",
    "        dim_2 = torch.arange(NUM_ACTIONS).repeat_interleave(max_entities)\n",
    "        dim_2 = einops.repeat(dim_2, 'd -> b d', b=BATCH_SIZE)\n",
    "\n",
    "        loss_R[:, dim_1, dim_2] = 0.0\n",
    "        \n",
    "        #entity embeddings, selected visual grounding embeddings, adjacency list for\n",
    "        #ref resolution\n",
    "        return loss_E, loss_V, loss_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=25, lr=0.001, batch_size=10, y=0.5):\n",
    "    '''\n",
    "    Training loop for the model.\n",
    "    '''\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Ouput losses.\n",
    "    train_loss = np.zeros(epochs)\n",
    "    \n",
    "    # Output accuracies.\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    \n",
    "    m_RR = None\n",
    "    m_VG = None\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Zero out any gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run inference (forward pass).\n",
    "        loss_E, loss_V, loss_R = model(265, 2, 10, 5, steps, features, boxes, ENTITIES_COUNT)\n",
    "\n",
    "        # Loss from alignment.\n",
    "        loss_ = loss.loss_RA_MIL(y, loss_R, loss_E, loss_V)\n",
    "        print(loss_)\n",
    "\n",
    "        # Backpropagation (backward pass).\n",
    "        loss_.backward()\n",
    "\n",
    "        # Update parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        #m_RR = RR\n",
    "        #m_VG = VG\n",
    "            \n",
    "        # TODO: save loss and accuracy at each epoch, plot (and checkpoint).\n",
    "        train_loss[epoch] = loss_\n",
    "        \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(397.5083, grad_fn=<SumBackward0>)\n",
      "tensor(3777.6223, grad_fn=<SumBackward0>)\n",
      "tensor(1839.2795, grad_fn=<SumBackward0>)\n",
      "tensor(1486.8861, grad_fn=<SumBackward0>)\n",
      "tensor(1778.6123, grad_fn=<SumBackward0>)\n",
      "tensor(1102.3772, grad_fn=<SumBackward0>)\n",
      "tensor(1398.0608, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "train_loss = train(model, epochs=50, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lxmert): LxmertModel(\n",
       "    (embeddings): LxmertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LxmertEncoder(\n",
       "      (visn_fc): LxmertVisualFeatureEncoder(\n",
       "        (visn_fc): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (visn_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (box_fc): Linear(in_features=4, out_features=768, bias=True)\n",
       "        (box_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer): ModuleList(\n",
       "        (0): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (x_layers): ModuleList(\n",
       "        (0): LxmertXLayer(\n",
       "          (visual_attention): LxmertCrossAttentionLayer(\n",
       "            (att): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (visn_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (lang_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (visn_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (visn_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LxmertXLayer(\n",
       "          (visual_attention): LxmertCrossAttentionLayer(\n",
       "            (att): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (visn_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (lang_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (visn_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (visn_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LxmertXLayer(\n",
       "          (visual_attention): LxmertCrossAttentionLayer(\n",
       "            (att): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (visn_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (lang_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (visn_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (visn_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LxmertXLayer(\n",
       "          (visual_attention): LxmertCrossAttentionLayer(\n",
       "            (att): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (visn_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (lang_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (visn_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (visn_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LxmertXLayer(\n",
       "          (visual_attention): LxmertCrossAttentionLayer(\n",
       "            (att): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (visn_self_att): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (lang_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (lang_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (visn_inter): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (visn_output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (r_layers): ModuleList(\n",
       "        (0): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LxmertLayer(\n",
       "          (attention): LxmertSelfAttentionLayer(\n",
       "            (self): LxmertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LxmertAttentionOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LxmertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LxmertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LxmertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grill [unused3] the tomatoes in [unused3] a pan. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] oil into [unused3] the pan. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] cook [unused3] the bacon. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] spread [unused3] some mayonnaise onto [unused3] the bread. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] place [unused3] a piece of [unused3] lettuce onto [unused3] it. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] place [unused3] the tomatoes over [unused3] it. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] sprinkle [unused3] some salt and pepper onto [unused3] it. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] place [unused3] the bacon at [unused3] the top. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] place the [unused3] piece of bread at the top. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused1]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "loss_E, loss_V, loss_R = model(265, 2, 10, 5, steps, features, boxes, ENTITIES_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 1., 1., 1., 1., 1., 1., 1., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(loss_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 2048])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " MAX_INSTRUCTION_LENGTH, BATCH_SIZE, NUM_ACTIONS, CANDIDATES, steps, features, boxes, entity_count):\n",
    "        '''\n",
    "            MAX_INSTRUCTION_LENGTH\n",
    "                : maximum number of words in a combined string of steps for a video,\n",
    "                  this instruction string must contain the NULL step.\n",
    "                  \n",
    "                  ex. | E ... E . [SEP] [PAD] [PAD] | .... [SEP] [PAD] | NULL | \n",
    "                  \n",
    "            steps\n",
    "                : batched video steps of size (BATCH_SIZE, NUM_ACTIONS), list of lists,\n",
    "                  each instructional video must have the same number of steps = NUM_ACTIONS.\n",
    "                \n",
    "                  ex. [['step 1.1', 'step 1.2'], ['step 2.1', 'step 2.2']]\n",
    "                \n",
    "            features\n",
    "                : batched detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 2048)\n",
    "                \n",
    "            boxes\n",
    "                : batched bounding boxes of detection features of size (BATCH_SIZE, CANDIDATES * NUM_ACTIONS, 4)\n",
    "                \n",
    "            entity_count:\n",
    "                : number of entities per action, size (BATCH_SIZE, NUM_ACTIONS), list of lists\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "        inputs = tokenizer(\n",
    "            steps,\n",
    "            padding=\"max_length\",\n",
    "            max_length= 265 + 2, # [CLS] and [SEP] token\n",
    "            truncation=False,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(steps[1].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 18651,     4,  1996, 12851,  1999,     4,  1037,  6090,  1012,\n",
       "           102,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,  5587,     4,  3514,  2046,     4,  1996,  6090,  1012,\n",
       "           102,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,  5660,     4,  1996, 11611,  1012,   102,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,  3659,     4,  2070, 14415,  9516,  5562,\n",
       "          3031,     4,  1996,  7852,  1012,   102,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,  2173,     4,  1037,  3538,  1997,\n",
       "             4,  2292,  8525,  3401,  3031,     4,  2009,  1012,   102,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,  2173,     4,  1996, 12851,\n",
       "          2058,     4,  2009,  1012,   102,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3, 11867,  6657, 19099,\n",
       "             4,  2070,  5474,  1998, 11565,  3031,     4,  2009,  1012,   102,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,  2173,     4,\n",
       "          1996, 11611,  2012,     4,  1996,  2327,  1012,   102,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,  2173,\n",
       "          1996,     4,  3538,  1997,  7852,  2012,  1996,  2327,  1012,   102,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             2,   102],\n",
       "        [  101, 18651,     4,  1996, 12851,  1012,   102,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,  5587,     4,  3514,  1012,   102,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,  5660,     4,  1996, 11611,  1012,   102,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,  3659,     4,  2070, 14415,  9516,  5562,\n",
       "          1012,   102,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,  2173,     4,  1037,  3538,  1997,\n",
       "             4,  2292,  8525,  3401,  3031,     4,  2009,  1012,   102,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,  2173,     4,  1996, 12851,\n",
       "          2058,     4,  2009,  1012,   102,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3, 11867,  6657, 19099,\n",
       "          2070, 11565,  3031,     4,  2009,  1012,   102,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,  2173,     4,\n",
       "          1996, 11611,  2012,     4,  1996,  2327,  1012,   102,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,  2173,\n",
       "          1996,     4,  3538,  1997,  7852,  2012,  1996,  2327,  1012,   102,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             2,   102]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_steps = tokenizer.batch_decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded_steps[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "        output = lxmert(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            visual_feats=features,\n",
    "            visual_pos=boxes,\n",
    "            token_type_ids=inputs.token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 282, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['language_output'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS=10\n",
    "BATCH_SIZE=2\n",
    "\n",
    "edge_mask = torch.ones(NUM_ACTIONS, NUM_ACTIONS).tril(diagonal=-1)\n",
    "edge_mask[-1, :] = 0\n",
    "edge_mask[:, -1] = 1\n",
    "edge_mask = einops.repeat(edge_mask, 'x y -> b x c y', b=BATCH_SIZE, c=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 1., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.diagonal(dim1 = 1, dim2= 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
