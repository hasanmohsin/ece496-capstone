{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thie notebook processes video files specified in the\n",
    "# \"Finding \"It\": Weakly-Supervised Reference-Aware\n",
    "# Visual Grounding in Instructional Videos\" paper (FI),\n",
    "# and extracts key frames from the annotated test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/h/sagar/ece496-capstone/train/utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import webvtt\n",
    "\n",
    "from detector import Detector\n",
    "from parser import parse\n",
    "from transformers import LxmertModel, LxmertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_dl_custom.py\n",
    "# Embedded youtube-dl script adapted from:\n",
    "# https://github.com/ytdl-org/youtube-dl/blob/master/README.md\n",
    "# Needed to download FI dataset with specific youtube-dl opts\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import youtube_dl\n",
    "import os\n",
    "\n",
    "class MyLogger(object):\n",
    "    def debug(self, msg):\n",
    "        pass\n",
    "    def warning(self, msg):\n",
    "        pass\n",
    "    def error(self, msg):\n",
    "        print('[INFO] Cannot download video')\n",
    "        print(msg)\n",
    "\n",
    "def my_hook(d):\n",
    "    if d['status'] == 'finished':\n",
    "#         print('[INFO] Downloaded video')\n",
    "        pass\n",
    "\n",
    "def youtube_dl_wrapper(vid_id, video_dir):\n",
    "    # ydl_opts adapted from:\n",
    "    # https://finding-it.github.io/README.txt\n",
    "    ydl_opts = {\n",
    "        'ignoreerrors': True,\n",
    "        'writesubtitles':False,\n",
    "        'format': '18',\n",
    "        'noplaylist' : True,\n",
    "        'outtmpl': os.path.join(video_dir, '%(id)s.%(ext)s'),\n",
    "        'logger': MyLogger(),\n",
    "        'progress_hooks': [my_hook],\n",
    "    }\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download(['https://www.youtube.com/watch?v=' + vid_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path='output.json'):\n",
    "    \"\"\"\n",
    "    Check for valid JSON format and read content\n",
    "    path: path to JSON file\n",
    "    \"\"\"\n",
    "    file = open(path)\n",
    "    line = file.read().replace('\\n', ' ')\n",
    "    file.close()\n",
    "    try:\n",
    "        parsed_json = json.loads(line)\n",
    "    except:\n",
    "        assert False, 'Invalid JSON'\n",
    "    return parsed_json\n",
    "\n",
    "def get_vid_ext(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Returns video file extension\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    vid_prefix = os.path.join(video_dir, vid_id)\n",
    "    if os.path.exists(vid_prefix+'.mp4'):\n",
    "        return '.mp4'\n",
    "    elif os.path.exists(vid_prefix+'.mkv'):\n",
    "        return '.mkv'\n",
    "    elif os.path.exists(vid_prefix+'.webm'):\n",
    "        return '.webm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(vid_id, video_dir, frame_dir, fps=5):\n",
    "    \"\"\"\n",
    "    Sample video into frames at fixed fps\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    frame_dir: directory path to video frames\n",
    "    fps: fps for frame extraction\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(frame_dir):\n",
    "        os.mkdir(frame_dir)\n",
    "    vid_ext = get_vid_ext(vid_id, video_dir)\n",
    "    ff_command = 'ffmpeg -i {}/{}{} -y -an -qscale 0 -vf fps={} {}/%06d.jpg'.format(video_dir, vid_id, vid_ext, fps, frame_dir)\n",
    "    os.system(ff_command)\n",
    "\n",
    "\n",
    "def remove_video(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Delete video\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    vid_prefix = os.path.join(video_dir, vid_id)\n",
    "    vid_ext = get_vid_ext(vid_id, video_dir)\n",
    "    os.remove(vid_prefix+vid_ext)\n",
    "\n",
    "\n",
    "# # Note: Different for FI and YCII datasets\n",
    "# def select_frames(vid_id, frame_dir, vg_annotations):\n",
    "#     \"\"\"\n",
    "#     Returns representative frames for actions and removes unused frames\n",
    "#     vid_id: video id\n",
    "#     frame_dir: directory path to video frames\n",
    "#     vg_annotations: dictionary of visual grounding annotations\n",
    "#     Returns required_frames: set contataining names of representative frames\n",
    "#     \"\"\"\n",
    "#     required_frames = set()\n",
    "\n",
    "#     for entity in vg_annotations[vid_id]:\n",
    "#         for bb in vg_annotations[vid_id][entity]['bboxes']:\n",
    "#             required_frames.add(bb['img'].split('/')[1].split('.')[0] + '.jpg')\n",
    "\n",
    "#     curr_frame_list = os.listdir(os.path.join(frame_dir, vid_id))\n",
    "#     for frame in curr_frame_list:\n",
    "#         if frame not in required_frames:\n",
    "#             os.remove(os.path.join(frame_dir, vid_id, frame))\n",
    "#     return required_frames\n",
    "\n",
    "\n",
    "def select_frames(timestamps, num_frames_per_step):\n",
    "    \"\"\"\n",
    "    Return representative frames for actions\n",
    "    timestamps: list of timestamp tuples (start, end), in seconds, for each action step\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    Return required_frames: list of lists of strings contataining names of representative frames for each step\n",
    "    Return required_frames_set: set contataining names of representative frames\n",
    "    \"\"\"\n",
    "    required_frames_set = set()\n",
    "    required_frames = []\n",
    "    for idx, timestamp in enumerate(timestamps):\n",
    "        required_frames.append([])\n",
    "        action_start = timestamp[0]\n",
    "        action_end = timestamp[1]\n",
    "        # action_delta = (action_end - action_start) / (num_frames_per_step + 1)    # need num_frames_per_step+1 intervals for num_frames_per_step inner frames\n",
    "        action_delta = (action_end - action_start) / (num_frames_per_step - 1)    # take outer frames for consistency with FI dataset (frame--interval--frame--interval--frame)\n",
    "        for i in range(num_frames_per_step):\n",
    "            # frame_time = action_start + action_delta * (i+1)    # in seconds\n",
    "            frame_time = action_start + action_delta * i    # in seconds\n",
    "            # frame_id = int( frame_time*(num_frames_per_step + 1) )\n",
    "            frame_id = int( frame_time*(num_frames_per_step) ) + 1\n",
    "            frame_name = '{}.jpg'.format(str(frame_id).zfill(6))\n",
    "            required_frames[idx].append(frame_name)\n",
    "            required_frames_set.add(frame_name)\n",
    "    return required_frames, required_frames_set\n",
    "\n",
    "\n",
    "def remove_frames(frame_dir, required_frames):\n",
    "    \"\"\"\n",
    "    Remove unused frames\n",
    "    frame_dir: directory path to video frames\n",
    "    required_frames: set contataining names of representative frames\n",
    "    \"\"\"\n",
    "    if os.path.isdir(frame_dir):\n",
    "        curr_frames = os.listdir(frame_dir)\n",
    "        for frame in curr_frames:\n",
    "            if frame not in required_frames:\n",
    "                os.remove(os.path.join(frame_dir, frame))\n",
    "\n",
    "\n",
    "def pickle_data(data, pickles_dir, fname):\n",
    "    \"\"\"\n",
    "    Pickle data into bytestreams\n",
    "    data: data to be pickled\n",
    "    pickles_dir: directory path to pickled data\n",
    "    fname: name of pickled file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(pickles_dir):\n",
    "        os.mkdir(pickles_dir)\n",
    "    pickle_out = open(os.path.join(pickles_dir, fname+'.pickle'), 'wb')\n",
    "    pickle.dump(data, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "\n",
    "def depickle_data(pickles_dir, fname):\n",
    "    \"\"\"\n",
    "    Depickle data from bytestreams\n",
    "    pickles_dir: directory path to pickled data\n",
    "    fname: name of pickled file\n",
    "    Return data: depickled data\n",
    "    \"\"\"\n",
    "    pickle_path = os.path.join(pickles_dir, fname+'.pickle')\n",
    "    if os.path.exists(pickle_path):\n",
    "        pickle_in = open(pickle_path, 'rb')\n",
    "        data = pickle.load(pickle_in)\n",
    "        return data\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fi_vidlists(dataset_root='/h/mkhan/ece496-capstone/datasets'):\n",
    "    \"\"\"\n",
    "    Generate video lists for FI dataset splits by reading from subsets.json\n",
    "    dataset_root: directory path to dataset base\n",
    "    \"\"\"\n",
    "    fi_root = os.path.join(dataset_root, 'fi_datasets')\n",
    "#     if not os.path.isdir(fi_root):\n",
    "#         os.mkdir(fi_root)\n",
    "\n",
    "    subsets = read_json(os.path.join(fi_root, 'subsets.json'))   # Subsets: {all:58,simple:23,medium:12,hard:23}\n",
    "    subset_all = subsets['ycii']['eval_recipes_all']\n",
    "    subset_simple = subsets['ycii']['eval_recipes_simple']\n",
    "    subset_medium = subsets['ycii']['eval_recipes_medium']\n",
    "    subset_hard = subsets['ycii']['eval_recipes_hard']\n",
    "\n",
    "    rr_base = os.path.join(fi_root, 'YCII/RR')  # directory path to reference resolution annotations\n",
    "\n",
    "    with open(os.path.join(fi_root, 'fi_all.txt'), 'w') as f:\n",
    "        for sample in subset_all:\n",
    "            vid_id = os.listdir(os.path.join(rr_base, sample))[0].split('.')[0]  # transcript_path: <rr_base>/<sample>/<vid_id>.en.vtt\n",
    "            f.write(vid_id + '\\n')\n",
    "\n",
    "    with open(os.path.join(fi_root, 'fi_simple.txt'), 'w') as f:\n",
    "        for sample in subset_simple:\n",
    "            vid_id = os.listdir(os.path.join(rr_base, sample))[0].split('.')[0]  # transcript_path: <rr_base>/<sample>/<vid_id>.en.vtt\n",
    "            f.write(vid_id + '\\n')\n",
    "\n",
    "    with open(os.path.join(fi_root, 'fi_medium.txt'), 'w') as f:\n",
    "        for sample in subset_medium:\n",
    "            vid_id = os.listdir(os.path.join(rr_base, sample))[0].split('.')[0]  # transcript_path: <rr_base>/<sample>/<vid_id>.en.vtt\n",
    "            f.write(vid_id + '\\n')\n",
    "\n",
    "    with open(os.path.join(fi_root, 'fi_hard.txt'), 'w') as f:\n",
    "        for sample in subset_hard:\n",
    "            vid_id = os.listdir(os.path.join(rr_base, sample))[0].split('.')[0]  # transcript_path: <rr_base>/<sample>/<vid_id>.en.vtt\n",
    "            f.write(vid_id + '\\n')\n",
    "\n",
    "\n",
    "def generate_ycii_vidlists(dataset_root='/h/mkhan/ece496-capstone/datasets'):\n",
    "    \"\"\"\n",
    "    Generate video lists for YCII dataset splits\n",
    "    dataset_root: directory path to dataset base\n",
    "    \"\"\"\n",
    "    vidlist_root = os.path.join(dataset_root, 'vid_list')\n",
    "    subset_train = os.path.join(vidlist_root, 'train_list.txt')    # Subsets: {train:1333,val:457,test:210}\n",
    "    subset_val = os.path.join(vidlist_root, 'val_list.txt')        # Subsets: {train:1333,val:457,test:210}\n",
    "    subset_test = os.path.join(vidlist_root, 'test_list.txt')      # Subsets: {train:1333,val:457,test:210}\n",
    "\n",
    "    ycii_train = []\n",
    "    with open(subset_train, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rcp_type,vid_id = line.replace('\\n','').split('/')\n",
    "            ycii_train.append(vid_id)\n",
    "\n",
    "    with open(os.path.join(vidlist_root, 'ycii_train.txt'), 'w') as f:\n",
    "        for vid_id in ycii_train:\n",
    "            f.write(vid_id + '\\n')\n",
    "\n",
    "    ycii_val = []\n",
    "    with open(subset_val, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rcp_type,vid_id = line.replace('\\n','').split('/')\n",
    "            ycii_val.append(vid_id)\n",
    "\n",
    "    with open(os.path.join(vidlist_root, 'ycii_val.txt'), 'w') as f:\n",
    "        for vid_id in ycii_val:\n",
    "            f.write(vid_id + '\\n')\n",
    "\n",
    "    ycii_test = []\n",
    "    with open(subset_test, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rcp_type,vid_id = line.replace('\\n','').split('/')\n",
    "            ycii_test.append(vid_id)\n",
    "\n",
    "    with open(os.path.join(vidlist_root, 'ycii_test.txt'), 'w') as f:\n",
    "        for vid_id in ycii_test:\n",
    "            f.write(vid_id + '\\n')\n",
    "\n",
    "\n",
    "def remove_duplicates(fpath, orig_set):\n",
    "    \"\"\"\n",
    "    Remove duplicates from fpath file that are within orig_set\n",
    "    fpath: file path from where to remove lines\n",
    "    orig_set: set of orignal lines that should not occur in fpath\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            item = line.replace('\\n','')\n",
    "            all_items.append(item)\n",
    "    print(\"Length before: \" + str(len(all_items)))\n",
    "\n",
    "    with open(fpath, 'w') as f:\n",
    "        for item in all_items:\n",
    "            if item not in orig_set:\n",
    "                f.write(item + '\\n')\n",
    "            # else:\n",
    "                # print(item)\n",
    "    print(\"Length after: \" + str(len(open(fpath, 'r').readlines())))\n",
    "\n",
    "\n",
    "def clean_vidlists(dataset_root='/h/mkhan/ece496-capstone/datasets'):\n",
    "    \"\"\"\n",
    "    Remove duplicates from YCII dataset splits that are in FI validation set\n",
    "    dataset_root: directory path to dataset base\n",
    "    \"\"\"\n",
    "    fi_all = set()\n",
    "    fi_root = os.path.join(dataset_root, 'fi_datasets')\n",
    "    vidlist_root = os.path.join(dataset_root, 'vid_list')\n",
    "\n",
    "    fpath = os.path.join(fi_root, 'fi_all.txt')\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            vid_id = line.replace('\\n','')\n",
    "            fi_all.add(vid_id)\n",
    "\n",
    "    # Clean YCII train set\n",
    "    remove_duplicates(os.path.join(vidlist_root, 'ycii_train.txt'), fi_all) # 1333 --> 1290\n",
    "    remove_duplicates(os.path.join(vidlist_root, 'ycii_val.txt'), fi_all)   # 457 --> 444\n",
    "\n",
    "    # Note: since YCII test split does not provide annotations,\n",
    "    # the overlapping samples should be removed from FI dataset instead of from the YCII split\n",
    "    ycii_test = set()\n",
    "    fpath = os.path.join(vidlist_root, 'ycii_test.txt')\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            vid_id = line.replace('\\n','')\n",
    "            ycii_test.add(vid_id)\n",
    "    # remove_duplicates(os.path.join(dataset_root, 'ycii_test.txt'), fi_all)  # 210 --> 208\n",
    "    remove_duplicates(os.path.join(fi_root, 'fi_all.txt'), ycii_test)  # 58 --> 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import re\n",
    "\n",
    "from ling_obj_classes import *\n",
    "\n",
    "# Generate entity objects from (entity text, action ID) pairs.\n",
    "def make_entities(raw_entities, entity_type):\n",
    "    raw_entities = raw_entities.split(',')\n",
    "    raw_entities = [raw_entity.strip() for raw_entity in raw_entities]\n",
    "    \n",
    "    entities = []\n",
    "\n",
    "    for raw_entity in raw_entities:\n",
    "        split = raw_entity.rsplit(' ', 1)\n",
    "        action_id = re.search('\\((.*\\d*)\\)', raw_entity)\n",
    "        \n",
    "        # Sometimes entity texts or action IDs are null.\n",
    "        if action_id and len(split) == 2:\n",
    "            entities.append(Entity(split[0], entity_type, action_id.group(1)))\n",
    "        elif action_id:\n",
    "            entities.append(Entity(None, entity_type, action_id.group(1)))\n",
    "        else:\n",
    "            entities.append(Entity(raw_entity, entity_type, None))\n",
    "            \n",
    "    return entities\n",
    "\n",
    "#given the reference resolution vtt file name, parses into step text list + \n",
    "#action_step list (of type ActionStep, containing extracted entities) \n",
    "def parse_rr_vtt(rr_vtt_file_name):\n",
    "    annotations = []\n",
    "    action_steps = []\n",
    "\n",
    "    #sample file\n",
    "    #file = '/content/fi/YCII/RR/101/'+id+'.en.vtt'\n",
    "\n",
    "    #parse file\n",
    "    # Generate action steps.\n",
    "    for caption in webvtt.read(rr_vtt_file_name):\n",
    "        annotation = re.search('annot: (.*)\\n?', caption.text).group(1)\n",
    "        action_id = re.search('ACTID: (.*)\\n?', caption.text).group(1)\n",
    "        predicate = re.search('PRED: (.*)\\n?', caption.text).group(1)\n",
    "        \n",
    "        # Number of direct objects ∈ [0, inf).\n",
    "        direct_objects = re.search('\\[DOBJ, .*\\] (.*)?', caption.text)\n",
    "        if direct_objects:\n",
    "            direct_objects = make_entities(','.join(direct_objects.groups()), 'DOBJ')\n",
    "\n",
    "        # Number of propositional phreases ∈ [0, inf).\n",
    "        prop_phrases = re.search('\\[PP, .*\\] (.*)?', caption.text)\n",
    "        if prop_phrases:\n",
    "            prop_phrases = make_entities(','.join(prop_phrases.groups()), 'PP')\n",
    "                        \n",
    "        annotations.append(annotation)\n",
    "        action_steps.append(ActionStep(action_id, predicate, direct_objects, prop_phrases))\n",
    "            \n",
    "    # Return both raw annotation texts and groudn truth action steps.\n",
    "    return annotations, action_steps\n",
    "\n",
    "def get_entity_list_from_action_steps(action_steps):\n",
    "    action_step_entity_list = []\n",
    "    for actions in action_steps:\n",
    "        ent_list = actions.dobj_list + actions.pp_list\n",
    "        entity_text_list = [ent.ent_text for ent in ent_list if ent] \n",
    "        action_step_entity_list.append(entity_text_list)\n",
    "\n",
    "    return action_step_entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_testset(dataset_root='/h/mkhan/ece496-capstone/datasets', max_detections=5):\n",
    "    \"\"\"\n",
    "    Download and prepare FI dataset files\n",
    "    dataset_root: directory path to dataset base\n",
    "    vid_list: file path to video list\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    max_detections: number of detections per frame\n",
    "    \"\"\"\n",
    "\n",
    "    num_frames_per_step = 5    # for FI dataset, this should be fixed\n",
    "\n",
    "    fi_root = os.path.join(dataset_root, 'fi')\n",
    "    if not os.path.isdir(fi_root):\n",
    "        os.mkdir(fi_root)\n",
    "    \n",
    "    videos_root = os.path.join(dataset_root, 'fi_videos')\n",
    "    if not os.path.isdir(videos_root):\n",
    "        os.mkdir(videos_root)\n",
    "\n",
    "    fi_dataset_root = os.path.join(dataset_root, 'fi_datasets')\n",
    "    annot_root = os.path.join(fi_dataset_root, 'annotations')\n",
    "    if not os.path.isdir(annot_root):\n",
    "        os.mkdir(annot_root)\n",
    "    \n",
    "    missing_vid_list = []\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    detector = Detector(device)\n",
    "    \n",
    "    vg_path = os.path.join(fi_dataset_root, 'YCII/VG/gnding_annot_all.json')  # file path to visual grounding annotations\n",
    "    vg_annotations = read_json(vg_path)\n",
    "\n",
    "    # copy all vtt files to annot_root and name them by vid_id\n",
    "    rr_base = os.path.join(fi_dataset_root, 'YCII/RR')  # directory path to reference resolution annotations\n",
    "    annotation_paths = os.listdir(rr_base)\n",
    "    for annotation_path in annotation_paths:\n",
    "        if annotation_path == 'README.md':\n",
    "            continue\n",
    "        annot_path = os.listdir(os.path.join(rr_base, annotation_path))[0]\n",
    "        os.system('cp {} {}'.format(os.path.join(rr_base, annotation_path, annot_path), os.path.join(annot_root, annot_path)))\n",
    "\n",
    "    # generate files containing list of video id's for different dataset\n",
    "    generate_fi_vidlists(dataset_root)\n",
    "    generate_ycii_vidlists(dataset_root)\n",
    "    clean_vidlists(dataset_root)\n",
    "    print('[INFO] Generated FI video lists')\n",
    "\n",
    "    # load FI video list\n",
    "    fi_all = []\n",
    "    fpath = os.path.join(fi_dataset_root, 'fi_all.txt')\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            vid_id = line.replace('\\n','')\n",
    "            fi_all.append(vid_id)\n",
    "    # print(len(fi_all))  # 58 --> 56\n",
    "\n",
    "    # main video-wise loop\n",
    "    for vid_id in fi_all[34:]:\n",
    "        print('[INFO] Processing video {}'.format(vid_id))\n",
    "\n",
    "        # download the video\n",
    "        youtube_dl_wrapper(vid_id, videos_root)\n",
    "\n",
    "        # check if the video is available\n",
    "        vid_prefix = os.path.join(videos_root, vid_id)\n",
    "        if os.path.exists(vid_prefix+'.mp4') or os.path.exists(vid_prefix+'.mkv') or os.path.exists(vid_prefix+'.webm'):\n",
    "            print('[INFO] Downloaded video {}'.format(vid_id))\n",
    "        else:\n",
    "            missing_vid_list.append(line)\n",
    "            print('[INFO] Cannot download video {}'.format(vid_id))\n",
    "            continue\n",
    "\n",
    "        # get annotations list (and action count)\n",
    "        transcript_path = os.path.join(annot_root, (vid_id + '.en.vtt'))\n",
    "        with open(transcript_path, 'r') as f:\n",
    "            data = f.read().splitlines(True)\n",
    "        if (data[0] == '\\n'):    # remove first line of webvtt files if it's empty\n",
    "            with open(transcript_path, 'w') as f:\n",
    "                f.writelines(data[1:])\n",
    "\n",
    "        actions_list, action_steps = parse_rr_vtt(transcript_path)    # list of action annotations for a single video\n",
    "        actions_count = len(actions_list)\n",
    "\n",
    "        print('[INFO] Extracted {} actions for video {}'.format(actions_count, vid_id))\n",
    "\n",
    "        # setup directories\n",
    "        parent_root = os.path.join(fi_root, str(actions_count))\n",
    "        if not os.path.isdir(parent_root):\n",
    "            os.mkdir(parent_root)\n",
    "\n",
    "        sample_index = 0    # change this to 1 to ensure 1-indexing for samples\n",
    "        samples_list = os.listdir(parent_root)    # list of samples of same actions_count\n",
    "        if samples_list:\n",
    "            sample_index = max([int(index) for index in samples_list]) + 1    # set sample counter to next available integer\n",
    "        sample_index = str(sample_index).zfill(5)    # required to ensure sortability\n",
    "        sample_root = os.path.join(parent_root, sample_index)    # all data for this video will be stored under here\n",
    "        if not os.path.isdir(sample_root):\n",
    "            os.mkdir(sample_root)\n",
    "\n",
    "        frames_root = os.path.join(sample_root, 'frames')    # all sampled images for this video will be under here\n",
    "        if not os.path.isdir(frames_root):\n",
    "            os.mkdir(frames_root)\n",
    "        pickles_root = os.path.join(sample_root, 'pickles')    # all raw data for this video will be under here (stored by variable names)\n",
    "        if not os.path.isdir(pickles_root):\n",
    "            os.mkdir(pickles_root)\n",
    "\n",
    "        # sample frames at fixed fps\n",
    "#         sample_frames(vid_id, videos_root, frames_root, fps=num_frames_per_step+1)    # need num_frames_per_step+1 intervals for num_frames_per_step inner frames\n",
    "        sample_frames(vid_id, videos_root, frames_root, fps=num_frames_per_step)\n",
    "        print('[INFO] Sampled frames for video {}'.format(vid_id))\n",
    "\n",
    "        # remove sampled video file (optional)\n",
    "        remove_video(vid_id, videos_root)\n",
    "        print('[INFO] Removed video {}'.format(vid_id))\n",
    "\n",
    "        # select representative frames for actions\n",
    "        annotation = webvtt.read(transcript_path)\n",
    "        timestamps = []\n",
    "        for annot in annotation:\n",
    "            start_time = sum(factor * int(coef) for factor, coef in zip([3600, 60, 1], annot.start.split('.')[0].split(\":\")))\n",
    "            end_time = sum(factor * int(coef) for factor, coef in zip([3600, 60, 1], annot.end.split('.')[0].split(\":\")))\n",
    "            timestamps.append((start_time, end_time))\n",
    "            # print(annot.start + '-->' + str(start_time) + ' ' + annot.end + '-->' + str(end_time))\n",
    "        required_frames, required_frames_set = select_frames(timestamps, num_frames_per_step=num_frames_per_step)\n",
    "        print('[INFO] Selected frames for video {}'.format(vid_id))\n",
    "        \n",
    "        # remove unsued frames\n",
    "        remove_frames(frames_root, required_frames_set)\n",
    "        print('[INFO] Removed unused frames for video {}'.format(vid_id))\n",
    "\n",
    "        # get candidates for images\n",
    "#         frames = sorted(glob.glob(os.path.join(frames_root, '*.*')))\n",
    "        frame_paths = [os.path.join(frames_root, frame) for action_frames in required_frames for frame in action_frames]\n",
    "        candidates = [detector.inference(frame, max_detections=max_detections) for frame in frame_paths]\n",
    "        print('[INFO] Extracted candidates for video {}'.format(vid_id))\n",
    "\n",
    "        # save pickled files for vid_id\n",
    "        pickle_data(vid_id, pickles_root, 'vid_id')\n",
    "        print('[INFO] Saved vid_id for video {}'.format(vid_id))\n",
    "\n",
    "        # save pickeled files for candidates\n",
    "        pickle_data(candidates, pickles_root, 'candidates')\n",
    "        print('[INFO] Saved candidates for video {}'.format(vid_id))\n",
    "        \n",
    "        # save pickeled files for frame paths\n",
    "        pickle_data(frame_paths, pickles_root, 'frame_paths')\n",
    "        print('[INFO] Saved frame_paths for video {}'.format(vid_id))\n",
    "\n",
    "        # save pickled files for annotations list\n",
    "        pickle_data(actions_list, pickles_root, 'actions_list')\n",
    "        print('[INFO] Saved actions_list for video {}'.format(vid_id))\n",
    "        \n",
    "        # save pickled files for action steps\n",
    "        pickle_data(action_steps, pickles_root, 'action_steps')\n",
    "        print('[INFO] Saved action_steps for video {}'.format(vid_id))\n",
    "    \n",
    "    # write the missing videos to file\n",
    "    missing_vid = open(os.path.join(fi_dataset_root, 'missing_videos.txt'), 'w')\n",
    "    for line in missing_vid_list:\n",
    "        missing_vid.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(dataset_root='/h/mkhan/ece496-capstone/datasets', actions_count=10, sample_index=0):\n",
    "    \"\"\"\n",
    "    Load the sample_index'th sample with actions_count actions from saved files\n",
    "    dataset_root: directory path to dataset base\n",
    "    actions_count: number of actions in sample (bucket id of sample)\n",
    "    sample_index: index of sample within the bucket\n",
    "    Return vid_id: video id\n",
    "    Return candidates: list of candidate data (bboxes, features) for a single video\n",
    "    Return actions_list: list of action annotations for a single video\n",
    "    \"\"\"\n",
    "    pickles_root = os.path.join(dataset_root, 'fi', str(actions_count), str(sample_index).zfill(5), 'pickles')\n",
    "    if not os.path.isdir(pickles_root):\n",
    "        print('[INFO] Cannot load data for {}\\'th sample with {} action(s)'.format(sample_index, actions_count))\n",
    "        return '', [], [], []\n",
    "    else:\n",
    "        vid_id = depickle_data(pickles_root, 'vid_id')\n",
    "        candidates = depickle_data(pickles_root, 'candidates')\n",
    "        actions_list = depickle_data(pickles_root, 'actions_list')\n",
    "        action_steps = depickle_data(pickles_root, 'action_steps')\n",
    "        return vid_id, candidates, actions_list, action_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /h/sagar/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n",
      "Length before: 1333\n",
      "Length after: 1290\n",
      "Length before: 457\n",
      "Length after: 444\n",
      "Length before: 58\n",
      "Length after: 56\n",
      "[INFO] Generated FI video lists\n",
      "[INFO] Processing video a3ZvOvo49WE\n",
      "[INFO] Downloaded video a3ZvOvo49WE\n",
      "[INFO] Extracted 15 actions for video a3ZvOvo49WE\n",
      "[INFO] Sampled frames for video a3ZvOvo49WE\n",
      "[INFO] Removed video a3ZvOvo49WE\n",
      "[INFO] Selected frames for video a3ZvOvo49WE\n",
      "[INFO] Removed unused frames for video a3ZvOvo49WE\n",
      "[INFO] Extracted candidates for video a3ZvOvo49WE\n",
      "[INFO] Saved vid_id for video a3ZvOvo49WE\n",
      "[INFO] Saved candidates for video a3ZvOvo49WE\n",
      "[INFO] Saved frame_paths for video a3ZvOvo49WE\n",
      "[INFO] Saved actions_list for video a3ZvOvo49WE\n",
      "[INFO] Saved action_steps for video a3ZvOvo49WE\n",
      "[INFO] Processing video Ax165ic4b3o\n",
      "[INFO] Downloaded video Ax165ic4b3o\n",
      "[INFO] Extracted 23 actions for video Ax165ic4b3o\n",
      "[INFO] Sampled frames for video Ax165ic4b3o\n",
      "[INFO] Removed video Ax165ic4b3o\n",
      "[INFO] Selected frames for video Ax165ic4b3o\n",
      "[INFO] Removed unused frames for video Ax165ic4b3o\n",
      "[INFO] Extracted candidates for video Ax165ic4b3o\n",
      "[INFO] Saved vid_id for video Ax165ic4b3o\n",
      "[INFO] Saved candidates for video Ax165ic4b3o\n",
      "[INFO] Saved frame_paths for video Ax165ic4b3o\n",
      "[INFO] Saved actions_list for video Ax165ic4b3o\n",
      "[INFO] Saved action_steps for video Ax165ic4b3o\n",
      "[INFO] Processing video 6gObQR5Vm4M\n",
      "[INFO] Downloaded video 6gObQR5Vm4M\n",
      "[INFO] Extracted 10 actions for video 6gObQR5Vm4M\n",
      "[INFO] Sampled frames for video 6gObQR5Vm4M\n",
      "[INFO] Removed video 6gObQR5Vm4M\n",
      "[INFO] Selected frames for video 6gObQR5Vm4M\n",
      "[INFO] Removed unused frames for video 6gObQR5Vm4M\n",
      "[INFO] Extracted candidates for video 6gObQR5Vm4M\n",
      "[INFO] Saved vid_id for video 6gObQR5Vm4M\n",
      "[INFO] Saved candidates for video 6gObQR5Vm4M\n",
      "[INFO] Saved frame_paths for video 6gObQR5Vm4M\n",
      "[INFO] Saved actions_list for video 6gObQR5Vm4M\n",
      "[INFO] Saved action_steps for video 6gObQR5Vm4M\n",
      "[INFO] Processing video 2heP32bqOV0\n",
      "[INFO] Downloaded video 2heP32bqOV0\n",
      "[INFO] Extracted 10 actions for video 2heP32bqOV0\n",
      "[INFO] Sampled frames for video 2heP32bqOV0\n",
      "[INFO] Removed video 2heP32bqOV0\n",
      "[INFO] Selected frames for video 2heP32bqOV0\n",
      "[INFO] Removed unused frames for video 2heP32bqOV0\n",
      "[INFO] Extracted candidates for video 2heP32bqOV0\n",
      "[INFO] Saved vid_id for video 2heP32bqOV0\n",
      "[INFO] Saved candidates for video 2heP32bqOV0\n",
      "[INFO] Saved frame_paths for video 2heP32bqOV0\n",
      "[INFO] Saved actions_list for video 2heP32bqOV0\n",
      "[INFO] Saved action_steps for video 2heP32bqOV0\n",
      "[INFO] Processing video ln77pskM4Es\n",
      "[INFO] Downloaded video ln77pskM4Es\n",
      "[INFO] Extracted 10 actions for video ln77pskM4Es\n",
      "[INFO] Sampled frames for video ln77pskM4Es\n",
      "[INFO] Removed video ln77pskM4Es\n",
      "[INFO] Selected frames for video ln77pskM4Es\n",
      "[INFO] Removed unused frames for video ln77pskM4Es\n",
      "[INFO] Extracted candidates for video ln77pskM4Es\n",
      "[INFO] Saved vid_id for video ln77pskM4Es\n",
      "[INFO] Saved candidates for video ln77pskM4Es\n",
      "[INFO] Saved frame_paths for video ln77pskM4Es\n",
      "[INFO] Saved actions_list for video ln77pskM4Es\n",
      "[INFO] Saved action_steps for video ln77pskM4Es\n",
      "[INFO] Processing video AoPDhr5qkxY\n",
      "[INFO] Downloaded video AoPDhr5qkxY\n",
      "[INFO] Extracted 8 actions for video AoPDhr5qkxY\n",
      "[INFO] Sampled frames for video AoPDhr5qkxY\n",
      "[INFO] Removed video AoPDhr5qkxY\n",
      "[INFO] Selected frames for video AoPDhr5qkxY\n",
      "[INFO] Removed unused frames for video AoPDhr5qkxY\n",
      "[INFO] Extracted candidates for video AoPDhr5qkxY\n",
      "[INFO] Saved vid_id for video AoPDhr5qkxY\n",
      "[INFO] Saved candidates for video AoPDhr5qkxY\n",
      "[INFO] Saved frame_paths for video AoPDhr5qkxY\n",
      "[INFO] Saved actions_list for video AoPDhr5qkxY\n",
      "[INFO] Saved action_steps for video AoPDhr5qkxY\n",
      "[INFO] Processing video oHISYcjakpk\n",
      "[INFO] Downloaded video oHISYcjakpk\n",
      "[INFO] Extracted 15 actions for video oHISYcjakpk\n",
      "[INFO] Sampled frames for video oHISYcjakpk\n",
      "[INFO] Removed video oHISYcjakpk\n",
      "[INFO] Selected frames for video oHISYcjakpk\n",
      "[INFO] Removed unused frames for video oHISYcjakpk\n",
      "[INFO] Extracted candidates for video oHISYcjakpk\n",
      "[INFO] Saved vid_id for video oHISYcjakpk\n",
      "[INFO] Saved candidates for video oHISYcjakpk\n",
      "[INFO] Saved frame_paths for video oHISYcjakpk\n",
      "[INFO] Saved actions_list for video oHISYcjakpk\n",
      "[INFO] Saved action_steps for video oHISYcjakpk\n",
      "[INFO] Processing video 9guuyTr8EUg\n",
      "[INFO] Downloaded video 9guuyTr8EUg\n",
      "[INFO] Extracted 17 actions for video 9guuyTr8EUg\n",
      "[INFO] Sampled frames for video 9guuyTr8EUg\n",
      "[INFO] Removed video 9guuyTr8EUg\n",
      "[INFO] Selected frames for video 9guuyTr8EUg\n",
      "[INFO] Removed unused frames for video 9guuyTr8EUg\n",
      "[INFO] Extracted candidates for video 9guuyTr8EUg\n",
      "[INFO] Saved vid_id for video 9guuyTr8EUg\n",
      "[INFO] Saved candidates for video 9guuyTr8EUg\n",
      "[INFO] Saved frame_paths for video 9guuyTr8EUg\n",
      "[INFO] Saved actions_list for video 9guuyTr8EUg\n",
      "[INFO] Saved action_steps for video 9guuyTr8EUg\n",
      "[INFO] Processing video 9pJToG30LdM\n",
      "[INFO] Downloaded video 9pJToG30LdM\n",
      "[INFO] Extracted 11 actions for video 9pJToG30LdM\n",
      "[INFO] Sampled frames for video 9pJToG30LdM\n",
      "[INFO] Removed video 9pJToG30LdM\n",
      "[INFO] Selected frames for video 9pJToG30LdM\n",
      "[INFO] Removed unused frames for video 9pJToG30LdM\n",
      "[INFO] Extracted candidates for video 9pJToG30LdM\n",
      "[INFO] Saved vid_id for video 9pJToG30LdM\n",
      "[INFO] Saved candidates for video 9pJToG30LdM\n",
      "[INFO] Saved frame_paths for video 9pJToG30LdM\n",
      "[INFO] Saved actions_list for video 9pJToG30LdM\n",
      "[INFO] Saved action_steps for video 9pJToG30LdM\n",
      "[INFO] Processing video wii9jNiNl9Y\n",
      "[INFO] Downloaded video wii9jNiNl9Y\n",
      "[INFO] Extracted 12 actions for video wii9jNiNl9Y\n",
      "[INFO] Sampled frames for video wii9jNiNl9Y\n",
      "[INFO] Removed video wii9jNiNl9Y\n",
      "[INFO] Selected frames for video wii9jNiNl9Y\n",
      "[INFO] Removed unused frames for video wii9jNiNl9Y\n",
      "[INFO] Extracted candidates for video wii9jNiNl9Y\n",
      "[INFO] Saved vid_id for video wii9jNiNl9Y\n",
      "[INFO] Saved candidates for video wii9jNiNl9Y\n",
      "[INFO] Saved frame_paths for video wii9jNiNl9Y\n",
      "[INFO] Saved actions_list for video wii9jNiNl9Y\n",
      "[INFO] Saved action_steps for video wii9jNiNl9Y\n",
      "[INFO] Processing video sQl3bQ1rQUM\n",
      "[INFO] Downloaded video sQl3bQ1rQUM\n",
      "[INFO] Extracted 9 actions for video sQl3bQ1rQUM\n",
      "[INFO] Sampled frames for video sQl3bQ1rQUM\n",
      "[INFO] Removed video sQl3bQ1rQUM\n",
      "[INFO] Selected frames for video sQl3bQ1rQUM\n",
      "[INFO] Removed unused frames for video sQl3bQ1rQUM\n",
      "[INFO] Extracted candidates for video sQl3bQ1rQUM\n",
      "[INFO] Saved vid_id for video sQl3bQ1rQUM\n",
      "[INFO] Saved candidates for video sQl3bQ1rQUM\n",
      "[INFO] Saved frame_paths for video sQl3bQ1rQUM\n",
      "[INFO] Saved actions_list for video sQl3bQ1rQUM\n",
      "[INFO] Saved action_steps for video sQl3bQ1rQUM\n",
      "[INFO] Processing video GrCrG-EMr8g\n",
      "[INFO] Downloaded video GrCrG-EMr8g\n",
      "[INFO] Extracted 17 actions for video GrCrG-EMr8g\n",
      "[INFO] Sampled frames for video GrCrG-EMr8g\n",
      "[INFO] Removed video GrCrG-EMr8g\n",
      "[INFO] Selected frames for video GrCrG-EMr8g\n",
      "[INFO] Removed unused frames for video GrCrG-EMr8g\n",
      "[INFO] Extracted candidates for video GrCrG-EMr8g\n",
      "[INFO] Saved vid_id for video GrCrG-EMr8g\n",
      "[INFO] Saved candidates for video GrCrG-EMr8g\n",
      "[INFO] Saved frame_paths for video GrCrG-EMr8g\n",
      "[INFO] Saved actions_list for video GrCrG-EMr8g\n",
      "[INFO] Saved action_steps for video GrCrG-EMr8g\n",
      "[INFO] Processing video 1U5GzTal-2Y\n",
      "[INFO] Downloaded video 1U5GzTal-2Y\n",
      "[INFO] Extracted 8 actions for video 1U5GzTal-2Y\n",
      "[INFO] Sampled frames for video 1U5GzTal-2Y\n",
      "[INFO] Removed video 1U5GzTal-2Y\n",
      "[INFO] Selected frames for video 1U5GzTal-2Y\n",
      "[INFO] Removed unused frames for video 1U5GzTal-2Y\n",
      "[INFO] Extracted candidates for video 1U5GzTal-2Y\n",
      "[INFO] Saved vid_id for video 1U5GzTal-2Y\n",
      "[INFO] Saved candidates for video 1U5GzTal-2Y\n",
      "[INFO] Saved frame_paths for video 1U5GzTal-2Y\n",
      "[INFO] Saved actions_list for video 1U5GzTal-2Y\n",
      "[INFO] Saved action_steps for video 1U5GzTal-2Y\n",
      "[INFO] Processing video VSrY0ORD394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloaded video VSrY0ORD394\n",
      "[INFO] Extracted 13 actions for video VSrY0ORD394\n",
      "[INFO] Sampled frames for video VSrY0ORD394\n",
      "[INFO] Removed video VSrY0ORD394\n",
      "[INFO] Selected frames for video VSrY0ORD394\n",
      "[INFO] Removed unused frames for video VSrY0ORD394\n",
      "[INFO] Extracted candidates for video VSrY0ORD394\n",
      "[INFO] Saved vid_id for video VSrY0ORD394\n",
      "[INFO] Saved candidates for video VSrY0ORD394\n",
      "[INFO] Saved frame_paths for video VSrY0ORD394\n",
      "[INFO] Saved actions_list for video VSrY0ORD394\n",
      "[INFO] Saved action_steps for video VSrY0ORD394\n",
      "[INFO] Processing video t9ry6TkD598\n",
      "[INFO] Downloaded video t9ry6TkD598\n",
      "[INFO] Extracted 4 actions for video t9ry6TkD598\n",
      "[INFO] Sampled frames for video t9ry6TkD598\n",
      "[INFO] Removed video t9ry6TkD598\n",
      "[INFO] Selected frames for video t9ry6TkD598\n",
      "[INFO] Removed unused frames for video t9ry6TkD598\n",
      "[INFO] Extracted candidates for video t9ry6TkD598\n",
      "[INFO] Saved vid_id for video t9ry6TkD598\n",
      "[INFO] Saved candidates for video t9ry6TkD598\n",
      "[INFO] Saved frame_paths for video t9ry6TkD598\n",
      "[INFO] Saved actions_list for video t9ry6TkD598\n",
      "[INFO] Saved action_steps for video t9ry6TkD598\n",
      "[INFO] Processing video um8-utUa9UI\n",
      "[INFO] Downloaded video um8-utUa9UI\n",
      "[INFO] Extracted 21 actions for video um8-utUa9UI\n",
      "[INFO] Sampled frames for video um8-utUa9UI\n",
      "[INFO] Removed video um8-utUa9UI\n",
      "[INFO] Selected frames for video um8-utUa9UI\n",
      "[INFO] Removed unused frames for video um8-utUa9UI\n",
      "[INFO] Extracted candidates for video um8-utUa9UI\n",
      "[INFO] Saved vid_id for video um8-utUa9UI\n",
      "[INFO] Saved candidates for video um8-utUa9UI\n",
      "[INFO] Saved frame_paths for video um8-utUa9UI\n",
      "[INFO] Saved actions_list for video um8-utUa9UI\n",
      "[INFO] Saved action_steps for video um8-utUa9UI\n",
      "[INFO] Processing video b_uKIQ4dn3A\n",
      "[INFO] Downloaded video b_uKIQ4dn3A\n",
      "[INFO] Extracted 8 actions for video b_uKIQ4dn3A\n",
      "[INFO] Sampled frames for video b_uKIQ4dn3A\n",
      "[INFO] Removed video b_uKIQ4dn3A\n",
      "[INFO] Selected frames for video b_uKIQ4dn3A\n",
      "[INFO] Removed unused frames for video b_uKIQ4dn3A\n",
      "[INFO] Extracted candidates for video b_uKIQ4dn3A\n",
      "[INFO] Saved vid_id for video b_uKIQ4dn3A\n",
      "[INFO] Saved candidates for video b_uKIQ4dn3A\n",
      "[INFO] Saved frame_paths for video b_uKIQ4dn3A\n",
      "[INFO] Saved actions_list for video b_uKIQ4dn3A\n",
      "[INFO] Saved action_steps for video b_uKIQ4dn3A\n",
      "[INFO] Processing video L4tdvpago7s\n",
      "[INFO] Downloaded video L4tdvpago7s\n",
      "[INFO] Extracted 10 actions for video L4tdvpago7s\n",
      "[INFO] Sampled frames for video L4tdvpago7s\n",
      "[INFO] Removed video L4tdvpago7s\n",
      "[INFO] Selected frames for video L4tdvpago7s\n",
      "[INFO] Removed unused frames for video L4tdvpago7s\n",
      "[INFO] Extracted candidates for video L4tdvpago7s\n",
      "[INFO] Saved vid_id for video L4tdvpago7s\n",
      "[INFO] Saved candidates for video L4tdvpago7s\n",
      "[INFO] Saved frame_paths for video L4tdvpago7s\n",
      "[INFO] Saved actions_list for video L4tdvpago7s\n",
      "[INFO] Saved action_steps for video L4tdvpago7s\n",
      "[INFO] Processing video KgpkoZrOqck\n",
      "[INFO] Downloaded video KgpkoZrOqck\n",
      "[INFO] Extracted 13 actions for video KgpkoZrOqck\n",
      "[INFO] Sampled frames for video KgpkoZrOqck\n",
      "[INFO] Removed video KgpkoZrOqck\n",
      "[INFO] Selected frames for video KgpkoZrOqck\n",
      "[INFO] Removed unused frames for video KgpkoZrOqck\n",
      "[INFO] Extracted candidates for video KgpkoZrOqck\n",
      "[INFO] Saved vid_id for video KgpkoZrOqck\n",
      "[INFO] Saved candidates for video KgpkoZrOqck\n",
      "[INFO] Saved frame_paths for video KgpkoZrOqck\n",
      "[INFO] Saved actions_list for video KgpkoZrOqck\n",
      "[INFO] Saved action_steps for video KgpkoZrOqck\n",
      "[INFO] Processing video 91Fz5ZBgeL4\n",
      "[INFO] Downloaded video 91Fz5ZBgeL4\n",
      "[INFO] Extracted 8 actions for video 91Fz5ZBgeL4\n",
      "[INFO] Sampled frames for video 91Fz5ZBgeL4\n",
      "[INFO] Removed video 91Fz5ZBgeL4\n",
      "[INFO] Selected frames for video 91Fz5ZBgeL4\n",
      "[INFO] Removed unused frames for video 91Fz5ZBgeL4\n",
      "[INFO] Extracted candidates for video 91Fz5ZBgeL4\n",
      "[INFO] Saved vid_id for video 91Fz5ZBgeL4\n",
      "[INFO] Saved candidates for video 91Fz5ZBgeL4\n",
      "[INFO] Saved frame_paths for video 91Fz5ZBgeL4\n",
      "[INFO] Saved actions_list for video 91Fz5ZBgeL4\n",
      "[INFO] Saved action_steps for video 91Fz5ZBgeL4\n",
      "[INFO] Processing video HJHV2nYz1L8\n",
      "[INFO] Downloaded video HJHV2nYz1L8\n",
      "[INFO] Extracted 19 actions for video HJHV2nYz1L8\n",
      "[INFO] Sampled frames for video HJHV2nYz1L8\n",
      "[INFO] Removed video HJHV2nYz1L8\n",
      "[INFO] Selected frames for video HJHV2nYz1L8\n",
      "[INFO] Removed unused frames for video HJHV2nYz1L8\n",
      "[INFO] Extracted candidates for video HJHV2nYz1L8\n",
      "[INFO] Saved vid_id for video HJHV2nYz1L8\n",
      "[INFO] Saved candidates for video HJHV2nYz1L8\n",
      "[INFO] Saved frame_paths for video HJHV2nYz1L8\n",
      "[INFO] Saved actions_list for video HJHV2nYz1L8\n",
      "[INFO] Saved action_steps for video HJHV2nYz1L8\n",
      "[INFO] Processing video yizxI2Gf_ww\n",
      "[INFO] Downloaded video yizxI2Gf_ww\n",
      "[INFO] Extracted 18 actions for video yizxI2Gf_ww\n",
      "[INFO] Sampled frames for video yizxI2Gf_ww\n",
      "[INFO] Removed video yizxI2Gf_ww\n",
      "[INFO] Selected frames for video yizxI2Gf_ww\n",
      "[INFO] Removed unused frames for video yizxI2Gf_ww\n",
      "[INFO] Extracted candidates for video yizxI2Gf_ww\n",
      "[INFO] Saved vid_id for video yizxI2Gf_ww\n",
      "[INFO] Saved candidates for video yizxI2Gf_ww\n",
      "[INFO] Saved frame_paths for video yizxI2Gf_ww\n",
      "[INFO] Saved actions_list for video yizxI2Gf_ww\n",
      "[INFO] Saved action_steps for video yizxI2Gf_ww\n"
     ]
    }
   ],
   "source": [
    "# USAGE: Run this just once to prepare and save FI data on disk\n",
    "prepare_testset(dataset_root='/h/sagar/ece496-capstone/datasets', max_detections=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Cell\n",
    "\n",
    "# USAGE: Run this to load the sample_index'th sample with actions_count actions from saved files for FI dataset\n",
    "vid_id, candidates, actions_list, action_steps = load_sample(dataset_root='/h/sagar/ece496-capstone/datasets', actions_count=25, sample_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(pickles_root, actions_list=[], action_steps=[]):\n",
    "    \"\"\"\n",
    "    Tokenize actions_list\n",
    "    pickles_root: directory path to pickled data\n",
    "    actions_list: list of action annotations for a single video\n",
    "    action_steps: list of action steps for a single video\n",
    "    Return steps: a single tokenized string representing all steps annotations for a single video\n",
    "    Return entity_count: list of entity counts per action step of a single video\n",
    "    Return entities: list of list of entities within each action step of a single video\n",
    "    Return indices: list of lists of indices indicating entity spans within each action step of a single video\n",
    "    Return max_step_length: word count of the longest pre-tokenized action step for a single video\n",
    "    \"\"\"\n",
    "    # Strip all whitespaces and periods\n",
    "    # Note, periods will be added back later\n",
    "    actions = [action.strip('.') for action in actions_list]\n",
    "#     print(actions)\n",
    "    \n",
    "    NULL = '[unused1]'\n",
    "    ENTITY = '[unused2]'\n",
    "    ACTION = '[unused3]'\n",
    "    \n",
    "    max_step_length = max(0, max([len(action.split()) for action in actions]))    # maximum word count in a single action step\n",
    "#     print(max_step_length)\n",
    "    \n",
    "    entities = get_entity_list_from_action_steps(action_steps)\n",
    "    indices = []\n",
    "    \n",
    "    for action_idx, step_entities in enumerate(entities):\n",
    "#         print(step_entities)\n",
    "#         print('ORIGINAL:  ' + actions[action_idx])\n",
    "        step_indices = []\n",
    "        for entity in reversed(step_entities):\n",
    "            if entity:    # ignore if entity is None\n",
    "#                 print(entity)\n",
    "                char_idx = actions[action_idx].find(entity)\n",
    "                if char_idx < 0:\n",
    "                    print(\"NO MATCH:  \" + entity)\n",
    "                    entities[action_idx].remove(entity)\n",
    "                    continue\n",
    "                else:\n",
    "                    word_idx = len(actions[action_idx][:char_idx].split())\n",
    "                    words = actions[action_idx].split()\n",
    "                    words.insert(word_idx, ENTITY)\n",
    "                    actions[action_idx] = ' '.join(words)\n",
    "                    step_indices.append(list(range(max_step_length*action_idx + word_idx, max_step_length*action_idx + word_idx + len(entity.split()))))\n",
    "#                     print(step_indices)\n",
    "#         print(step_indices[::-1])\n",
    "        for ind in step_indices[::-1]:\n",
    "            indices.append(ind)\n",
    "#         print('TOKENIZED: ' + actions[action_idx])\n",
    "#         print()\n",
    "    \n",
    "    for idx in range(len(entities)):\n",
    "        while None in entities[idx]:\n",
    "            entities[idx].remove(None)\n",
    "    \n",
    "    entity_count = [len(entity) for entity in entities]\n",
    "    \n",
    "    actions = [action + '.' if not action.endswith('.') else action for action in actions]\n",
    "    steps = ''\n",
    "    for action in actions:\n",
    "        steps = steps + action + ' ' + ACTION + ' '\n",
    "    steps = steps + ACTION    # TODO: check if this is correct\n",
    "#     print(steps)\n",
    "\n",
    "    pickle_data(steps, pickles_root, 'steps')\n",
    "    pickle_data(entity_count, pickles_root, 'entity_count')\n",
    "    pickle_data(entities, pickles_root, 'entities')\n",
    "    pickle_data(indices, pickles_root, 'indices')\n",
    "    pickle_data(max_step_length, pickles_root, 'max_step_length')\n",
    "\n",
    "    return steps, entity_count, entities, indices, max_step_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00001', '00000']\n",
      "NO MATCH:  the bottom of the other side\n",
      "['00003', '00000', '00002', '00001']\n",
      "NO MATCH:  a food processor \n",
      "['00000']\n",
      "['00000']\n",
      "['00000', '00001']\n",
      "['00000']\n",
      "['00000']\n",
      "['00000']\n",
      "['00000']\n",
      "['00000']\n",
      "NO MATCH:  1/4 cup of plain yoghurt\n",
      "['00000']\n",
      "['00001', '00002', '00000', '00003']\n",
      "['00000', '00001']\n",
      "NO MATCH:  1/4 inch slices\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_action_directories = os.listdir(\"/h/sagar/ece496-capstone/datasets/fi/\")\n",
    "\n",
    "for num_action in num_action_directories:\n",
    "    videos = os.listdir(\"/h/sagar/ece496-capstone/datasets/fi/{}/\".format(num_action))\n",
    "    print(videos)\n",
    "    for video in videos:\n",
    "        pickles_root = \"/h/sagar/ece496-capstone/datasets/fi/{}/{}/pickles\".format(num_action, video)\n",
    "        vid_id, candidates, actions_list, action_steps = load_sample(dataset_root='/h/sagar/ece496-capstone/datasets', actions_count=num_action, sample_index=int(video))\n",
    "        steps, entity_count, entities, indices, max_step_length = tokenize_sample(pickles_root, actions_list, action_steps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Cell\n",
    "\n",
    "# USAGE: Run this to tokenize actions_list of a single video and save data on disk\n",
    "pickles_root = '/h/sagar/ece496-capstone/datasets/fi/4/00000/pickles'\n",
    "vid_id, candidates, actions_list, action_steps = load_sample(dataset_root='/h/sagar/ece496-capstone/datasets', actions_count=4, sample_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, entity_count, entities, indices, max_step_length = tokenize_sample(pickles_root, actions_list, action_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cut [unused2] the salmon into [unused2] slices. [unused3] form [unused2] the rice into [unused2] a ball. [unused3] place [unused2] the rice on [unused2] the fish slice. [unused3] Flip [unused2] the piece of sushi over. [unused3] [unused3]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
