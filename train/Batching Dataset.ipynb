{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from dataset import YouCookII\n",
    "from dataset import YouCookIICollate\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(data, pickles_dir, fname):\n",
    "    \"\"\"\n",
    "    Pickle data into bytestreams\n",
    "    data: data to be pickled\n",
    "    pickles_dir: directory path to pickled data\n",
    "    fname: name of pickled file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(pickles_dir):\n",
    "        os.mkdir(pickles_dir)\n",
    "    pickle_out = open(os.path.join(pickles_dir, fname + '.pickle'), 'wb')\n",
    "    pickle.dump(data, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "def depickle_data(pickles_dir, fname):\n",
    "    pickle_path = os.path.join(pickles_dir, fname + '.pickle')\n",
    "    pickle_in = open(pickle_path, 'rb')\n",
    "    data = pickle.load(pickle_in)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DETECTIONS = 20\n",
    "FRAMES = 5\n",
    "\n",
    "COORDINATES = 4\n",
    "EMBEDDING_SIZE = 2048\n",
    "\n",
    "root = \"/h/sagar/ece496-capstone/datasets/ycii\"\n",
    "collate = YouCookIICollate(MAX_DETECTIONS=MAX_DETECTIONS)\n",
    "num_actions = sorted(list(map(int, os.listdir(root))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_list = []\n",
    "bboxes_tensor = None\n",
    "features_tensor = None\n",
    "steps_list = []\n",
    "entity_list = []\n",
    "entity_count_list = []\n",
    "step_length_list = []\n",
    "\n",
    "for num_action in num_actions:\n",
    "    print(\"Processing videos of size {}\".format(num_action))\n",
    "    \n",
    "    dataset = YouCookII(num_action, \"/h/sagar/ece496-capstone/datasets/ycii\")\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate)\n",
    "    \n",
    "    for data in dataloader:\n",
    "        video_id, bboxes, features, actions, _, entities, entity_count, max_step_length = data\n",
    "        \n",
    "        video_id = video_id[0]\n",
    "        bboxes = bboxes[0]\n",
    "        features = features[0]\n",
    "        actions = actions[0]\n",
    "        entities = entities[0]\n",
    "        entity_count = entity_count[0]\n",
    "                \n",
    "        for i in range(num_action):\n",
    "            video_id_list.append(video_id)\n",
    "            step_length_list.append(len(actions[i].split()))\n",
    "            \n",
    "        if bboxes_tensor is None:\n",
    "            bboxes_tensor = bboxes\n",
    "        else:\n",
    "            bboxes_tensor = torch.cat((bboxes_tensor, bboxes))\n",
    "\n",
    "        if features_tensor is None:\n",
    "            features_tensor = features\n",
    "        else:\n",
    "            features_tensor = torch.cat((features_tensor, features))\n",
    "            \n",
    "        steps_list = steps_list + actions\n",
    "        entity_list = entity_list + entities\n",
    "        entity_count_list = entity_count_list + entity_count\n",
    "\n",
    "print(len(video_id_list), len(steps_list), len(entity_list), len(entity_count_list), len(step_length_list))\n",
    "print(bboxes_tensor.shape[0], features_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 128\n",
    "\n",
    "video_id_chunks = [video_id_list[i:(i+chunk)] for i in range(0, len(video_id_list), chunk)][:-1]\n",
    "steps_chunks = [steps_list[i:(i+chunk)] for i in range(0, len(steps_list), chunk)][:-1]\n",
    "entity_chunks = [entity_list[i:(i+chunk)] for i in range(0, len(entity_list), chunk)][:-1]\n",
    "entity_count_chunks = [entity_count_list[i:(i+chunk)] for i in range(0, len(entity_count_list), chunk)][:-1]\n",
    "step_length_chunks = [step_length_list[i:(i+chunk)] for i in range(0, len(step_length_list), chunk)][:-1]\n",
    "\n",
    "bboxes_chunks = bboxes_tensor.split(MAX_DETECTIONS * FRAMES * chunk)[:-1]\n",
    "features_chunks = features_tensor.split(MAX_DETECTIONS * FRAMES * chunk)[:-1]\n",
    "\n",
    "print(len(video_id_chunks), len(steps_chunks), len(entity_chunks), len(entity_count_chunks), len(step_length_chunks))\n",
    "print(len(bboxes_chunks), len(features_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of steps: {}\".format(chunk * len(steps_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/h/sagar/ece496-capstone/datasets/ycii_{}/{}\".format(chunk, chunk)\n",
    "os.makedirs(root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(zip(video_id_chunks, steps_chunks, entity_chunks, entity_count_chunks, step_length_chunks, bboxes_chunks, features_chunks)):\n",
    "    print(\"Processing pseudo-video {}\".format(idx))\n",
    "    \n",
    "    video_ids, actions, entities, entity_counts, step_lengths, bboxes, features = data\n",
    "    \n",
    "    #     bboxes = bboxes.reshape(-1, MAX_DETECTIONS, COORDINATES).unsqueeze(1)\n",
    "    #     features = features.reshape(-1, MAX_DETECTIONS, EMBEDDING_SIZE).unsqueeze(1)\n",
    "    #     candidates = [(data[0], data[1]) for data in zip(bboxes, features)]\n",
    "    \n",
    "    steps = '. '.join(actions) + '.'\n",
    "    max_step_length = max(step_lengths)\n",
    "    \n",
    "    bboxes = bboxes.clone()\n",
    "    features = features.clone()\n",
    "    \n",
    "    dir = os.path.join(root, str(idx).zfill(5), 'pickles')\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    \n",
    "    pickle_data(video_ids, dir, 'vid_id')\n",
    "    pickle_data(bboxes, dir, 'bboxes')\n",
    "    pickle_data(features, dir, 'features')\n",
    "    pickle_data(actions, dir, 'actions_list')\n",
    "    pickle_data(steps, dir, 'steps')\n",
    "    pickle_data(entities, dir, 'entities')\n",
    "    pickle_data(entity_counts, dir, 'entity_count')\n",
    "    pickle_data(max_step_length, dir, 'max_step_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YouCookII(4, \"/h/sagar/ece496-capstone/datasets/ycii_4\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(dataloader):\n",
    "    print(\"Fetching pseudo-video {}\".format(idx))\n",
    "    video_id, bboxes, features, actions, steps, entities, entity_count, max_step_length = data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
