{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from detector import Detector\n",
    "from parser import parse\n",
    "from transformers import LxmertModel, LxmertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path='output.json'):\n",
    "    \"\"\"\n",
    "    Check for valid JSON format and read content\n",
    "    path: path to JSON file\n",
    "    \"\"\"\n",
    "    file = open(path)\n",
    "    line = file.read().replace('\\n', ' ')\n",
    "    file.close()\n",
    "    try:\n",
    "        parsed_json = json.loads(line)\n",
    "    except:\n",
    "        assert False, 'Invalid JSON'\n",
    "    return parsed_json\n",
    "\n",
    "def get_vid_ext(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Returns video file extension\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    vid_prefix = os.path.join(video_dir, vid_id)\n",
    "    if os.path.exists(vid_prefix+'.mp4'):\n",
    "        return '.mp4'\n",
    "    elif os.path.exists(vid_prefix+'.mkv'):\n",
    "        return '.mkv'\n",
    "    elif os.path.exists(vid_prefix+'.webm'):\n",
    "        return '.webm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Download video\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    # download the video\n",
    "    vid_url = 'www.youtube.com/watch?v='+vid_id\n",
    "    vid_prefix = os.path.join(video_dir, vid_id) \n",
    "    os.system(' '.join((\"youtube-dl -o\", vid_prefix, vid_url)))\n",
    "\n",
    "\n",
    "def sample_frames(vid_id, video_dir, frame_dir, fps=5):\n",
    "    \"\"\"\n",
    "    Sample video into frames at fixed fps\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    frame_dir: directory path to video frames\n",
    "    fps: fps for frame extraction\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(frame_dir):\n",
    "        os.mkdir(frame_dir)\n",
    "    vid_ext = get_vid_ext(vid_id, video_dir)\n",
    "    ff_command = 'ffmpeg -i {}/{}{} -y -an -qscale 0 -vf fps={} {}/%06d.jpg'.format(video_dir, vid_id, vid_ext, fps, frame_dir)\n",
    "    os.system(ff_command)\n",
    "\n",
    "\n",
    "def remove_video(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Delete video\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    vid_prefix = os.path.join(video_dir, vid_id)\n",
    "    vid_ext = get_vid_ext(vid_id, video_dir)\n",
    "    os.remove(vid_prefix+vid_ext)\n",
    "\n",
    "\n",
    "def select_frames(actions, num_frames_per_step):\n",
    "    \"\"\"\n",
    "    Return representative frames for actions\n",
    "    actions: list of action annotations from YCII annotations\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    Return required_frames: set contataining names of representative frames\n",
    "    \"\"\"\n",
    "    required_frames = set()\n",
    "    for action in actions:\n",
    "        action_start = action['segment'][0]\n",
    "        action_end = action['segment'][1]\n",
    "        action_delta = (action_end - action_start) / (num_frames_per_step + 1)    # need num_frames_per_step+1 intervals for num_frames_per_step inner frames\n",
    "        for i in range(num_frames_per_step):\n",
    "            frame_time = action_start + action_delta * (i+1)    # in seconds\n",
    "            frame_id = int( frame_time*(num_frames_per_step + 1) )\n",
    "            frame_name = '{}.jpg'.format(str(frame_id).zfill(6))\n",
    "            required_frames.add(frame_name)\n",
    "    return required_frames\n",
    "\n",
    "\n",
    "def remove_frames(frame_dir, required_frames):\n",
    "    \"\"\"\n",
    "    Remove unused frames\n",
    "    frame_dir: directory path to video frames\n",
    "    required_frames: set contataining names of representative frames\n",
    "    \"\"\"\n",
    "    if os.path.isdir(frame_dir):\n",
    "        curr_frames = os.listdir(frame_dir)\n",
    "        for frame in curr_frames:\n",
    "            if frame not in required_frames:\n",
    "                os.remove(os.path.join(frame_dir, frame))\n",
    "\n",
    "\n",
    "def get_actions(actions):\n",
    "    \"\"\"\n",
    "    Return list of actions text for video\n",
    "    actions: list of action annotations from YCII annotations\n",
    "    Return actions_text: list of actions text for video\n",
    "    \"\"\"\n",
    "    actions_text = []\n",
    "    for action in actions:\n",
    "        actions_text.append(action['sentence'])\n",
    "    return actions_text\n",
    "\n",
    "\n",
    "def pickle_data(data, pickles_dir, fname):\n",
    "    \"\"\"\n",
    "    Pickle data into bytestreams\n",
    "    data: data to be pickled\n",
    "    pickles_dir: directory path to pickled data\n",
    "    fname: name of pickled file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(pickles_dir):\n",
    "        os.mkdir(pickles_dir)\n",
    "    pickle_out = open(os.path.join(pickles_dir, fname+'.pickle'), 'wb')\n",
    "    pickle.dump(data, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "\n",
    "def depickle_data(pickles_dir, fname):\n",
    "    \"\"\"\n",
    "    Depickle data from bytestreams\n",
    "    pickles_dir: directory path to pickled data\n",
    "    fname: name of pickled file\n",
    "    Return data: depickled data\n",
    "    \"\"\"\n",
    "    pickle_path = os.path.join(pickles_dir, fname+'.pickle')\n",
    "    if os.path.exists(pickle_path):\n",
    "        pickle_in = open(pickle_path, 'rb')\n",
    "        data = pickle.load(pickle_in)\n",
    "        return data\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets', vid_list='/h/mkhan/ece496-capstone/datasets/vid_list/vid_list_ycii_val_short.txt', num_frames_per_step=4, max_detections=5):\n",
    "    \"\"\"\n",
    "    Download and prepare YCII dataset files\n",
    "    dataset_root: directory path to dataset base\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    max_detections: number of detections per frame\n",
    "    \"\"\"\n",
    "\n",
    "    annotations = read_json(os.path.join(dataset_root, 'annotations', 'ycii_annotations_trainval.json'))['database']\n",
    "\n",
    "    ycii_root = os.path.join(dataset_root, 'ycii')\n",
    "    if not os.path.isdir(ycii_root):\n",
    "        os.mkdir(ycii_root)\n",
    "\n",
    "    videos_root = os.path.join(dataset_root, 'ycii_videos')\n",
    "    if not os.path.isdir(videos_root):\n",
    "        os.mkdir(videos_root)\n",
    "\n",
    "    missing_vid_list = []\n",
    "\n",
    "    detector = Detector()\n",
    "\n",
    "    with open(vid_list) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rcp_type,vid_id = line.replace('\\n','').split('/')\n",
    "            print('[INFO] Processing video {}'.format(vid_id))\n",
    "\n",
    "            # download the video\n",
    "#             download_video(vid_id, videos_root)\n",
    "            vid_url = 'www.youtube.com/watch?v='+vid_id\n",
    "            vid_prefix = os.path.join(videos_root, vid_id)\n",
    "            os.system(' '.join((\"youtube-dl -o\", vid_prefix, vid_url)))\n",
    "\n",
    "            # check if the video is available\n",
    "            if os.path.exists(vid_prefix+'.mp4') or os.path.exists(vid_prefix+'.mkv') or os.path.exists(vid_prefix+'.webm'):\n",
    "                print('[INFO] Downloaded video {}'.format(vid_id))\n",
    "            else:\n",
    "                missing_vid_list.append(line)\n",
    "                print('[INFO] Cannot download video {}'.format(vid_id))\n",
    "                continue\n",
    "\n",
    "            # get annotations list (and action count)\n",
    "            actions = annotations[vid_id]['annotations']\n",
    "            actions_list = get_actions(actions)    # list of action annotations for a single video\n",
    "            actions_count = len(actions_list)\n",
    "            print('[INFO] Extracted {} actions for video {}'.format(actions_count, vid_id))\n",
    "\n",
    "            # setup directories\n",
    "            parent_root = os.path.join(ycii_root, str(actions_count))\n",
    "            if not os.path.isdir(parent_root):\n",
    "                os.mkdir(parent_root)\n",
    "\n",
    "            sample_index = 0    # change this to 1 to ensure 1-indexing for samples\n",
    "            samples_list = os.listdir(parent_root)    # list of samples of same actions_count\n",
    "            if samples_list:\n",
    "                sample_index = max([int(index) for index in samples_list]) + 1    # set sample counter to next available integer\n",
    "            sample_index = str(sample_index).zfill(5)    # required to ensure sortability\n",
    "            sample_root = os.path.join(parent_root, sample_index)    # all data for this video will be stored under here\n",
    "            if not os.path.isdir(sample_root):\n",
    "                os.mkdir(sample_root)\n",
    "\n",
    "            frames_root = os.path.join(sample_root, 'frames')    # all sampled images for this video will be under here\n",
    "            if not os.path.isdir(frames_root):\n",
    "                os.mkdir(frames_root)\n",
    "            pickles_root = os.path.join(sample_root, 'pickles')    # all raw data for this video will be under here (stored by variable names)\n",
    "            if not os.path.isdir(pickles_root):\n",
    "                os.mkdir(pickles_root)\n",
    "\n",
    "            # sample frames at fixed fps\n",
    "            sample_frames(vid_id, videos_root, frames_root, fps=num_frames_per_step+1)    # need num_frames_per_step+1 intervals for num_frames_per_step inner frames\n",
    "            print('[INFO] Sampled frames for video {}'.format(vid_id))\n",
    "\n",
    "            # remove sampled video file (optional)\n",
    "            remove_video(vid_id, videos_root)\n",
    "            print('[INFO] Removed video {}'.format(vid_id))\n",
    "\n",
    "            # select representative frames for actions\n",
    "            selected_frames = select_frames(actions, num_frames_per_step)\n",
    "            print('[INFO] Selected frames for video {}'.format(vid_id))\n",
    "\n",
    "            # remove unsued frames\n",
    "            remove_frames(frames_root, selected_frames)\n",
    "            print('[INFO] Removed unused frames for video {}'.format(vid_id))\n",
    "\n",
    "            # get candidates for images\n",
    "            frames = sorted(glob.glob(os.path.join(frames_root, '*.*')))\n",
    "            candidates = [detector.inference(frame, max_detections=max_detections) for frame in frames]\n",
    "            print('[INFO] Extracted candidates for video {}'.format(vid_id))\n",
    "\n",
    "            # save pickled files for vid_id\n",
    "            pickle_data(vid_id, pickles_root, 'vid_id')\n",
    "            print('[INFO] Saved vid_id for video {}'.format(vid_id))\n",
    "\n",
    "            # save pickeled files for candidates\n",
    "            pickle_data(candidates, pickles_root, 'candidates')\n",
    "            print('[INFO] Saved candidates for video {}'.format(vid_id))\n",
    "\n",
    "            # save pickled files for annotations list\n",
    "            pickle_data(actions_list, pickles_root, 'actions_list')\n",
    "            print('[INFO] Saved actions for video {}'.format(vid_id))\n",
    "\n",
    "    # write the missing videos to file\n",
    "    missing_vid = open(os.path.join(dataset_root, 'vid_list', 'missing_videos.txt'), 'w')\n",
    "    for line in missing_vid_list:\n",
    "        missing_vid.write(line)\n",
    "\n",
    "    # sanitize and remove the intermediate files\n",
    "    # os.system(\"find {} -name '*.part*' -delete\".format(dataset_root))\n",
    "    os.system(\"find {} -name '*.f*' -delete\".format(dataset_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(dataset_root='/h/mkhan/ece496-capstone/datasets', actions_count=10, sample_index=0):\n",
    "    \"\"\"\n",
    "    Load the sample_index'th sample with actions_count actions from saved files\n",
    "    dataset_root: directory path to dataset base\n",
    "    actions_count: number of actions in sample (bucket id of sample)\n",
    "    sample_index: index of sample within the bucket\n",
    "    Return vid_id: video id\n",
    "    Return candidates: list of candidate data (bboxes, features) for a single video\n",
    "    Return actions_list: list of action annotations for a single video\n",
    "    \"\"\"\n",
    "    pickles_root = os.path.join(dataset_root, 'ycii', str(actions_count), str(sample_index).zfill(5), 'pickles')\n",
    "    if not os.path.isdir(pickles_root):\n",
    "        print('[INFO] Cannot load data for {}\\'th sample with {} action(s)'.format(sample_index, actions_count))\n",
    "        return '', [], []\n",
    "    else:\n",
    "        vid_id = depickle_data(pickles_root, 'vid_id')\n",
    "        candidates = depickle_data(pickles_root, 'candidates')\n",
    "        actions_list = depickle_data(pickles_root, 'actions_list')\n",
    "        return vid_id, candidates, actions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /h/mkhan/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n",
      "[INFO] Processing video sdB8qBlLS2E\n",
      "[INFO] Cannot download video sdB8qBlLS2E\n",
      "[INFO] Processing video fn9anlEL4FI\n",
      "[INFO] Downloaded video fn9anlEL4FI\n",
      "[INFO] Extracted 8 actions for video fn9anlEL4FI\n",
      "[INFO] Sampled frames for video fn9anlEL4FI\n",
      "[INFO] Removed video fn9anlEL4FI\n",
      "[INFO] Selected frames for video fn9anlEL4FI\n",
      "[INFO] Removed unused frames for video fn9anlEL4FI\n",
      "[INFO] Extracted candidates for video fn9anlEL4FI\n",
      "[INFO] Saved vid_id for video fn9anlEL4FI\n",
      "[INFO] Saved candidates for video fn9anlEL4FI\n",
      "[INFO] Saved actions for video fn9anlEL4FI\n",
      "[INFO] Processing video RnSl1LVrItI\n",
      "[INFO] Downloaded video RnSl1LVrItI\n",
      "[INFO] Extracted 8 actions for video RnSl1LVrItI\n",
      "[INFO] Sampled frames for video RnSl1LVrItI\n",
      "[INFO] Removed video RnSl1LVrItI\n",
      "[INFO] Selected frames for video RnSl1LVrItI\n",
      "[INFO] Removed unused frames for video RnSl1LVrItI\n",
      "[INFO] Extracted candidates for video RnSl1LVrItI\n",
      "[INFO] Saved vid_id for video RnSl1LVrItI\n",
      "[INFO] Saved candidates for video RnSl1LVrItI\n",
      "[INFO] Saved actions for video RnSl1LVrItI\n",
      "[INFO] Processing video vVZsj1t9R70\n",
      "[INFO] Downloaded video vVZsj1t9R70\n",
      "[INFO] Extracted 11 actions for video vVZsj1t9R70\n",
      "[INFO] Sampled frames for video vVZsj1t9R70\n",
      "[INFO] Removed video vVZsj1t9R70\n",
      "[INFO] Selected frames for video vVZsj1t9R70\n",
      "[INFO] Removed unused frames for video vVZsj1t9R70\n",
      "[INFO] Extracted candidates for video vVZsj1t9R70\n",
      "[INFO] Saved vid_id for video vVZsj1t9R70\n",
      "[INFO] Saved candidates for video vVZsj1t9R70\n",
      "[INFO] Saved actions for video vVZsj1t9R70\n"
     ]
    }
   ],
   "source": [
    "# USAGE: Run this just once to prepare and save data on disk\n",
    "prepare_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets', vid_list='/h/mkhan/ece496-capstone/datasets/vid_list/vid_list_ycii_val_short.txt', num_frames_per_step=4, max_detections=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Cell\n",
    "\n",
    "# USAGE: Run this to load the sample_index'th sample with actions_count actions from saved files\n",
    "vid_id, candidates, actions_list = load_sample(dataset_root='/h/mkhan/ece496-capstone/datasets', actions_count=8, sample_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn9anlEL4FI\n",
      "[(tensor([[[8.2457e-01, 3.2102e-02, 1.0000e+00, 9.7234e-01],\n",
      "         [2.7613e-01, 2.6707e-01, 9.1785e-01, 9.0073e-01],\n",
      "         [2.4003e-02, 6.3864e-03, 3.4081e-01, 8.3746e-01],\n",
      "         [5.6791e-01, 1.7344e-01, 9.7794e-01, 9.6122e-01],\n",
      "         [6.3707e-01, 1.0543e-01, 9.9941e-01, 9.1054e-01],\n",
      "         [3.9213e-04, 9.0776e-02, 3.0502e-01, 9.2364e-01],\n",
      "         [4.5422e-01, 2.0420e-01, 9.1860e-01, 9.4682e-01],\n",
      "         [7.5874e-01, 1.2866e-01, 9.9850e-01, 9.8333e-01],\n",
      "         [3.5250e-01, 1.5967e-01, 9.6422e-01, 8.3697e-01],\n",
      "         [0.0000e+00, 8.1945e-01, 6.3655e-02, 1.0000e+00]]]), tensor([[[1.7991e-01, 0.0000e+00, 7.0093e-03,  ..., 0.0000e+00,\n",
      "          1.4755e-01, 0.0000e+00],\n",
      "         [2.2051e-01, 3.1070e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.4792e-01, 4.2602e-02],\n",
      "         [2.0538e-01, 0.0000e+00, 5.4833e-01,  ..., 1.1816e-03,\n",
      "          1.5883e+00, 1.1352e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 2.5063e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.8272e-02, 3.0853e-02],\n",
      "         [3.4266e-01, 0.0000e+00, 0.0000e+00,  ..., 9.3461e-03,\n",
      "          1.3615e+00, 3.2686e-01],\n",
      "         [0.0000e+00, 5.0394e-01, 1.0580e-02,  ..., 0.0000e+00,\n",
      "          3.9955e-01, 0.0000e+00]]])), (tensor([[[0.6396, 0.1099, 0.9983, 0.9002],\n",
      "         [0.2426, 0.3374, 0.8781, 0.9378],\n",
      "         [0.5178, 0.1672, 0.9982, 0.8331],\n",
      "         [0.2942, 0.2084, 0.9117, 0.8404],\n",
      "         [0.7607, 0.1254, 0.9980, 0.9828],\n",
      "         [0.5057, 0.3449, 0.9257, 0.9976],\n",
      "         [0.1295, 0.4181, 0.7310, 0.9737],\n",
      "         [0.0267, 0.0297, 0.3435, 0.9566],\n",
      "         [0.3641, 0.3865, 0.9564, 0.9982],\n",
      "         [0.3191, 0.2809, 0.7494, 0.9167]]]), tensor([[[0.1892, 0.0772, 0.0000,  ..., 0.0000, 0.8963, 0.1359],\n",
      "         [0.1391, 0.1164, 0.0000,  ..., 0.0000, 0.2040, 0.1778],\n",
      "         [0.4358, 0.0000, 0.0000,  ..., 0.0000, 2.8058, 0.1172],\n",
      "         ...,\n",
      "         [0.2590, 0.0000, 0.0640,  ..., 0.0000, 0.1422, 0.2474],\n",
      "         [0.2863, 0.0398, 0.0000,  ..., 0.0000, 0.4290, 0.0940],\n",
      "         [0.0089, 0.1114, 0.0182,  ..., 0.0000, 1.6424, 0.4566]]])), (tensor([[[0.6246, 0.2104, 0.9979, 0.9971],\n",
      "         [0.5106, 0.2386, 0.9419, 0.9959],\n",
      "         [0.1096, 0.0366, 0.5572, 0.7182],\n",
      "         [0.1335, 0.0563, 0.7346, 0.6576],\n",
      "         [0.1687, 0.0044, 0.5873, 0.6200],\n",
      "         [0.2192, 0.0071, 0.6475, 0.6875],\n",
      "         [0.1854, 0.0856, 0.6093, 0.7581],\n",
      "         [0.5355, 0.1553, 0.9960, 0.8177],\n",
      "         [0.7888, 0.2239, 0.9982, 0.9998],\n",
      "         [0.2087, 0.0094, 0.8249, 0.6308]]]), tensor([[[1.3866e-01, 8.4141e-02, 4.7090e-03,  ..., 0.0000e+00,\n",
      "          8.3143e-02, 4.5144e-01],\n",
      "         [1.9632e-01, 8.0210e-02, 1.6851e-02,  ..., 0.0000e+00,\n",
      "          3.5706e-01, 4.1560e-01],\n",
      "         [8.1128e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.0039e+00, 3.2270e-01],\n",
      "         ...,\n",
      "         [3.8161e-01, 0.0000e+00, 5.0869e-02,  ..., 0.0000e+00,\n",
      "          3.4827e+00, 3.1057e-02],\n",
      "         [2.5223e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.9641e-02, 1.7302e-02],\n",
      "         [2.7115e-02, 0.0000e+00, 2.3122e-02,  ..., 0.0000e+00,\n",
      "          5.0152e+00, 2.9220e-02]]])), (tensor([[[1.1795e-03, 2.5768e-01, 3.4948e-01, 1.0000e+00],\n",
      "         [5.4534e-01, 9.9098e-02, 9.7262e-01, 9.0764e-01],\n",
      "         [9.0843e-05, 9.4408e-02, 3.0267e-01, 9.4613e-01],\n",
      "         [0.0000e+00, 2.0725e-01, 5.3327e-01, 9.4780e-01],\n",
      "         [6.5244e-01, 2.1550e-01, 9.9761e-01, 9.8600e-01],\n",
      "         [4.6022e-01, 1.0733e-01, 9.2178e-01, 8.6890e-01],\n",
      "         [2.6255e-02, 8.0159e-03, 3.4046e-01, 8.2949e-01],\n",
      "         [5.6810e-02, 1.1270e-01, 5.3189e-01, 8.4842e-01],\n",
      "         [3.7227e-01, 2.1204e-01, 9.7159e-01, 9.2476e-01],\n",
      "         [7.6210e-02, 1.0083e-01, 4.2222e-01, 9.9216e-01]]]), tensor([[[6.9320e-01, 5.1331e-02, 7.5296e-02,  ..., 8.7926e-02,\n",
      "          3.9148e-01, 3.2783e-01],\n",
      "         [4.7731e-01, 4.0664e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          6.9011e-01, 4.3946e-01],\n",
      "         [6.8205e-01, 5.7496e-04, 1.0187e-01,  ..., 3.7786e-02,\n",
      "          5.2857e-01, 1.8988e-02],\n",
      "         ...,\n",
      "         [4.6713e-01, 0.0000e+00, 2.4190e-01,  ..., 0.0000e+00,\n",
      "          1.7275e+00, 7.7888e-02],\n",
      "         [3.5746e-01, 9.9425e-02, 2.4231e-06,  ..., 0.0000e+00,\n",
      "          4.1629e-01, 1.1475e-01],\n",
      "         [3.9133e-01, 0.0000e+00, 3.8369e-01,  ..., 0.0000e+00,\n",
      "          8.1661e-02, 3.1260e-01]]])), (tensor([[[0.2289, 0.1758, 0.6363, 0.7930],\n",
      "         [0.1895, 0.2881, 0.6178, 0.9132],\n",
      "         [0.1415, 0.3192, 0.5617, 0.9898],\n",
      "         [0.2782, 0.4224, 0.7462, 0.9918],\n",
      "         [0.3372, 0.3143, 0.7782, 0.9612],\n",
      "         [0.5954, 0.1409, 0.9804, 0.9533],\n",
      "         [0.8024, 0.0192, 0.9998, 0.9683],\n",
      "         [0.0011, 0.0720, 0.3220, 0.9314],\n",
      "         [0.0000, 0.1029, 0.4527, 0.8978],\n",
      "         [0.2097, 0.4158, 0.6525, 0.9912]]]), tensor([[[0.0398, 0.0000, 0.0154,  ..., 0.0000, 3.6509, 0.4013],\n",
      "         [0.0304, 0.0305, 0.0316,  ..., 0.0000, 0.6099, 0.2095],\n",
      "         [0.0585, 0.0066, 0.0232,  ..., 0.0000, 0.2057, 0.1735],\n",
      "         ...,\n",
      "         [0.5658, 0.0358, 0.0300,  ..., 0.0205, 0.9904, 0.0843],\n",
      "         [0.5581, 0.0000, 0.0163,  ..., 0.0918, 0.6584, 0.0428],\n",
      "         [0.0607, 0.0483, 0.0642,  ..., 0.0000, 0.0868, 0.1684]]])), (tensor([[[0.1599, 0.4232, 0.7364, 0.9928],\n",
      "         [0.1911, 0.3492, 0.7710, 0.9171],\n",
      "         [0.3589, 0.3208, 0.7918, 0.9769],\n",
      "         [0.1409, 0.3841, 0.5661, 0.9899],\n",
      "         [0.1692, 0.2595, 0.5997, 0.9404],\n",
      "         [0.2230, 0.2370, 0.6486, 0.8690],\n",
      "         [0.3131, 0.2321, 0.7589, 0.9363],\n",
      "         [0.1134, 0.5302, 0.6764, 0.9951],\n",
      "         [0.2815, 0.1029, 0.8629, 0.7401],\n",
      "         [0.1715, 0.2102, 0.7324, 0.8291]]]), tensor([[[1.0692e-01, 9.0756e-02, 8.8436e-03,  ..., 0.0000e+00,\n",
      "          8.0172e-01, 4.9726e-01],\n",
      "         [9.9196e-02, 1.6292e-01, 6.9693e-02,  ..., 2.0955e-02,\n",
      "          2.5866e+00, 5.2524e-01],\n",
      "         [1.0201e-02, 9.3233e-02, 2.4288e-02,  ..., 0.0000e+00,\n",
      "          1.6819e+00, 1.2172e-01],\n",
      "         ...,\n",
      "         [1.3921e-01, 0.0000e+00, 1.6126e-05,  ..., 0.0000e+00,\n",
      "          1.7175e-01, 1.4346e-01],\n",
      "         [5.0882e-01, 1.8549e-02, 0.0000e+00,  ..., 2.1228e-02,\n",
      "          6.0736e+00, 7.5176e-01],\n",
      "         [1.4702e-01, 5.3097e-02, 4.8782e-03,  ..., 1.2582e-02,\n",
      "          3.8656e+00, 8.2624e-01]]])), (tensor([[[0.1646, 0.1621, 0.5740, 0.8139],\n",
      "         [0.1523, 0.4935, 0.7247, 0.9932],\n",
      "         [0.1486, 0.2521, 0.7154, 0.8476],\n",
      "         [0.8311, 0.0484, 1.0000, 0.9538],\n",
      "         [0.3299, 0.3937, 0.7852, 0.9873],\n",
      "         [0.2257, 0.4017, 0.6735, 0.9890],\n",
      "         [0.2010, 0.4418, 0.8130, 0.9664],\n",
      "         [0.2515, 0.1076, 0.8252, 0.7369],\n",
      "         [0.1871, 0.2868, 0.6229, 0.9435],\n",
      "         [0.1870, 0.6236, 0.7919, 0.9949]]]), tensor([[[4.9976e-02, 0.0000e+00, 3.4380e-02,  ..., 0.0000e+00,\n",
      "          2.9162e+00, 8.0097e-01],\n",
      "         [2.9882e-01, 1.4110e-02, 9.5550e-03,  ..., 0.0000e+00,\n",
      "          6.2598e-01, 3.3880e-01],\n",
      "         [6.0095e-02, 7.1268e-04, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.8659e+00, 5.9982e-01],\n",
      "         ...,\n",
      "         [3.3533e-01, 2.5623e-03, 1.6733e-04,  ..., 6.2282e-03,\n",
      "          6.5958e+00, 1.3006e+00],\n",
      "         [2.1616e-02, 0.0000e+00, 2.3248e-02,  ..., 0.0000e+00,\n",
      "          5.0654e-01, 9.5626e-01],\n",
      "         [4.3413e-02, 7.2025e-01, 1.7106e-02,  ..., 0.0000e+00,\n",
      "          8.5012e-01, 1.5115e-01]]])), (tensor([[[0.1463, 0.1009, 0.4671, 0.9375],\n",
      "         [0.1166, 0.1961, 0.5153, 0.8779],\n",
      "         [0.2349, 0.1589, 0.7929, 0.8452],\n",
      "         [0.2817, 0.0899, 0.8751, 0.8111],\n",
      "         [0.2211, 0.0691, 0.5671, 0.8836],\n",
      "         [0.3521, 0.0679, 0.7844, 0.7526],\n",
      "         [0.4129, 0.0284, 0.8548, 0.7461],\n",
      "         [0.6777, 0.1749, 0.9609, 0.9993],\n",
      "         [0.7457, 0.1253, 0.9982, 0.9752],\n",
      "         [0.0861, 0.4268, 0.5073, 0.9942]]]), tensor([[[3.9566e-03, 4.4224e-02, 6.9572e-03,  ..., 2.5729e-03,\n",
      "          3.0232e+00, 1.7668e-02],\n",
      "         [0.0000e+00, 4.7158e-02, 0.0000e+00,  ..., 1.1799e-02,\n",
      "          5.3217e+00, 0.0000e+00],\n",
      "         [1.6921e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.2732e+00, 1.2857e-01],\n",
      "         ...,\n",
      "         [5.2455e-01, 1.3978e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          7.0807e-01, 1.0404e+00],\n",
      "         [3.4256e-01, 1.1142e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.5208e-01, 1.0892e+00],\n",
      "         [0.0000e+00, 5.5926e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          5.1841e+00, 9.5806e-03]]])), (tensor([[[0.4352, 0.4927, 0.8531, 0.9960],\n",
      "         [0.3505, 0.0119, 0.7465, 0.6315],\n",
      "         [0.1130, 0.0802, 0.7040, 0.7825],\n",
      "         [0.1247, 0.0292, 0.4468, 0.8038],\n",
      "         [0.1782, 0.0160, 0.5978, 0.6093],\n",
      "         [0.4141, 0.3316, 0.8386, 0.9932],\n",
      "         [0.2597, 0.0792, 0.8724, 0.7609],\n",
      "         [0.2802, 0.0142, 0.6841, 0.6078],\n",
      "         [0.4821, 0.1796, 0.8832, 0.8748],\n",
      "         [0.2247, 0.0555, 0.7369, 0.6487]]]), tensor([[[0.0000e+00, 1.4039e+00, 0.0000e+00,  ..., 1.9909e-01,\n",
      "          2.9878e+00, 8.7848e-01],\n",
      "         [7.9485e-02, 1.3571e+00, 0.0000e+00,  ..., 5.0994e-02,\n",
      "          1.2867e+01, 1.9589e-01],\n",
      "         [3.0233e-01, 2.0396e-01, 0.0000e+00,  ..., 4.5556e-01,\n",
      "          9.2440e+00, 1.4431e-01],\n",
      "         ...,\n",
      "         [3.7150e-02, 1.7819e+00, 7.0822e-03,  ..., 2.5883e-01,\n",
      "          1.2115e+01, 6.0239e-02],\n",
      "         [4.3560e-02, 3.2013e-01, 0.0000e+00,  ..., 2.9388e-01,\n",
      "          5.6106e+00, 1.3434e+00],\n",
      "         [3.3040e-01, 7.8411e-01, 0.0000e+00,  ..., 3.4233e-01,\n",
      "          1.6405e+01, 5.6803e-02]]])), (tensor([[[0.8332, 0.7394, 0.9976, 0.9989],\n",
      "         [0.3494, 0.1080, 0.9050, 0.7804],\n",
      "         [0.7450, 0.1548, 0.9960, 0.9884],\n",
      "         [0.2106, 0.0188, 0.7677, 0.6841],\n",
      "         [0.0972, 0.0783, 0.6789, 0.7833],\n",
      "         [0.4384, 0.4942, 0.8523, 0.9956],\n",
      "         [0.1363, 0.0151, 0.5481, 0.6201],\n",
      "         [0.2160, 0.2614, 0.8392, 0.9111],\n",
      "         [0.1304, 0.0291, 0.4438, 0.8038],\n",
      "         [0.2588, 0.0097, 0.6727, 0.4648]]]), tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.3271e-01, 0.0000e+00],\n",
      "         [1.9403e-01, 3.9709e-02, 0.0000e+00,  ..., 2.0289e-01,\n",
      "          8.2789e+00, 1.4623e+00],\n",
      "         [2.6876e-02, 6.5987e-02, 0.0000e+00,  ..., 2.9279e-03,\n",
      "          1.3627e-01, 2.7273e-02],\n",
      "         ...,\n",
      "         [3.0870e-02, 1.5352e-01, 0.0000e+00,  ..., 4.7568e-01,\n",
      "          4.0703e+00, 1.6650e+00],\n",
      "         [4.4704e-01, 7.3965e-01, 7.5767e-02,  ..., 2.6727e-02,\n",
      "          8.5273e+00, 5.7962e-01],\n",
      "         [2.9718e-01, 2.0962e+00, 6.6505e-03,  ..., 7.4343e-01,\n",
      "          1.1563e+01, 3.2141e-03]]])), (tensor([[[0.2992, 0.0497, 0.8835, 0.7315],\n",
      "         [0.1461, 0.0830, 0.7145, 0.7500],\n",
      "         [0.4362, 0.4915, 0.8511, 0.9956],\n",
      "         [0.3591, 0.1424, 0.9090, 0.8191],\n",
      "         [0.7761, 0.1781, 0.9984, 0.9992],\n",
      "         [0.2408, 0.1724, 0.8323, 0.8391],\n",
      "         [0.2078, 0.0360, 0.6309, 0.7698],\n",
      "         [0.2505, 0.0201, 0.6948, 0.6970],\n",
      "         [0.3208, 0.2665, 0.8991, 0.9446],\n",
      "         [0.1246, 0.0293, 0.4406, 0.8087]]]), tensor([[[ 0.3065,  0.6410,  0.0000,  ...,  0.0807, 11.1919,  0.6701],\n",
      "         [ 0.4776,  0.3273,  0.0000,  ...,  0.2795, 11.3954,  0.3817],\n",
      "         [ 0.0000,  1.3711,  0.0000,  ...,  0.3679,  2.1720,  0.8870],\n",
      "         ...,\n",
      "         [ 0.1312,  1.2637,  0.0000,  ...,  0.1714, 12.5895,  0.1677],\n",
      "         [ 0.0217,  0.2071,  0.0000,  ...,  0.2191,  2.2497,  1.7735],\n",
      "         [ 0.8428,  0.7252,  0.0527,  ...,  0.0137,  8.1720,  0.6399]]])), (tensor([[[0.4932, 0.1676, 0.8920, 0.8893],\n",
      "         [0.4187, 0.4876, 0.8490, 0.9943],\n",
      "         [0.4690, 0.3011, 0.8830, 0.9954],\n",
      "         [0.3127, 0.2797, 0.7350, 0.9738],\n",
      "         [0.5175, 0.0178, 0.8203, 0.7907],\n",
      "         [0.0123, 0.4059, 0.5960, 0.9991],\n",
      "         [0.0030, 0.5291, 0.5031, 0.9997],\n",
      "         [0.0722, 0.3594, 0.6888, 0.9285],\n",
      "         [0.5202, 0.1261, 0.8265, 0.9957],\n",
      "         [0.3161, 0.1231, 0.7477, 0.8724]]]), tensor([[[1.4178e-01, 7.6162e-02, 0.0000e+00,  ..., 1.5171e-01,\n",
      "          4.6597e+00, 9.8847e-01],\n",
      "         [0.0000e+00, 7.1550e-01, 0.0000e+00,  ..., 3.1163e-01,\n",
      "          1.2586e+00, 3.1960e-01],\n",
      "         [9.6667e-03, 2.4262e-02, 0.0000e+00,  ..., 2.0339e-01,\n",
      "          1.5376e+00, 1.3364e+00],\n",
      "         ...,\n",
      "         [4.4787e-03, 0.0000e+00, 0.0000e+00,  ..., 3.4881e-02,\n",
      "          1.2560e+00, 5.0927e-01],\n",
      "         [0.0000e+00, 1.3031e-01, 1.2791e-02,  ..., 1.6124e-01,\n",
      "          2.2556e+00, 1.5259e+00],\n",
      "         [3.6483e-02, 1.0981e-01, 0.0000e+00,  ..., 3.4754e-02,\n",
      "          4.7532e+00, 4.9248e-01]]])), (tensor([[[0.1424, 0.0070, 0.5052, 0.9423],\n",
      "         [0.0665, 0.0122, 0.4863, 0.7784],\n",
      "         [0.0993, 0.0788, 0.6524, 0.7627],\n",
      "         [0.1304, 0.1859, 0.6948, 0.8448],\n",
      "         [0.0614, 0.0098, 0.6123, 0.6501],\n",
      "         [0.1152, 0.2038, 0.5392, 0.8829],\n",
      "         [0.2336, 0.1604, 0.8256, 0.8445],\n",
      "         [0.1061, 0.3149, 0.6767, 0.9474],\n",
      "         [0.1801, 0.2560, 0.7501, 0.9030],\n",
      "         [0.1466, 0.0013, 0.7430, 0.6675]]]), tensor([[[2.3728e-02, 2.5133e-03, 0.0000e+00,  ..., 1.6497e-01,\n",
      "          4.1049e+00, 1.3680e+00],\n",
      "         [1.6128e-02, 6.3711e-02, 0.0000e+00,  ..., 6.8153e-02,\n",
      "          1.0048e+01, 4.8024e-01],\n",
      "         [1.1291e-02, 2.1929e-02, 0.0000e+00,  ..., 9.5290e-02,\n",
      "          9.7890e+00, 2.2644e-01],\n",
      "         ...,\n",
      "         [3.4750e-02, 2.3054e-02, 0.0000e+00,  ..., 4.2099e-02,\n",
      "          2.4721e+00, 5.5158e-01],\n",
      "         [1.1273e-02, 1.3496e-02, 0.0000e+00,  ..., 1.7424e-01,\n",
      "          4.4455e+00, 4.5714e-01],\n",
      "         [2.9238e-02, 0.0000e+00, 0.0000e+00,  ..., 1.2415e-01,\n",
      "          1.3008e+01, 1.6970e-01]]])), (tensor([[[0.1118, 0.1002, 0.5720, 0.8219],\n",
      "         [0.1420, 0.0569, 0.7161, 0.7204],\n",
      "         [0.1402, 0.0201, 0.5560, 0.6845],\n",
      "         [0.0619, 0.0012, 0.5057, 0.7453],\n",
      "         [0.1912, 0.1106, 0.8335, 0.7685],\n",
      "         [0.1481, 0.1539, 0.7739, 0.8886],\n",
      "         [0.1028, 0.0000, 0.6849, 0.5528],\n",
      "         [0.0902, 0.0000, 0.4941, 0.4969],\n",
      "         [0.1974, 0.0000, 0.6401, 0.6443],\n",
      "         [0.0039, 0.1330, 0.5324, 0.7626]]]), tensor([[[1.0285e-01, 0.0000e+00, 2.0843e-02,  ..., 6.0131e-03,\n",
      "          5.5117e+00, 1.6729e+00],\n",
      "         [1.4496e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          7.4506e+00, 9.1021e-01],\n",
      "         [1.3857e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          6.8435e+00, 1.1256e+00],\n",
      "         ...,\n",
      "         [2.0047e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.0211e+01, 5.6536e-01],\n",
      "         [1.5866e-01, 0.0000e+00, 1.0595e-02,  ..., 0.0000e+00,\n",
      "          8.3363e+00, 1.0681e+00],\n",
      "         [5.3383e-01, 0.0000e+00, 1.8971e-02,  ..., 0.0000e+00,\n",
      "          8.9814e+00, 3.0820e-01]]])), (tensor([[[0.1284, 0.0600, 0.7016, 0.7433],\n",
      "         [0.1507, 0.0000, 0.7205, 0.5812],\n",
      "         [0.1273, 0.0017, 0.5027, 0.9604],\n",
      "         [0.1129, 0.0000, 0.7604, 0.4094],\n",
      "         [0.1453, 0.1251, 0.7794, 0.8874],\n",
      "         [0.1372, 0.0817, 0.5814, 0.8224],\n",
      "         [0.0606, 0.0000, 0.5074, 0.7479],\n",
      "         [0.3181, 0.0000, 0.8852, 0.6161],\n",
      "         [0.2397, 0.0919, 0.8538, 0.7109],\n",
      "         [0.1149, 0.0000, 0.5373, 0.6127]]]), tensor([[[5.8361e-02, 0.0000e+00, 2.8874e-02,  ..., 0.0000e+00,\n",
      "          3.6556e+00, 1.4523e+00],\n",
      "         [6.4666e-02, 0.0000e+00, 2.6313e-02,  ..., 0.0000e+00,\n",
      "          7.7878e+00, 9.5233e-01],\n",
      "         [2.3982e-02, 1.7953e-02, 6.2742e-02,  ..., 0.0000e+00,\n",
      "          6.0602e-01, 3.0894e+00],\n",
      "         ...,\n",
      "         [3.1853e-01, 5.3441e-02, 1.2929e-03,  ..., 0.0000e+00,\n",
      "          7.6093e+00, 1.0924e+00],\n",
      "         [2.2244e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.1829e+00, 2.4055e+00],\n",
      "         [6.9682e-02, 1.9811e-02, 1.2366e-01,  ..., 0.0000e+00,\n",
      "          5.7777e+00, 7.4343e-01]]])), (tensor([[[0.1513, 0.0439, 0.7151, 0.7194],\n",
      "         [0.0616, 0.0019, 0.5044, 0.7456],\n",
      "         [0.1067, 0.0000, 0.7113, 0.5085],\n",
      "         [0.1260, 0.0000, 0.4965, 0.9603],\n",
      "         [0.3112, 0.0000, 0.8784, 0.6430],\n",
      "         [0.1646, 0.1858, 0.7601, 0.8969],\n",
      "         [0.0937, 0.1663, 0.5303, 0.8971],\n",
      "         [0.2089, 0.0000, 0.8519, 0.4423],\n",
      "         [0.2821, 0.2012, 0.8903, 0.9016],\n",
      "         [0.4028, 0.1174, 0.8265, 0.8473]]]), tensor([[[6.0626e-02, 0.0000e+00, 5.4980e-03,  ..., 0.0000e+00,\n",
      "          5.8271e+00, 8.3331e-01],\n",
      "         [3.1145e-01, 0.0000e+00, 4.8416e-02,  ..., 1.2957e-02,\n",
      "          9.3978e+00, 5.1550e-01],\n",
      "         [1.2622e-01, 0.0000e+00, 0.0000e+00,  ..., 1.2415e-02,\n",
      "          1.2293e+01, 2.8068e-01],\n",
      "         ...,\n",
      "         [1.8343e-01, 0.0000e+00, 0.0000e+00,  ..., 4.8266e-02,\n",
      "          1.4660e+01, 3.5602e-01],\n",
      "         [1.3884e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.9260e+00, 1.7529e+00],\n",
      "         [1.8992e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          5.2951e+00, 1.5798e+00]]])), (tensor([[[0.3545, 0.3166, 0.8162, 0.9206],\n",
      "         [0.4174, 0.5358, 0.8503, 0.9933],\n",
      "         [0.0539, 0.0953, 0.4731, 0.7547],\n",
      "         [0.2457, 0.1204, 0.6982, 0.8182],\n",
      "         [0.1132, 0.0791, 0.5384, 0.7897],\n",
      "         [0.3154, 0.3891, 0.7669, 0.9901],\n",
      "         [0.4897, 0.3391, 0.9074, 0.9911],\n",
      "         [0.4157, 0.1476, 0.8528, 0.8514],\n",
      "         [0.1714, 0.1068, 0.6233, 0.8004],\n",
      "         [0.1824, 0.0152, 0.6233, 0.6045]]]), tensor([[[0.0000e+00, 1.1166e-01, 1.9008e-03,  ..., 4.2511e-02,\n",
      "          3.2776e+00, 3.0454e-01],\n",
      "         [5.6250e-04, 2.4529e-02, 0.0000e+00,  ..., 1.0924e-02,\n",
      "          8.2807e-01, 2.4322e-01],\n",
      "         [2.6642e-02, 2.6496e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          6.6655e+00, 6.3044e-01],\n",
      "         ...,\n",
      "         [8.2727e-02, 6.9723e-02, 0.0000e+00,  ..., 3.2366e-02,\n",
      "          4.6624e+00, 3.4283e-02],\n",
      "         [1.7099e-02, 1.5710e-01, 1.0019e-04,  ..., 0.0000e+00,\n",
      "          4.0308e+00, 7.5967e-01],\n",
      "         [1.7035e-02, 3.3337e-01, 0.0000e+00,  ..., 3.8818e-02,\n",
      "          7.0678e+00, 5.3966e-01]]])), (tensor([[[0.4129, 0.4287, 0.8475, 0.9924],\n",
      "         [0.4009, 0.2080, 0.8387, 0.9188],\n",
      "         [0.3413, 0.3652, 0.7941, 0.9919],\n",
      "         [0.4715, 0.3505, 0.9023, 0.9922],\n",
      "         [0.4446, 0.1595, 0.8820, 0.8466],\n",
      "         [0.2427, 0.2659, 0.6848, 0.9572],\n",
      "         [0.2681, 0.3030, 0.8442, 0.9247],\n",
      "         [0.2551, 0.0812, 0.7104, 0.7892],\n",
      "         [0.3222, 0.1250, 0.7751, 0.8453],\n",
      "         [0.3988, 0.0000, 0.8289, 0.6467]]]), tensor([[[0.0000e+00, 1.3504e-01, 9.2726e-03,  ..., 0.0000e+00,\n",
      "          1.7534e+00, 5.1640e-01],\n",
      "         [2.1355e-02, 1.7138e-02, 5.9215e-03,  ..., 0.0000e+00,\n",
      "          2.9590e+00, 5.4424e-01],\n",
      "         [0.0000e+00, 2.9831e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.8329e-01, 1.0888e+00],\n",
      "         ...,\n",
      "         [1.8271e-05, 4.8643e-02, 0.0000e+00,  ..., 2.1689e-02,\n",
      "          5.6288e+00, 7.5262e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.6697e+00, 2.0666e-01],\n",
      "         [4.3378e-02, 2.9540e-01, 0.0000e+00,  ..., 1.7589e-02,\n",
      "          1.0145e+01, 1.2179e-01]]])), (tensor([[[0.1137, 0.0111, 0.5394, 0.5855],\n",
      "         [0.4218, 0.3517, 0.8597, 0.9908],\n",
      "         [0.3577, 0.3331, 0.8067, 0.9320],\n",
      "         [0.1344, 0.0799, 0.5736, 0.7738],\n",
      "         [0.2156, 0.1312, 0.6638, 0.8188],\n",
      "         [0.2081, 0.0125, 0.6526, 0.4711],\n",
      "         [0.0998, 0.2153, 0.5462, 0.9358],\n",
      "         [0.3907, 0.0000, 0.8163, 0.6073],\n",
      "         [0.3818, 0.0337, 0.8544, 0.7970],\n",
      "         [0.1973, 0.0000, 0.8199, 0.5525]]]), tensor([[[1.2142e-02, 5.9269e-03, 1.9770e-03,  ..., 3.9048e-02,\n",
      "          8.6958e+00, 2.3091e-02],\n",
      "         [0.0000e+00, 1.5859e-01, 1.7914e-04,  ..., 6.5599e-02,\n",
      "          1.6748e+00, 7.0685e-01],\n",
      "         [0.0000e+00, 1.0785e-01, 0.0000e+00,  ..., 1.1424e-01,\n",
      "          1.7804e+00, 7.9910e-01],\n",
      "         ...,\n",
      "         [1.4979e-02, 1.0721e-01, 0.0000e+00,  ..., 2.0310e-01,\n",
      "          8.9562e+00, 4.5215e-03],\n",
      "         [0.0000e+00, 2.3580e-02, 0.0000e+00,  ..., 3.2154e-01,\n",
      "          6.9406e+00, 1.1606e-01],\n",
      "         [1.6753e-02, 9.5189e-02, 0.0000e+00,  ..., 2.0605e-01,\n",
      "          9.6596e+00, 1.0749e-01]]])), (tensor([[[0.4115, 0.3628, 0.8487, 0.9915],\n",
      "         [0.3341, 0.1972, 0.7814, 0.8647],\n",
      "         [0.2720, 0.3741, 0.8332, 0.9342],\n",
      "         [0.1799, 0.1020, 0.6347, 0.7831],\n",
      "         [0.2436, 0.2783, 0.6885, 0.9489],\n",
      "         [0.2898, 0.1093, 0.7387, 0.7956],\n",
      "         [0.1650, 0.2261, 0.6318, 0.9050],\n",
      "         [0.1517, 0.0177, 0.6021, 0.6531],\n",
      "         [0.1393, 0.0118, 0.5828, 0.4882],\n",
      "         [0.0365, 0.2188, 0.5945, 0.8467]]]), tensor([[[2.3174e-02, 2.3684e-01, 0.0000e+00,  ..., 3.0810e-02,\n",
      "          1.7814e+00, 1.3486e+00],\n",
      "         [8.4257e-03, 9.9221e-02, 0.0000e+00,  ..., 3.1432e-02,\n",
      "          5.1078e+00, 5.4134e-01],\n",
      "         [3.3685e-02, 3.4980e-01, 1.1409e-02,  ..., 8.4314e-02,\n",
      "          3.0185e+00, 1.0119e+00],\n",
      "         ...,\n",
      "         [5.8805e-02, 6.1327e-02, 9.6337e-04,  ..., 0.0000e+00,\n",
      "          7.1611e+00, 2.0775e-02],\n",
      "         [1.1156e-01, 1.7740e-02, 2.3030e-02,  ..., 3.6539e-03,\n",
      "          7.0437e+00, 0.0000e+00],\n",
      "         [1.0652e-02, 1.6812e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.8235e+00, 9.9379e-02]]])), (tensor([[[0.3395, 0.1239, 0.5602, 0.5235],\n",
      "         [0.0521, 0.1350, 0.4536, 0.8614],\n",
      "         [0.0872, 0.0512, 0.4146, 0.9319],\n",
      "         [0.1849, 0.1313, 0.6177, 0.8441],\n",
      "         [0.1053, 0.0479, 0.5010, 0.7351],\n",
      "         [0.2768, 0.0131, 0.6544, 0.6889],\n",
      "         [0.0645, 0.2520, 0.4768, 0.9770],\n",
      "         [0.0992, 0.1797, 0.5234, 0.9107],\n",
      "         [0.1759, 0.0078, 0.5935, 0.5482],\n",
      "         [0.2932, 0.0635, 0.6202, 0.9594]]]), tensor([[[0.0000e+00, 2.0671e-01, 3.7503e-03,  ..., 2.4282e-02,\n",
      "          1.2446e+01, 1.4853e+00],\n",
      "         [6.3794e-02, 0.0000e+00, 0.0000e+00,  ..., 1.2555e-03,\n",
      "          4.9647e+00, 1.9319e-02],\n",
      "         [3.2664e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.1544e+00, 8.2446e-03],\n",
      "         ...,\n",
      "         [2.8128e-02, 0.0000e+00, 5.5132e-03,  ..., 2.1662e-02,\n",
      "          4.6707e+00, 3.5174e-02],\n",
      "         [1.7161e-01, 1.6588e-02, 0.0000e+00,  ..., 4.4329e-02,\n",
      "          1.6230e+01, 9.8394e-04],\n",
      "         [0.0000e+00, 1.4920e-02, 0.0000e+00,  ..., 1.3342e-02,\n",
      "          5.5989e+00, 5.0499e-01]]])), (tensor([[[0.2303, 0.0288, 0.8166, 0.6675],\n",
      "         [0.2634, 0.1424, 0.7056, 0.8709],\n",
      "         [0.1158, 0.1059, 0.7328, 0.8018],\n",
      "         [0.2412, 0.0000, 0.6009, 0.9394],\n",
      "         [0.1679, 0.1663, 0.6278, 0.9010],\n",
      "         [0.2828, 0.2030, 0.8257, 0.8385],\n",
      "         [0.2253, 0.2765, 0.8312, 1.0000],\n",
      "         [0.4342, 0.1088, 0.8811, 0.8556],\n",
      "         [0.0881, 0.2159, 0.7046, 0.8864],\n",
      "         [0.4254, 0.3698, 0.8402, 0.9978]]]), tensor([[[2.6069e-02, 3.5187e-01, 0.0000e+00,  ..., 5.8347e-01,\n",
      "          1.4776e+01, 6.3072e-03],\n",
      "         [2.7581e-02, 4.9861e-01, 5.2627e-03,  ..., 1.8144e-01,\n",
      "          7.0430e+00, 1.4353e-01],\n",
      "         [2.9714e-02, 1.0661e-01, 0.0000e+00,  ..., 3.0240e-01,\n",
      "          9.8647e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [1.3941e-04, 3.1137e-01, 0.0000e+00,  ..., 1.6111e-01,\n",
      "          1.0689e+01, 6.6338e-02],\n",
      "         [4.3724e-02, 9.1221e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.9283e+00, 3.4827e-02],\n",
      "         [6.4719e-03, 4.6582e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.8850e+00, 6.9710e-01]]])), (tensor([[[0.2710, 0.0779, 0.8574, 0.7755],\n",
      "         [0.2419, 0.0000, 0.8350, 0.5504],\n",
      "         [0.1565, 0.1731, 0.5923, 0.8904],\n",
      "         [0.2528, 0.0931, 0.6855, 0.7746],\n",
      "         [0.0618, 0.2140, 0.6740, 0.9036],\n",
      "         [0.4221, 0.0000, 0.8099, 0.9232],\n",
      "         [0.4329, 0.0430, 0.8766, 0.7561],\n",
      "         [0.2821, 0.0150, 0.6979, 0.6380],\n",
      "         [0.1723, 0.2666, 0.7912, 0.9278],\n",
      "         [0.2907, 0.2267, 0.8838, 0.8923]]]), tensor([[[1.3909e-03, 3.2083e-01, 0.0000e+00,  ..., 5.5204e-02,\n",
      "          1.1351e+01, 8.0088e-02],\n",
      "         [1.8592e-02, 6.3702e-01, 0.0000e+00,  ..., 3.3554e-01,\n",
      "          1.5032e+01, 9.9264e-02],\n",
      "         [0.0000e+00, 5.7977e-01, 2.2186e-04,  ..., 6.9663e-03,\n",
      "          3.4900e+00, 5.4773e-02],\n",
      "         ...,\n",
      "         [0.0000e+00, 7.0614e-01, 2.4984e-02,  ..., 7.2776e-02,\n",
      "          1.3089e+01, 1.3213e-01],\n",
      "         [0.0000e+00, 3.5057e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.9004e+00, 9.5130e-02],\n",
      "         [1.4323e-02, 3.9184e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          5.3125e+00, 6.5082e-02]]])), (tensor([[[0.1690, 0.0275, 0.5598, 0.9615],\n",
      "         [0.2752, 0.0062, 0.7036, 0.6385],\n",
      "         [0.7417, 0.0000, 0.9925, 0.6151],\n",
      "         [0.1764, 0.2363, 0.7906, 0.9112],\n",
      "         [0.2889, 0.2327, 0.8988, 0.8987],\n",
      "         [0.2737, 0.1125, 0.7066, 0.7845],\n",
      "         [0.0895, 0.2204, 0.6906, 0.8804],\n",
      "         [0.6380, 0.0000, 0.9909, 0.6628],\n",
      "         [0.6971, 0.0000, 0.9908, 0.4514],\n",
      "         [0.8217, 0.0025, 0.9950, 0.7071]]]), tensor([[[4.1144e-03, 1.2492e-01, 3.2988e-03,  ..., 8.1529e-03,\n",
      "          2.7688e+00, 1.4195e-02],\n",
      "         [6.4354e-03, 8.2534e-01, 1.3257e-02,  ..., 9.0730e-02,\n",
      "          1.4194e+01, 4.1512e-02],\n",
      "         [1.6605e-01, 1.3746e-01, 0.0000e+00,  ..., 7.9554e-03,\n",
      "          7.5174e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [1.6416e-01, 1.5000e-01, 0.0000e+00,  ..., 9.1971e-02,\n",
      "          9.4902e+00, 0.0000e+00],\n",
      "         [2.8134e-01, 9.7118e-02, 0.0000e+00,  ..., 3.6080e-02,\n",
      "          8.4704e+00, 0.0000e+00],\n",
      "         [1.2870e-01, 2.7266e-02, 0.0000e+00,  ..., 4.2578e-03,\n",
      "          4.9396e+00, 2.2048e-03]]])), (tensor([[[8.0764e-04, 3.7782e-01, 2.6814e-01, 9.9605e-01],\n",
      "         [6.7337e-03, 6.5750e-01, 3.8625e-01, 9.9958e-01],\n",
      "         [3.8963e-03, 1.0803e-02, 3.5888e-01, 5.6548e-01],\n",
      "         [1.5003e-01, 1.5284e-01, 4.4832e-01, 9.8441e-01],\n",
      "         [7.9364e-02, 2.0140e-02, 4.5245e-01, 7.1975e-01],\n",
      "         [0.0000e+00, 1.1751e-01, 2.5457e-01, 9.0252e-01],\n",
      "         [3.3923e-01, 1.0876e-01, 7.3261e-01, 8.4077e-01],\n",
      "         [1.9205e-02, 2.5885e-01, 4.0100e-01, 9.3751e-01],\n",
      "         [0.0000e+00, 2.7356e-01, 1.7688e-01, 9.9153e-01],\n",
      "         [6.0897e-01, 2.3972e-01, 8.5779e-01, 9.8615e-01]]]), tensor([[[2.3146e-02, 2.1788e-02, 8.9078e-03,  ..., 1.0336e-01,\n",
      "          2.8889e+00, 4.0893e-01],\n",
      "         [3.7742e-02, 4.2472e-01, 0.0000e+00,  ..., 5.5854e-02,\n",
      "          1.6170e+00, 1.3618e-01],\n",
      "         [3.0675e-01, 4.3436e-02, 0.0000e+00,  ..., 1.0310e-02,\n",
      "          1.3906e+01, 9.3243e-01],\n",
      "         ...,\n",
      "         [1.1948e-01, 8.1803e-02, 0.0000e+00,  ..., 5.9755e-02,\n",
      "          4.9060e+00, 4.2738e-01],\n",
      "         [5.2710e-03, 1.0098e-01, 0.0000e+00,  ..., 2.2415e-01,\n",
      "          3.7941e+00, 7.1246e-02],\n",
      "         [7.8496e-01, 7.6395e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          5.6966e+00, 1.9892e-01]]])), (tensor([[[8.9667e-01, 4.4590e-02, 9.9906e-01, 3.5465e-01],\n",
      "         [9.8660e-03, 6.5080e-01, 3.9247e-01, 1.0000e+00],\n",
      "         [2.7355e-04, 2.7782e-01, 2.0278e-01, 9.9436e-01],\n",
      "         [6.9034e-01, 2.5293e-03, 9.9056e-01, 4.2859e-01],\n",
      "         [3.8936e-03, 3.2948e-01, 3.4088e-01, 9.9559e-01],\n",
      "         [1.2050e-01, 0.0000e+00, 7.3213e-01, 6.9880e-01],\n",
      "         [5.4838e-01, 2.0655e-01, 8.3906e-01, 9.8667e-01],\n",
      "         [1.0922e-01, 3.3819e-01, 4.0298e-01, 9.9261e-01],\n",
      "         [2.0270e-01, 0.0000e+00, 8.4104e-01, 4.3278e-01],\n",
      "         [2.4253e-01, 0.0000e+00, 6.6314e-01, 6.0711e-01]]]), tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.2925e+00, 1.4113e-02],\n",
      "         [2.0385e-01, 2.3956e+00, 0.0000e+00,  ..., 2.1104e-02,\n",
      "          4.0181e+00, 0.0000e+00],\n",
      "         [7.6712e-04, 1.1207e-01, 0.0000e+00,  ..., 1.3975e-01,\n",
      "          1.9724e+00, 1.0488e-01],\n",
      "         ...,\n",
      "         [3.6522e-02, 8.9540e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          6.1459e+00, 6.5416e-02],\n",
      "         [1.0942e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3801e-01,\n",
      "          1.6910e+01, 0.0000e+00],\n",
      "         [4.1287e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.4507e+01, 2.8787e-01]]])), (tensor([[[7.4875e-01, 1.7869e-01, 9.9329e-01, 9.6199e-01],\n",
      "         [7.6232e-01, 7.6421e-03, 9.9403e-01, 8.6890e-01],\n",
      "         [0.0000e+00, 2.8685e-01, 2.4172e-01, 9.9079e-01],\n",
      "         [7.1238e-01, 4.6720e-01, 9.9219e-01, 9.9562e-01],\n",
      "         [0.0000e+00, 4.2615e-01, 2.9115e-01, 9.9521e-01],\n",
      "         [3.3110e-02, 4.7050e-01, 4.4503e-01, 9.9215e-01],\n",
      "         [2.9270e-04, 5.4058e-01, 3.5413e-01, 9.9432e-01],\n",
      "         [6.3281e-01, 1.7235e-01, 9.9339e-01, 9.1484e-01],\n",
      "         [0.0000e+00, 1.8054e-01, 3.1402e-01, 8.7398e-01],\n",
      "         [6.6546e-01, 2.9623e-01, 9.9333e-01, 9.9759e-01]]]), tensor([[[1.4094e-03, 0.0000e+00, 0.0000e+00,  ..., 3.6252e-03,\n",
      "          1.3809e+00, 0.0000e+00],\n",
      "         [3.4897e-02, 7.8201e-03, 0.0000e+00,  ..., 5.8402e-02,\n",
      "          2.6001e+00, 2.0560e-02],\n",
      "         [0.0000e+00, 5.3973e-02, 0.0000e+00,  ..., 8.8604e-02,\n",
      "          3.0402e+00, 1.6351e-02],\n",
      "         ...,\n",
      "         [3.8942e-04, 2.1435e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.3908e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7351e-02,  ..., 1.7211e-02,\n",
      "          5.1957e+00, 0.0000e+00],\n",
      "         [1.1872e-02, 2.5010e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          9.0666e-01, 4.8222e-03]]])), (tensor([[[0.4014, 0.1662, 0.8306, 0.8917],\n",
      "         [0.3621, 0.3010, 0.8050, 0.9854],\n",
      "         [0.3912, 0.4514, 0.8527, 1.0000],\n",
      "         [0.4293, 0.2771, 0.8743, 1.0000],\n",
      "         [0.5507, 0.1091, 0.9825, 0.8744],\n",
      "         [0.4028, 0.1901, 0.9896, 0.8751],\n",
      "         [0.5773, 0.2095, 1.0000, 0.9823],\n",
      "         [0.2307, 0.1469, 0.7935, 0.8620],\n",
      "         [0.2253, 0.3374, 0.7529, 0.9220],\n",
      "         [0.0000, 0.2035, 0.4486, 0.9907]]]), tensor([[[0.0000e+00, 7.6249e-02, 0.0000e+00,  ..., 4.8389e-03,\n",
      "          4.5401e+00, 2.3957e-02],\n",
      "         [0.0000e+00, 1.2423e-01, 0.0000e+00,  ..., 1.5828e-03,\n",
      "          1.3191e+00, 3.5876e-01],\n",
      "         [0.0000e+00, 3.3645e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.1143e+00, 5.1778e-01],\n",
      "         ...,\n",
      "         [1.6546e-04, 2.2373e-01, 0.0000e+00,  ..., 2.7433e-02,\n",
      "          6.3824e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 6.8238e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.9524e+00, 3.0237e-01],\n",
      "         [4.4231e-01, 7.6930e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.2079e+00, 1.2839e+00]]])), (tensor([[[5.0668e-01, 1.0753e-01, 7.2641e-01, 5.2178e-01],\n",
      "         [2.0846e-03, 3.9334e-01, 3.7553e-01, 9.9559e-01],\n",
      "         [6.3848e-02, 5.8822e-02, 4.8067e-01, 7.7501e-01],\n",
      "         [1.0987e-01, 1.6837e-01, 5.3345e-01, 8.2326e-01],\n",
      "         [4.7185e-04, 2.0347e-01, 3.5769e-01, 9.0658e-01],\n",
      "         [4.9087e-02, 7.9083e-02, 3.8673e-01, 9.6594e-01],\n",
      "         [9.6667e-02, 3.0936e-01, 5.0830e-01, 9.8902e-01],\n",
      "         [5.4659e-01, 1.9355e-01, 9.6651e-01, 9.4103e-01],\n",
      "         [2.4751e-03, 2.9658e-01, 4.4956e-01, 9.7612e-01],\n",
      "         [3.0466e-03, 4.3162e-01, 5.2189e-01, 9.9326e-01]]]), tensor([[[0.0000e+00, 3.5672e-01, 3.7229e-02,  ..., 9.3058e-04,\n",
      "          0.0000e+00, 1.3822e+00],\n",
      "         [3.7283e-02, 1.1375e-01, 0.0000e+00,  ..., 8.3011e-02,\n",
      "          2.0019e+00, 1.2446e-01],\n",
      "         [7.5914e-02, 3.0610e-02, 1.8348e-04,  ..., 4.6259e-02,\n",
      "          1.0236e+01, 4.5076e-02],\n",
      "         ...,\n",
      "         [4.2138e-02, 1.2544e+00, 0.0000e+00,  ..., 1.0252e-01,\n",
      "          1.1012e+00, 4.3118e-01],\n",
      "         [1.4036e-02, 1.2820e-01, 0.0000e+00,  ..., 1.2279e-01,\n",
      "          1.7267e+00, 1.4197e-01],\n",
      "         [1.0831e-01, 1.0170e-01, 0.0000e+00,  ..., 1.2236e-01,\n",
      "          1.3343e+00, 1.9480e-01]]])), (tensor([[[0.0015, 0.4138, 0.2901, 0.9950],\n",
      "         [0.2117, 0.1732, 0.8435, 0.8377],\n",
      "         [0.3332, 0.2015, 0.8022, 0.9138],\n",
      "         [0.0693, 0.3312, 0.4741, 0.9545],\n",
      "         [0.3281, 0.4491, 0.7841, 0.9977],\n",
      "         [0.0025, 0.4889, 0.5095, 0.9941],\n",
      "         [0.0761, 0.6089, 0.6687, 0.9952],\n",
      "         [0.1829, 0.3626, 0.6471, 1.0000],\n",
      "         [0.0019, 0.2718, 0.4173, 0.9634],\n",
      "         [0.0052, 0.6111, 0.4704, 0.9960]]]), tensor([[[2.9178e-02, 4.0314e-01, 0.0000e+00,  ..., 2.3843e-01,\n",
      "          2.6709e+00, 3.8018e-02],\n",
      "         [0.0000e+00, 1.1789e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          8.0854e+00, 5.9179e-02],\n",
      "         [0.0000e+00, 4.2435e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.0869e+00, 2.2148e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 5.4760e-01, 9.1822e-04,  ..., 0.0000e+00,\n",
      "          3.4869e+00, 2.3669e-01],\n",
      "         [2.3639e-03, 1.7497e-01, 5.4706e-03,  ..., 9.5614e-02,\n",
      "          3.2218e+00, 4.9405e-02],\n",
      "         [4.1432e-02, 4.2777e-01, 0.0000e+00,  ..., 1.2772e-01,\n",
      "          2.0056e+00, 1.7319e-02]]])), (tensor([[[0.0537, 0.2429, 0.6301, 0.8370],\n",
      "         [0.2155, 0.2168, 0.6712, 0.8569],\n",
      "         [0.2343, 0.0880, 0.6868, 0.7869],\n",
      "         [0.3411, 0.0969, 0.7783, 0.7912],\n",
      "         [0.3122, 0.2058, 0.7670, 0.8873],\n",
      "         [0.1446, 0.6535, 0.7496, 0.9926],\n",
      "         [0.4106, 0.4537, 0.8502, 0.9888],\n",
      "         [0.0968, 0.1806, 0.5257, 0.8435],\n",
      "         [0.1930, 0.2730, 0.6261, 0.9502],\n",
      "         [0.1125, 0.4288, 0.6985, 0.9892]]]), tensor([[[0.0000e+00, 1.7364e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          6.1128e+00, 2.5947e-01],\n",
      "         [0.0000e+00, 5.8184e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.8935e+00, 5.2904e-01],\n",
      "         [0.0000e+00, 6.4018e-01, 0.0000e+00,  ..., 2.8674e-02,\n",
      "          5.2638e+00, 5.9709e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.4089e+00, 8.5072e-01],\n",
      "         [0.0000e+00, 3.8187e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.2969e+00, 5.8033e-01],\n",
      "         [1.6120e-03, 7.8660e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.1871e+00, 2.9818e-01]]])), (tensor([[[0.2927, 0.3098, 0.7595, 0.9876],\n",
      "         [0.0938, 0.0000, 0.5621, 0.7366],\n",
      "         [0.1934, 0.2544, 0.8104, 0.8990],\n",
      "         [0.4077, 0.2470, 0.8421, 0.9468],\n",
      "         [0.3380, 0.4251, 0.7991, 0.9973],\n",
      "         [0.3209, 0.1838, 0.7846, 0.8936],\n",
      "         [0.0567, 0.2363, 0.6054, 0.8211],\n",
      "         [0.2475, 0.1119, 0.8600, 0.7935],\n",
      "         [0.1076, 0.0049, 0.7107, 0.6491],\n",
      "         [0.1575, 0.0732, 0.7724, 0.7583]]]), tensor([[[0.0000e+00, 2.6866e-01, 0.0000e+00,  ..., 3.6885e-02,\n",
      "          1.8153e+00, 9.0406e-01],\n",
      "         [0.0000e+00, 5.3994e-02, 0.0000e+00,  ..., 3.0720e-02,\n",
      "          8.3861e+00, 2.3812e-01],\n",
      "         [0.0000e+00, 6.0805e-02, 0.0000e+00,  ..., 1.9741e-01,\n",
      "          3.0253e+00, 9.3532e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 2.8596e-01, 0.0000e+00,  ..., 5.0938e-01,\n",
      "          9.0223e+00, 1.5404e-01],\n",
      "         [6.6156e-03, 2.6626e-01, 0.0000e+00,  ..., 3.1229e-01,\n",
      "          9.0947e+00, 2.3081e-01],\n",
      "         [0.0000e+00, 4.5373e-01, 0.0000e+00,  ..., 6.7083e-01,\n",
      "          8.5770e+00, 2.6473e-01]]]))]\n",
      "['add garram masala seeds and a bay leaf to the oil', 'add the lamb to the pot', 'add garlic ginger paste and chopped onions to the pot', 'add chili tumeric coriander cumin and salt', 'add water to the pot', 'add potatos to the pot', 'add the tomatos to the pot', 'add chili to the pot']\n"
     ]
    }
   ],
   "source": [
    "print(vid_id)\n",
    "print(candidates)\n",
    "print(actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Old Cell\n",
    "\n",
    "# # USAGE: Run this to load all candidate and actions data from disk\n",
    "# all_candidates, all_actions = load_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(pickles_root, actions_list=[]):\n",
    "    \"\"\"\n",
    "    Tokenize actions_list\n",
    "    pickles_root: directory path to pickled data\n",
    "    actions_list: list of action annotations for a single video\n",
    "    Return steps: a single tokenized string representing all steps annotations for a single video\n",
    "    Return entity_count: list of entity counts per action step of a single video\n",
    "    Return entities: list of list of entities within each action step of a single video\n",
    "    Return indices: list of lists of indices indicating entity spans within each action step of a single video\n",
    "    \"\"\"\n",
    "    # Strip all whitespaces and periods\n",
    "    # Note, periods will be added back later; need to temporarily remove periods before passing into parser\n",
    "    \n",
    "    actions = [action.strip('.') for action in actions_list]\n",
    "    print(actions)\n",
    "    \n",
    "    NULL = '[unused1]'\n",
    "    ENTITY = '[unused2]'\n",
    "    ACTION = '[unused3]'\n",
    "    \n",
    "    max_step_length = max(0, max([len(action.split()) for action in actions]))    # maximum word count in a single action step\n",
    "    print(max_step_length)\n",
    "    \n",
    "    tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, ENTITY]})   # TODO: check if list should contain ACTION as well\n",
    "    tokenizer.encode([NULL, ENTITY], add_special_tokens=True)    # TODO: check if this tokenization is required\n",
    "    \n",
    "    entities, indices = parse(actions, max_step_length=max_step_length)\n",
    "    entities.append([NULL])\n",
    "    entity_count = [len(entity) for entity in entities]\n",
    "    \n",
    "    # insert in reverse so preceeding word indices can still be used for modified actions\n",
    "    for ind in reversed(indices):\n",
    "        action_idx = ind[0]//max_step_length\n",
    "        entity_idx = ind[0]%max_step_length\n",
    "        words = actions[action_idx].split()\n",
    "        words.insert(entity_idx, ENTITY)\n",
    "        actions[action_idx] = ' '.join(words)\n",
    "\n",
    "    actions = [action + '.' if not action.endswith('.') else action for action in actions]\n",
    "\n",
    "    tokens_steps = tokenizer(\n",
    "                    actions,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "#                     add_special_tokens=True,\n",
    "                    add_special_tokens=False,\n",
    "#                     padding=\"max_length\",\n",
    "                    max_length=max_step_length + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "    tokens_steps = tokens_steps['input_ids'].flatten()\n",
    "    tokens_steps = tokens_steps[tokens_steps != 101]\n",
    "    tokens_steps = tokenizer.decode(tokens_steps) + ' ' + NULL + ' ' + ACTION    # TODO: check if this is correct\n",
    "    steps = tokens_steps\n",
    "    \n",
    "    pickle_data(steps, pickles_root, 'steps')\n",
    "    pickle_data(entity_count, pickles_root, 'entity_count')\n",
    "    pickle_data(entities, pickles_root, 'entities')\n",
    "    pickle_data(indices, pickles_root, 'indices')\n",
    "    \n",
    "    return steps, entity_count, entities, indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['add garram masala seeds and a bay leaf to the oil', 'add the lamb to the pot', 'add garlic ginger paste and chopped onions to the pot', 'add chili tumeric coriander cumin and salt', 'add water to the pot', 'add potatos to the pot', 'add the tomatos to the pot', 'add chili to the pot']\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to return token_type_ids while setting add_special_tokens to False results in an undefined behavior. Please set add_special_tokens to True or set return_token_type_ids to None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f4d71bad041e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# USAGE: Run this to tokenize actions_list of a single video and save data on disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickles_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/h/mkhan/ece496-capstone/datasets/ycii/8/00000/pickles'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickles_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-01280b15f294>\u001b[0m in \u001b[0;36mtokenize_sample\u001b[0;34m(pickles_root, actions_list)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#                     padding=\"max_length\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_step_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 )\n\u001b[1;32m     52\u001b[0m     \u001b[0mtokens_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd002/home/mkhan/ece496-capstone/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2136\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m             )\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd002/home/mkhan/ece496-capstone/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2321\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2324\u001b[0m         )\n\u001b[1;32m   2325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd002/home/mkhan/ece496-capstone/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd002/home/mkhan/ece496-capstone/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# We convert the whole batch to tensors at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             )\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd002/home/mkhan/ece496-capstone/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2617\u001b[0;31m                 \u001b[0;34m\"Asking to return token_type_ids while setting add_special_tokens to False \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2618\u001b[0m                 \u001b[0;34m\"results in an undefined behavior. Please set add_special_tokens to True or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2619\u001b[0m                 \u001b[0;34m\"set return_token_type_ids to None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to return token_type_ids while setting add_special_tokens to False results in an undefined behavior. Please set add_special_tokens to True or set return_token_type_ids to None."
     ]
    }
   ],
   "source": [
    "# Required Cell\n",
    "\n",
    "# USAGE: Run this to tokenize actions_list of a single video and save data on disk\n",
    "pickles_root = '/h/mkhan/ece496-capstone/datasets/ycii/8/00000/pickles'\n",
    "steps, entity_count, entities, indices = tokenize_sample(pickles_root, actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e040be167eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'steps' is not defined"
     ]
    }
   ],
   "source": [
    "print(steps)\n",
    "print(entity_count)\n",
    "print(entities)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old Cell (do NOT run)\n",
    "# # Handles building steps and entity_count\n",
    "\n",
    "\n",
    "# # for actions in all_actions:\n",
    "# #     for i, action in enumerate(actions):\n",
    "# #         print(str(i) + ': ' + action)\n",
    "\n",
    "\n",
    "# # Strip all whitespaces and periods\n",
    "# # Note, periods will be added back later; need to temporarily remove periods before passing into parser\n",
    "# all_actions = [[action.strip('.') for action in actions] for actions in all_actions]\n",
    "\n",
    "# NULL = '[unused1]'\n",
    "# ENTITY = '[unused2]'\n",
    "# ACTION = '[unused3]'\n",
    "\n",
    "# # TODO: iterate through all annotations to find max_step_length\n",
    "# MAX_STEP_LENGTH = 30\n",
    "\n",
    "# tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\", pad_token=PAD)\n",
    "# tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "# tokenizer.encode([NULL, PAD, ENTITY], add_special_tokens=True)\n",
    "\n",
    "# # TODO: iterate through all annotations to find max_step_length\n",
    "# max_step_length = 30\n",
    "\n",
    "# entity_count = []\n",
    "# steps = []\n",
    "\n",
    "# for idx in range(len(all_actions)):\n",
    "    \n",
    "#     entities, indices = parse(all_actions[idx], max_step_length=max_step_length)\n",
    "#     entities.append([NULL])\n",
    "#     entity_count.append([len(en) for en in entities])\n",
    "    \n",
    "#     # insert in reverse so preceeding word indices can still be used for modified actions\n",
    "#     for ind in reversed(indices):\n",
    "#         action_idx = ind[0]//max_step_length\n",
    "#         entity_idx = ind[0]%max_step_length\n",
    "#         words = all_actions[idx][action_idx].split()\n",
    "#         words.insert(entity_idx, ENTITY)\n",
    "#         all_actions[idx][action_idx] = ' '.join(words)\n",
    "\n",
    "#         all_actions[idx] = [action + '.' if not action.endswith('.') else action for action in all_actions[idx]]\n",
    "\n",
    "#     tokens_steps = tokenizer(\n",
    "#                     all_actions[idx],\n",
    "#                     return_token_type_ids=False,\n",
    "#                     return_attention_mask=False,\n",
    "#                     add_special_tokens=True,\n",
    "#                     padding=\"max_length\",\n",
    "#                     max_length=MAX_STEP_LENGTH + 2,\n",
    "#                     return_tensors=\"pt\"\n",
    "#                 )\n",
    "#     tokens_steps = tokens_steps['input_ids'].flatten()\n",
    "#     tokens_steps = tokens_steps[tokens_steps != 101]\n",
    "#     tokens_steps = tokenizer.decode(tokens_steps) + ' ' + NULL\n",
    "#     steps.append(tokens_steps)\n",
    "\n",
    "\n",
    "# # for actions in all_actions:\n",
    "# #     for i, action in enumerate(actions):\n",
    "# #         print(str(i) + ': ' + action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old Cell (do NOT run)\n",
    "#\n",
    "# for idx in range(len(all_actions)):\n",
    "#     print(idx)\n",
    "#     print(len(all_actions[idx]))\n",
    "#     for action in all_actions[idx]:\n",
    "#         print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP Cell: previously, all_candidates contained candidates from multiple videos.\n",
    "# This needs to be adjusted given a single video input\n",
    "# If a single video is used at a time,\n",
    "# try replacing \"candidate[0].numpy() for candidate in all_candidates\"\n",
    "# with \"candidates.numpy()\"\n",
    "# And check if dimensionality is as expected for model input\n",
    "\n",
    "# Reshape all of the candidate bounding box and feature tensors.\n",
    "boxes = torch.tensor([candidate[0].numpy() for candidate in all_candidates]).squeeze(1)\n",
    "features = torch.tensor([candidate[1].numpy() for candidate in all_candidates]).squeeze(1)\n",
    "\n",
    "boxes = boxes.flatten(start_dim=0, end_dim=1)\n",
    "features = features.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "# TODO: please check this stacking here. This was originally adapted from the train notebook.\n",
    "# But I am not sure why/whether this is needed\n",
    "boxes = torch.stack((boxes, boxes))\n",
    "features = torch.stack((features, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entity_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_frame(action_id, action_start, action_end, num_frames_per_step):\n",
    "    \"\"\"\n",
    "    Returns first frame name of a given action, and time delay into the segment when the frame appears (in sec)\n",
    "    action_id: id of action within video\n",
    "    action_start: start time (in sec) of action\n",
    "    action_end: end time (in sec) of action\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    Returns frame_name: name of first frame for an action\n",
    "    Returns action_delta: time delay into the segment when the frame appears (in sec)\n",
    "    \"\"\"\n",
    "    action_delta = (action_end - action_start) / (num_frames_per_step + 1)    # need num_frames_per_step+1 intervals for num_frames_per_step inner frames\n",
    "    frame_time = action_start + action_delta    # in seconds\n",
    "    frame_id = int( frame_time*(num_frames_per_step + 1) )\n",
    "    frame_name = '{}.jpg'.format(str(frame_id).zfill(6))\n",
    "    return frame_name, action_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets', num_frames_per_step=4):\n",
    "    \"\"\"\n",
    "    Visualize the first frame of each step in dataset videos\n",
    "    dataset_root: directory path to dataset base\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    \"\"\"\n",
    "\n",
    "    vids_list = os.listdir(os.path.join(dataset_root, 'ycii_frames'))\n",
    "    annotations_bb_all = read_json(os.path.join(dataset_root, 'yc2_bb/annotations', 'yc2_bb_val_annotations.json'))['database']\n",
    "    \n",
    "    detector = Detector()\n",
    "#     visualize_candidates = []\n",
    "    \n",
    "    for vid_id in vids_list:\n",
    "        print('vid_id: ' + vid_id)\n",
    "\n",
    "        annotations_bb_vid = annotations_bb_all[vid_id]\n",
    "        vid_segments = annotations_bb_vid['segments']\n",
    "\n",
    "        annot_width = annotations_bb_vid['rwidth']\n",
    "        annot_height = annotations_bb_vid['rheight']\n",
    "        print('annotated image size: ' + str(annot_width) + ', ' + str(annot_height))\n",
    "\n",
    "        for segment_id in sorted(vid_segments):\n",
    "#             print(segment_id)    # 'segment_id'th segment of the video 'vid_id'\n",
    "#             print(vid_segments[segment_id]['segment'])    # displays start and end time (in sec) of the segment\n",
    "\n",
    "            frame_name, segment_delta = get_first_frame(segment_id, vid_segments[segment_id]['segment'][0], vid_segments[segment_id]['segment'][1], num_frames_per_step)\n",
    "            print('frame_name: ' + frame_name)\n",
    "            print('segment_start: ' + str(vid_segments[segment_id]['segment'][0]))\n",
    "            print('segment_delta: ' + str(segment_delta))            \n",
    "            \n",
    "            segment_frame_id = int(round(segment_delta))\n",
    "            \n",
    "            # read the image from disk\n",
    "            img = Image.open(os.path.join(dataset_root, 'ycii_frames', vid_id, frame_name))\n",
    "\n",
    "            # get saved image dimensions\n",
    "            saved_width = img.size[0]\n",
    "            saved_height = img.size[1]\n",
    "            print('saved image size: ' + str(saved_width) + ', ' + str(saved_height))\n",
    "            \n",
    "            # create figure and axes\n",
    "            fig, ax = plt.subplots()\n",
    "            \n",
    "            # display the image on the plot\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # add ground truth bounding boxes\n",
    "            for entity in vid_segments[segment_id]['objects']:\n",
    "                label = entity['label']\n",
    "                bboxes = entity['boxes']\n",
    "                bbox = bboxes[segment_frame_id]\n",
    "                \n",
    "                if (bbox['occluded']==1):\n",
    "                    continue\n",
    "                \n",
    "#                 print(bbox) \n",
    "                print('label: ' + label)\n",
    "                \n",
    "                left = int(bbox['xtl'] / annot_width * saved_width)\n",
    "                top = int(bbox['ytl'] / annot_height * saved_height)\n",
    "                width = int( (bbox['xbr'] - bbox['xtl']) / annot_width * saved_width)\n",
    "                height = int( (bbox['ybr'] - bbox['ytl']) / annot_height * saved_height)\n",
    "                \n",
    "\n",
    "                # Create a Rectangle patch ( (x, y), width, height )\n",
    "                rect = patches.Rectangle((left, top), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
    "                # Add the patch to the Axes\n",
    "                ax.add_patch(rect)\n",
    "            \n",
    "            \n",
    "            # get candidates for images\n",
    "            candidates = detector.inference(os.path.join(dataset_root, 'ycii_frames', vid_id, frame_name), max_detections=10)\n",
    "#             print(candidates)\n",
    "#             visualize_candidates.append(candidates)\n",
    "            boxes = torch.tensor([candidates[0].numpy()]).squeeze(1)\n",
    "            boxes = boxes.flatten(start_dim=0, end_dim=1)\n",
    "            boxes = boxes.numpy().tolist()\n",
    "            \n",
    "            # add detector candidate bounding boxes\n",
    "            for candidate_box in boxes:\n",
    "                print(candidate_box)\n",
    "                \n",
    "                left = int(round(candidate_box[0] * saved_width))\n",
    "                top = int(round(candidate_box[1] * saved_height))\n",
    "                width = int( ( candidate_box[2] - candidate_box[0] ) * saved_width )\n",
    "                height = int( ( candidate_box[3] - candidate_box[1] ) * saved_height )\n",
    "                \n",
    "                # Create a Rectangle patch ( (x, y), width, height )\n",
    "                rect = patches.Rectangle((left, top), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "                # Add the patch to the Axes\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            # display the plot\n",
    "            plt.show()\n",
    "#             return visualize_candidates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Cell\n",
    "# USAGE: Run this to display first frame of each step in dataset videos\n",
    "visualize_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets', num_frames_per_step=4)\n",
    "# candidates = visualize_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets', num_frames_per_step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Do not execute the cells below, they are rough code for testing stuff out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boxes.size())\n",
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for vid in os.listdir(os.path.join('/h/mkhan/ece496-capstone/datasets/ycii_frames/', '')):\n",
    "    print(os.listdir(os.path.join('/h/mkhan/ece496-capstone/datasets/ycii_frames/', vid)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8+11+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "dataset_root = '/h/mkhan/ece496-capstone/datasets'\n",
    "annotations = read_json(os.path.join(dataset_root, 'annotations', 'ycii_annotations_trainval.json'))['database']\n",
    "max_len = 0\n",
    "min_len = 1000\n",
    "actions_lenths = []\n",
    "steps_lenths = []\n",
    "vid_count = 0\n",
    "min_duration = 100\n",
    "\n",
    "with open(os.path.join(dataset_root, 'vid_list', 'vid_list_ycii_train.txt')) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        rcp_type,vid_id = line.replace('\\n','').split('/')\n",
    "#         print('[INFO] Processing video {}'.format(vid_id))\n",
    "        actions = annotations[vid_id]['annotations']\n",
    "        actions_lenths.append(len(actions))\n",
    "        for action in actions:\n",
    "#             print(len(action['sentence'].split()))\n",
    "            steps_lenths.append(len(action['sentence'].split()))\n",
    "            \n",
    "#         print(actions[0]['segment'])\n",
    "#         print()\n",
    "        if min_duration > actions[0]['segment'][1] - actions[0]['segment'][0]:\n",
    "            min_duration = actions[0]['segment'][1] - actions[0]['segment'][0]\n",
    "        \n",
    "#         print(len(actions))\n",
    "        if max_len < len(actions):\n",
    "            max_len = len(actions)\n",
    "        if min_len > len(actions):\n",
    "            min_len = len(actions)\n",
    "        vid_count += 1\n",
    "\n",
    "# print(max_len)\n",
    "\n",
    "# arr = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 5, 5] \n",
    "freq = collections.Counter(actions_lenths)\n",
    "word_len_freq = collections.Counter(steps_lenths)\n",
    "# print(freq)\n",
    "\n",
    "for key in sorted(freq):\n",
    "    print(\"%s: %s\" % (key, freq[key]))\n",
    "print('\\n')\n",
    "for key in sorted(word_len_freq):\n",
    "    print(\"%s: %s\" % (key, word_len_freq[key]))\n",
    "\n",
    "print('# of Videos: ' + str(vid_count))\n",
    "print('Max step count: ' + str(max_len))\n",
    "print('Min step count: ' + str(min_len))\n",
    "print('Min step duration: ' + str(min_duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = mpimg.imread('/h/mkhan/ece496-capstone/datasets/ycii_frames/fn9anlEL4FI/000069.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/h/mkhan/ece496-capstone/datasets/ycii_frames/fn9anlEL4FI/000159.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((50, 100), 40, 30, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(boxes.size())\n",
    "# print(boxes[0].size())\n",
    "print(boxes[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '/h/mkhan/ece496-capstone/datasets/'\n",
    "\n",
    "annotations = read_json(os.path.join(dataset_root, 'annotations', 'ycii_annotations_trainval.json'))['database']\n",
    "print(annotations['fn9anlEL4FI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '/h/mkhan/ece496-capstone/datasets'\n",
    "annotations_bb = read_json(os.path.join(dataset_root, 'yc2_bb/annotations', 'yc2_bb_val_annotations.json'))['database']\n",
    "# print(annotations_bb['fn9anlEL4FI'])\n",
    "# for key in annotations_bb:\n",
    "#     print(key)\n",
    "\n",
    "sample_vid = annotations_bb['fn9anlEL4FI']\n",
    "for key in sample_vid:\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in sorted(sample_vid['segments']):\n",
    "#     print(sample_vid['segments'][key])\n",
    "\n",
    "sample_segment = sample_vid['segments']['1']\n",
    "\n",
    "print(sample_segment['segment'])\n",
    "\n",
    "# for key in sorted(sample_segment):\n",
    "#     print(key)\n",
    "\n",
    "# print(sample_segment['objects'][0])\n",
    "# for key in sorted(sample_segment['objects'][0]):\n",
    "#     print(key)\n",
    "\n",
    "# print(sample_segment['objects'][0]['label'])\n",
    "# print(sample_segment['objects'][0]['boxes'])\n",
    "for bbox in sample_segment['objects'][0]['boxes']:\n",
    "    print(bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Do not execute the cells below, they are rough code for testing stuff out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_actions)\n",
    "\n",
    "for actions in all_actions:\n",
    "    for action in actions:\n",
    "        print(action)\n",
    "# actions = all_actions[0]\n",
    "# print(actions)\n",
    "# actions = [action + '.' if not action.endswith('.') else action for action in actions]\n",
    "# print(actions)\n",
    "\n",
    "# print(all_actions)\n",
    "for actions in all_actions:\n",
    "    for action in actions:\n",
    "        print(action)\n",
    "\n",
    "\n",
    "# for actions in all_actions:\n",
    "#     for action in actions:\n",
    "# #         print(action)\n",
    "#         if not action.endswith('.'):\n",
    "#             action += '.'\n",
    "\n",
    "# print(all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all actions terminate with periods (Note, make sure NULL action is not yet added to actions lists)\n",
    "# all_actions = [[action + '.' if not action.endswith('.') else action for action in actions] for actions in all_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 0\n",
    "# print(all_actions[idx])\n",
    "print(all_actions[idx][0])\n",
    "words = all_actions[idx][0].split()\n",
    "print(words)\n",
    "\n",
    "indices = [[1, 2, 3], [9, 10]]\n",
    "\n",
    "for ind in reversed(indices):\n",
    "    print(ind[0])\n",
    "    words.insert(ind[0], '[unused3]')\n",
    "\n",
    "all_actions[idx][0] = ' '.join(words)\n",
    "print(all_actions[idx][0])\n",
    "\n",
    "# sentence = 'add  garram masala seeds and a bay leaf to the oil.'\n",
    "# print(sentence)\n",
    "# words = sentence.split()\n",
    "# print(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_actions)\n",
    "\n",
    "steps = all_actions[0]\n",
    "steps1 = [step.strip() for step in steps]\n",
    "\n",
    "# print(steps)\n",
    "# print(steps1)\n",
    "for s in steps1:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities, indices = parse(steps, max_step_length=20)\n",
    "entities.append([NULL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(steps))\n",
    "print(len(entities))\n",
    "print(len(indices))\n",
    "\n",
    "print(steps)\n",
    "print(entities)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities.append(NULL)\n",
    "ent_len = [len(e) for e in entities]\n",
    "# ent_len.append(1)    # For appened NULL (alternatively, do entities.append(null))\n",
    "print(ent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1 = []\n",
    "steps1.append('Grill [unused3] the tomatoes in [unused3] a pan.')\n",
    "steps1.append('Add [unused3] oil into [unused3] the pan.')\n",
    "steps1.append('Add [unused3] oil into [unused3] the pan.')\n",
    "steps1.append('Cook [unused3] the bacon.')\n",
    "steps1.append('Spread [unused3] some mayonnaise onto [unused3] the bread.')\n",
    "steps1.append('Place [unused3] a piece of [unused3] lettuce onto [unused3] it.')\n",
    "steps1.append('Place [unused3] the tomatoes over [unused3] it.')\n",
    "steps1.append('Sprinkle [unused3] some salt and pepper onto [unused3] it.')\n",
    "steps1.append('Place [unused3] the bacon at [unused3] the top.')\n",
    "steps1.append('Place the [unused3] piece of bread at the top.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL = '[unused1]'\n",
    "PAD = '[unused2]'\n",
    "ENTITY = '[unused3]'\n",
    "ACTION = '[SEP]'\n",
    "\n",
    "MAX_STEP_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "\n",
    "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\", pad_token=PAD)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "tokenizer.encode([NULL, PAD, ENTITY], add_special_tokens=True)\n",
    "\n",
    "tokens_steps1 = tokenizer(\n",
    "                    steps1,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_steps1['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1_flat = tokens_steps1['input_ids'].flatten()\n",
    "# print(steps1_flat)\n",
    "steps1_flat = steps1_flat[steps1_flat != 101]\n",
    "# print(steps1_flat)\n",
    "steps1_flat = tokenizer.decode(steps1_flat) + ' ' + NULL\n",
    "print(steps1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = 'hello'\n",
    "qwer = 'world'\n",
    "print(type([asdf,qwer]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steps1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokens_steps1['input_ids'])\n",
    "\n",
    "steps1_flat = tokens_steps1['input_ids'].flatten()\n",
    "print(steps1_flat)\n",
    "steps1_flat = steps1_flat[steps1_flat != 101]\n",
    "print(steps1_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = read_json(os.path.join('/h/mkhan/ece496-capstone/datasets', 'annotations', 'ycii_annotations_trainval.json'))['database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = annotations['fn9anlEL4FI']['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for item in annotations:\n",
    "    if annotations[item]['subset']=='validation':\n",
    "        print(item)\n",
    "#         print(annotations[item])\n",
    "        print(annotations[item]['duration'])\n",
    "#         print(annotations[item]['annotations'])\n",
    "        segments = annotations[item]['annotations']\n",
    "        for segment in segments:\n",
    "            print(segment)\n",
    "#             start = segment['segment'][0]\n",
    "            end = segment['segment'][1]\n",
    "#             print(str(start) + \" \" + str(end))\n",
    "        \n",
    "        print(end)\n",
    "\n",
    "        i += 1\n",
    "        if i==3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
