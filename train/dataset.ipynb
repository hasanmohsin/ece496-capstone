{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from detector import Detector\n",
    "from parser import parse\n",
    "from transformers import LxmertModel, LxmertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path='output.json'):\n",
    "    \"\"\"\n",
    "    Check for valid JSON format and read content\n",
    "    path: path to JSON file\n",
    "    \"\"\"\n",
    "    file = open(path)\n",
    "    line = file.read().replace('\\n', ' ')\n",
    "    file.close()\n",
    "    try:\n",
    "        parsed_json = json.loads(line)\n",
    "    except:\n",
    "        assert False, 'Invalid JSON'\n",
    "    return parsed_json\n",
    "\n",
    "def get_vid_ext(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Returns video file extension\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    vid_prefix = os.path.join(video_dir, vid_id)\n",
    "    if os.path.exists(vid_prefix+'.mp4'):\n",
    "        return '.mp4'\n",
    "    elif os.path.exists(vid_prefix+'.mkv'):\n",
    "        return '.mkv'\n",
    "    elif os.path.exists(vid_prefix+'.webm'):\n",
    "        return '.webm'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Download video\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    # download the video\n",
    "    vid_url = 'www.youtube.com/watch?v='+vid_id\n",
    "    vid_prefix = os.path.join(video_dir, vid_id) \n",
    "    os.system(' '.join((\"youtube-dl -o\", vid_prefix, vid_url)))\n",
    "\n",
    "\n",
    "def sample_frames(vid_id, video_dir, frame_dir, fps=5):\n",
    "    \"\"\"\n",
    "    Sample video into frames at fixed fps\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    frame_dir: directory path to video frames\n",
    "    fps: fps for frame extraction\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(os.path.join(frame_dir, vid_id)):\n",
    "        os.mkdir(os.path.join(frame_dir, vid_id))\n",
    "    vid_ext = get_vid_ext(vid_id, video_dir)\n",
    "    ff_command = 'ffmpeg -i {}/{}{} -y -an -qscale 0 -vf fps={} {}/{}/%06d.jpg'.format(video_dir, vid_id, vid_ext, fps, frame_dir, vid_id)\n",
    "    os.system(ff_command)\n",
    "\n",
    "\n",
    "def remove_video(vid_id, video_dir):\n",
    "    \"\"\"\n",
    "    Delete video\n",
    "    vid_id: video id\n",
    "    video_dir: directory path to video files\n",
    "    \"\"\"\n",
    "    vid_prefix = os.path.join(video_dir, vid_id)\n",
    "    vid_ext = get_vid_ext(vid_id, video_dir)\n",
    "    os.remove(vid_prefix+vid_ext)\n",
    "\n",
    "\n",
    "def select_frames(actions, vid_id, num_frames_per_step):\n",
    "    \"\"\"\n",
    "    Returns representative frames for actions\n",
    "    actions: list of action annotations from YCII annotations\n",
    "    vid_id: video id\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    Returns required_frames: set contataining names of representative frames\n",
    "    \"\"\"\n",
    "    required_frames = set()\n",
    "    for action in actions:\n",
    "        action_start = action['segment'][0]\n",
    "        action_end = action['segment'][1]\n",
    "        action_delta = (action_end - action_start) / (num_frames_per_step + 1)    # need num_frames_per_step+1 intervals for num_frames_per_step inner frames\n",
    "        for i in range(num_frames_per_step):\n",
    "            frame_time = action_start + action_delta * (i+1)    # in seconds\n",
    "            frame_id = int( frame_time*(num_frames_per_step + 1) )\n",
    "            frame_name = '{}.jpg'.format(str(frame_id).zfill(6))\n",
    "            required_frames.add(frame_name)\n",
    "    return required_frames\n",
    "\n",
    "\n",
    "def remove_frames(vid_id, frame_dir, required_frames):\n",
    "    \"\"\"\n",
    "    Remove unused frames\n",
    "    vid_id: video id\n",
    "    frame_dir: directory path to video frames\n",
    "    required_frames: set contataining names of representative frames\n",
    "    \"\"\"\n",
    "    if os.path.isdir(os.path.join(frame_dir, vid_id)):\n",
    "        curr_frames = os.listdir(os.path.join(frame_dir, vid_id))\n",
    "        for frame in curr_frames:\n",
    "            if frame not in required_frames:\n",
    "                os.remove(os.path.join(frame_dir, vid_id, frame))\n",
    "\n",
    "\n",
    "def get_actions(actions):\n",
    "    \"\"\"\n",
    "    Returns list of actions text for video\n",
    "    actions: list of action annotations from YCII annotations\n",
    "    Returns actions_text: list of actions text for video\n",
    "    \"\"\"\n",
    "    actions_text = []\n",
    "    for action in actions:\n",
    "        actions_text.append(action['sentence'])\n",
    "    return actions_text\n",
    "\n",
    "\n",
    "def pickle_data(data, pickles_dir, vid_id, fname):\n",
    "    \"\"\"\n",
    "    Pickle data into bytestreams\n",
    "    data: data to be pickled\n",
    "    pickles_dir: directory path to pickled data\n",
    "    vid_id: video id\n",
    "    fname: name of pickled file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(os.path.join(pickles_dir, vid_id)):\n",
    "        os.mkdir(os.path.join(pickles_dir, vid_id))\n",
    "    pickle_out = open(os.path.join(pickles_dir, vid_id, fname+'.pickle'), 'wb')\n",
    "    pickle.dump(data, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "\n",
    "def depickle_data(pickles_dir, vid_id, fname):\n",
    "    \"\"\"\n",
    "    Depickle data from bytestreams\n",
    "    pickles_dir: directory path to pickled data\n",
    "    vid_id: video id\n",
    "    fname: name of pickled file\n",
    "    \"\"\"\n",
    "    pickle_path = os.path.join(pickles_dir, vid_id, fname+'.pickle')\n",
    "    if os.path.exists(pickle_path):\n",
    "        pickle_in = open(pickle_path, 'rb')\n",
    "        candidates = pickle.load(pickle_in)\n",
    "        return candidates\n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets', num_frames_per_step=4):\n",
    "    \"\"\"\n",
    "    Download and prepare dataset files\n",
    "    dataset_root: directory path to dataset base\n",
    "    num_frames_per_step: number of frames per action step\n",
    "    \"\"\"\n",
    "\n",
    "    annotations = read_json(os.path.join(dataset_root, 'annotations', 'ycii_annotations_trainval.json'))['database']\n",
    "    \n",
    "    videos_root = os.path.join(dataset_root, 'ycii_videos')\n",
    "    if not os.path.isdir(videos_root):\n",
    "        os.mkdir(videos_root)\n",
    "    frames_root = os.path.join(dataset_root, 'ycii_frames')\n",
    "    if not os.path.isdir(frames_root):\n",
    "        os.mkdir(frames_root)\n",
    "    pickles_root = os.path.join(dataset_root, 'ycii_pickles')\n",
    "    if not os.path.isdir(pickles_root):\n",
    "        os.mkdir(pickles_root)\n",
    "\n",
    "    missing_vid_list = []\n",
    "\n",
    "    detector = Detector()\n",
    "    \n",
    "    with open(os.path.join(dataset_root, 'vid_list', 'vid_list_ycii_val_short.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rcp_type,vid_id = line.replace('\\n','').split('/')\n",
    "            print('[INFO] Processing video {}'.format(vid_id))\n",
    "            \n",
    "            # download the video\n",
    "#             download_video(vid_id, videos_root)\n",
    "            vid_url = 'www.youtube.com/watch?v='+vid_id\n",
    "            vid_prefix = os.path.join(videos_root, vid_id) \n",
    "            os.system(' '.join((\"youtube-dl -o\", vid_prefix, vid_url)))\n",
    "\n",
    "            # check if the video is available\n",
    "            if os.path.exists(vid_prefix+'.mp4') or os.path.exists(vid_prefix+'.mkv') or os.path.exists(vid_prefix+'.webm'):\n",
    "                print('[INFO] Downloaded video {}'.format(vid_id))\n",
    "            else:\n",
    "                missing_vid_list.append(line)\n",
    "                print('[INFO] Cannot download video {}'.format(vid_id))\n",
    "                continue\n",
    "\n",
    "            # sample frames at fixed fps\n",
    "            sample_frames(vid_id, videos_root, frames_root, fps=5)\n",
    "            print('[INFO] Sampled frames for video {}'.format(vid_id))\n",
    "\n",
    "            # remove sampled video file (optional)\n",
    "            remove_video(vid_id, videos_root)\n",
    "            print('[INFO] Removed video {}'.format(vid_id))\n",
    "            \n",
    "            # select representative frames for actions\n",
    "            actions = annotations[vid_id]['annotations']\n",
    "            selected_frames = select_frames(actions, vid_id, num_frames_per_step)\n",
    "            print('[INFO] Selected frames for video {}'.format(vid_id))\n",
    "\n",
    "            # remove unsued frames\n",
    "            remove_frames(vid_id, frames_root, selected_frames)\n",
    "            print('[INFO] Removed unused frames for video {}'.format(vid_id))\n",
    "\n",
    "            # get candidates for images\n",
    "            frames = sorted(glob.glob(os.path.join(frames_root, vid_id, '*.*')))\n",
    "            candidates = [detector.inference(frame, max_detections=5) for frame in frames]\n",
    "            print('[INFO] Extracted candidates for video {}'.format(vid_id))\n",
    "\n",
    "            # save pickeled files for candidates\n",
    "            pickle_data(candidates, pickles_root, vid_id, 'candidates')\n",
    "            print('[INFO] Saved candidates for video {}'.format(vid_id))\n",
    "            \n",
    "            # get annotations list\n",
    "            actions_list = get_actions(actions)\n",
    "            print('[INFO] Extracted actions for video {}'.format(vid_id))\n",
    "            \n",
    "            # save pickled files for annotations list\n",
    "            pickle_data(actions_list, pickles_root, vid_id, 'actions')\n",
    "            print('[INFO] Saved candidates for video {}'.format(vid_id))\n",
    "\n",
    "\n",
    "    # write the missing videos to file\n",
    "    missing_vid = open(os.path.join(dataset_root, 'vid_list', 'missing_videos.txt'), 'w')\n",
    "    for line in missing_vid_list:\n",
    "        missing_vid.write(line)\n",
    "\n",
    "    # sanitize and remove the intermediate files\n",
    "    # os.system(\"find {} -name '*.part*' -delete\".format(dataset_root))\n",
    "    os.system(\"find {} -name '*.f*' -delete\".format(dataset_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets'):\n",
    "    \"\"\"\n",
    "    Load dataset values from saved files\n",
    "    dataset_root: directory path to dataset base\n",
    "    \"\"\"\n",
    "    pickles_root = os.path.join(dataset_root, 'ycii_pickles')\n",
    "    \n",
    "    all_candidates = []\n",
    "    all_actions = []\n",
    "    with open(os.path.join(dataset_root, 'vid_list', 'vid_list_ycii_val_short.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rcp_type,vid_id = line.replace('\\n','').split('/')\n",
    "            print('[INFO] Loading data for video {}'.format(vid_id))\n",
    "            \n",
    "            # load candidates data\n",
    "            candidates = depickle_data(pickles_root, vid_id, 'candidates')\n",
    "            if candidates:\n",
    "                all_candidates.extend(candidates)\n",
    "                print('[INFO] Loaded candidates for video {}'.format(vid_id))\n",
    "            else:\n",
    "                print('[INFO] Cannot load candidates for video {}'.format(vid_id))\n",
    "\n",
    "            # load actions data\n",
    "            actions = depickle_data(pickles_root, vid_id, 'actions')\n",
    "            if actions:\n",
    "                all_actions.append(actions)\n",
    "                print('[INFO] Loaded actions for video {}'.format(vid_id))\n",
    "            else:\n",
    "                print('[INFO] Cannot load actions for video {}'.format(vid_id))\n",
    "\n",
    "    return all_candidates, all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /h/mkhan/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n",
      "[INFO] Processing video sdB8qBlLS2E\n",
      "[INFO] Cannot download video sdB8qBlLS2E\n",
      "[INFO] Processing video fn9anlEL4FI\n",
      "[INFO] Downloaded video fn9anlEL4FI\n",
      "[INFO] Sampled frames for video fn9anlEL4FI\n",
      "[INFO] Removed video fn9anlEL4FI\n",
      "[INFO] Selected frames for video fn9anlEL4FI\n",
      "[INFO] Removed unused frames for video fn9anlEL4FI\n",
      "[INFO] Extracted candidates for video fn9anlEL4FI\n",
      "[INFO] Saved candidates for video fn9anlEL4FI\n",
      "[INFO] Extracted actions for video fn9anlEL4FI\n",
      "[INFO] Saved candidates for video fn9anlEL4FI\n",
      "[INFO] Processing video RnSl1LVrItI\n",
      "[INFO] Downloaded video RnSl1LVrItI\n",
      "[INFO] Sampled frames for video RnSl1LVrItI\n",
      "[INFO] Removed video RnSl1LVrItI\n",
      "[INFO] Selected frames for video RnSl1LVrItI\n",
      "[INFO] Removed unused frames for video RnSl1LVrItI\n",
      "[INFO] Extracted candidates for video RnSl1LVrItI\n",
      "[INFO] Saved candidates for video RnSl1LVrItI\n",
      "[INFO] Extracted actions for video RnSl1LVrItI\n",
      "[INFO] Saved candidates for video RnSl1LVrItI\n",
      "[INFO] Processing video vVZsj1t9R70\n",
      "[INFO] Downloaded video vVZsj1t9R70\n",
      "[INFO] Sampled frames for video vVZsj1t9R70\n",
      "[INFO] Removed video vVZsj1t9R70\n",
      "[INFO] Selected frames for video vVZsj1t9R70\n",
      "[INFO] Removed unused frames for video vVZsj1t9R70\n",
      "[INFO] Extracted candidates for video vVZsj1t9R70\n",
      "[INFO] Saved candidates for video vVZsj1t9R70\n",
      "[INFO] Extracted actions for video vVZsj1t9R70\n",
      "[INFO] Saved candidates for video vVZsj1t9R70\n"
     ]
    }
   ],
   "source": [
    "# USAGE: Run this just once to prepare and save data on disk\n",
    "prepare_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets', num_frames_per_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data for video sdB8qBlLS2E\n",
      "[INFO] Cannot load candidates for video sdB8qBlLS2E\n",
      "[INFO] Cannot load actions for video sdB8qBlLS2E\n",
      "[INFO] Loading data for video fn9anlEL4FI\n",
      "[INFO] Loaded candidates for video fn9anlEL4FI\n",
      "[INFO] Loaded actions for video fn9anlEL4FI\n",
      "[INFO] Loading data for video RnSl1LVrItI\n",
      "[INFO] Loaded candidates for video RnSl1LVrItI\n",
      "[INFO] Loaded actions for video RnSl1LVrItI\n",
      "[INFO] Loading data for video vVZsj1t9R70\n",
      "[INFO] Loaded candidates for video vVZsj1t9R70\n",
      "[INFO] Loaded actions for video vVZsj1t9R70\n"
     ]
    }
   ],
   "source": [
    "# Required Cell\n",
    "\n",
    "# USAGE: Run this to load all candidate and actions data from disk\n",
    "all_candidates, all_actions = load_dataset(dataset_root='/h/mkhan/ece496-capstone/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Cell\n",
    "# Handles building steps and entity_count\n",
    "\n",
    "\n",
    "# for actions in all_actions:\n",
    "#     for i, action in enumerate(actions):\n",
    "#         print(str(i) + ': ' + action)\n",
    "\n",
    "\n",
    "# Strip all whitespaces and periods\n",
    "# Note, periods will be added back later; need to temporarily remove periods before passing into parser\n",
    "all_actions = [[action.strip('.') for action in actions] for actions in all_actions]\n",
    "    \n",
    "NULL = '[unused1]'\n",
    "PAD = '[unused2]'\n",
    "ENTITY = '[unused3]'\n",
    "ACTION = '[SEP]'\n",
    "\n",
    "# TODO: iterate through all annotations to find max_step_length\n",
    "MAX_STEP_LENGTH = 30\n",
    "\n",
    "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\", pad_token=PAD)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "tokenizer.encode([NULL, PAD, ENTITY], add_special_tokens=True)\n",
    "\n",
    "# TODO: iterate through all annotations to find max_step_length\n",
    "max_step_length = 20    \n",
    "\n",
    "entity_count = []\n",
    "steps = []\n",
    "\n",
    "for idx in range(len(all_actions)):\n",
    "    \n",
    "    entities, indices = parse(all_actions[idx], max_step_length=max_step_length)\n",
    "    entities.append([NULL])\n",
    "    entity_count.append([len(en) for en in entities])\n",
    "    \n",
    "    # insert in reverse so preceeding word indices can still be used for modified actions\n",
    "    for ind in reversed(indices):\n",
    "        action_idx = ind[0]//max_step_length\n",
    "        entity_idx = ind[0]%max_step_length\n",
    "        words = all_actions[idx][action_idx].split()\n",
    "        words.insert(entity_idx, ENTITY)\n",
    "        all_actions[idx][action_idx] = ' '.join(words)\n",
    "\n",
    "        all_actions[idx] = [action + '.' if not action.endswith('.') else action for action in all_actions[idx]]\n",
    "\n",
    "    tokens_steps = tokenizer(\n",
    "                    all_actions[idx],\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "    tokens_steps = tokens_steps['input_ids'].flatten()\n",
    "    tokens_steps = tokens_steps[tokens_steps != 101]\n",
    "    tokens_steps = tokenizer.decode(tokens_steps) + ' ' + NULL\n",
    "    steps.append(tokens_steps)\n",
    "\n",
    "\n",
    "# for actions in all_actions:\n",
    "#     for i, action in enumerate(actions):\n",
    "#         print(str(i) + ': ' + action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Cell\n",
    "# Handles building boxes and features\n",
    "\n",
    "\n",
    "# Reshape all of the candidate bounding box and feature tensors.\n",
    "boxes = torch.tensor([candidate[0].numpy() for candidate in all_candidates]).squeeze(1)\n",
    "features = torch.tensor([candidate[1].numpy() for candidate in all_candidates]).squeeze(1)\n",
    "\n",
    "boxes = boxes.flatten(start_dim=0, end_dim=1)\n",
    "features = features.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "boxes = torch.stack((boxes, boxes))\n",
    "features = torch.stack((features, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['add [unused3] garram masala seeds and a bay leaf to [unused3] the oil. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] the lamb to [unused3] the pot. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] garlic ginger paste and chopped onions to the pot. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] chili tumeric coriander cumin and salt. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] water to [unused3] the pot. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] potatos to [unused3] the pot. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] the tomatos to [unused3] the pot. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] chili to [unused3] the pot. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused1]', 'add [unused3] chicken ribs to [unused3] a bowl of [unused3] water and let it boil. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] pound [unused3] the shallots cut the chilli pepper cut the lemon grass into [unused3] slices and cut some galongo. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] remove [unused3] the foamy froth from [unused3] the boiling water. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] the shallots chilli pepper and lemon grass into [unused3] the bowl and cover the lid. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] some lemon grass fish sauce sugar salt chicken powder chilli flakes and stir it to combine. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] put [unused3] all the vegetable into [unused3] the bowl and cook them. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] cut [unused3] the cilantro and lime. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] transfer [unused3] the chicken soup into [unused3] a bowl sprinkle cilantro and add some lime juice. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused1]', 'pour [unused3] some boiling water over [unused3] the black fungus mushrooms. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] cut [unused3] the chicken breast into [unused3] thin slices and shred them into small pieces. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add soy sauce [unused3] corn starch and sesame oil and mix. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] cut [unused3] the mushroom into [unused3] little thin strips. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] chop [unused3] the tofu into [unused3] small strips. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] slice the bamboo shoots into [unused3] strips. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] boil [unused3] the chicken stock and add the chicken mushroom tofu and the bamboo shoots. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] soy sauce [unused3] rice vinegar sugar and white pepper. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] corn starch. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] eggs and stir. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] add [unused3] the chopped scallions. [SEP] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused2] [unused1]']\n"
     ]
    }
   ],
   "source": [
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5.2232e-01, 2.9430e-01, 3.0040e-02,  ..., 0.0000e+00,\n",
      "          4.8199e+00, 0.0000e+00],\n",
      "         [6.7331e-01, 0.0000e+00, 5.0316e-03,  ..., 2.4784e-01,\n",
      "          2.2119e+00, 1.4384e-02],\n",
      "         [7.0783e-02, 1.1483e-01, 0.0000e+00,  ..., 1.7114e-02,\n",
      "          2.2685e-01, 1.2317e-01],\n",
      "         ...,\n",
      "         [4.2445e-02, 4.1700e+00, 7.6074e-03,  ..., 7.0933e-02,\n",
      "          5.6186e+00, 7.6773e-02],\n",
      "         [3.9388e-02, 8.3052e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.6912e-02, 3.2373e+00],\n",
      "         [7.0633e-03, 3.3752e+00, 1.2944e-01,  ..., 9.5188e-03,\n",
      "          1.8473e+00, 2.0603e+00]],\n",
      "\n",
      "        [[5.2232e-01, 2.9430e-01, 3.0040e-02,  ..., 0.0000e+00,\n",
      "          4.8199e+00, 0.0000e+00],\n",
      "         [6.7331e-01, 0.0000e+00, 5.0316e-03,  ..., 2.4784e-01,\n",
      "          2.2119e+00, 1.4384e-02],\n",
      "         [7.0783e-02, 1.1483e-01, 0.0000e+00,  ..., 1.7114e-02,\n",
      "          2.2685e-01, 1.2317e-01],\n",
      "         ...,\n",
      "         [4.2445e-02, 4.1700e+00, 7.6074e-03,  ..., 7.0933e-02,\n",
      "          5.6186e+00, 7.6773e-02],\n",
      "         [3.9388e-02, 8.3052e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          4.6912e-02, 3.2373e+00],\n",
      "         [7.0633e-03, 3.3752e+00, 1.2944e-01,  ..., 9.5188e-03,\n",
      "          1.8473e+00, 2.0603e+00]]])\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4.9113e-04, 3.0941e-01, 2.9766e-02, 3.8976e-01],\n",
      "         [6.5312e-01, 1.2063e-03, 9.9088e-01, 4.3768e-01],\n",
      "         [6.6566e-01, 1.9197e-01, 9.9766e-01, 9.5529e-01],\n",
      "         ...,\n",
      "         [6.3899e-01, 0.0000e+00, 9.8486e-01, 1.7960e-01],\n",
      "         [2.3436e-01, 4.4089e-02, 5.2754e-01, 2.8973e-01],\n",
      "         [6.5031e-01, 2.2177e-03, 9.9128e-01, 3.5090e-01]],\n",
      "\n",
      "        [[4.9113e-04, 3.0941e-01, 2.9766e-02, 3.8976e-01],\n",
      "         [6.5312e-01, 1.2063e-03, 9.9088e-01, 4.3768e-01],\n",
      "         [6.6566e-01, 1.9197e-01, 9.9766e-01, 9.5529e-01],\n",
      "         ...,\n",
      "         [6.3899e-01, 0.0000e+00, 9.8486e-01, 1.7960e-01],\n",
      "         [2.3436e-01, 4.4089e-02, 5.2754e-01, 2.8973e-01],\n",
      "         [6.5031e-01, 2.2177e-03, 9.9128e-01, 3.5090e-01]]])\n"
     ]
    }
   ],
   "source": [
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2, 1, 1, 2, 2, 2, 2, 1], [3, 2, 2, 2, 1, 2, 1, 2, 1], [2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(entity_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Do not execute the cells below, they are rough code for testing stuff out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_actions)\n",
    "\n",
    "for actions in all_actions:\n",
    "    for action in actions:\n",
    "        print(action)\n",
    "# actions = all_actions[0]\n",
    "# print(actions)\n",
    "# actions = [action + '.' if not action.endswith('.') else action for action in actions]\n",
    "# print(actions)\n",
    "\n",
    "# print(all_actions)\n",
    "for actions in all_actions:\n",
    "    for action in actions:\n",
    "        print(action)\n",
    "\n",
    "\n",
    "# for actions in all_actions:\n",
    "#     for action in actions:\n",
    "# #         print(action)\n",
    "#         if not action.endswith('.'):\n",
    "#             action += '.'\n",
    "\n",
    "# print(all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all actions terminate with periods (Note, make sure NULL action is not yet added to actions lists)\n",
    "# all_actions = [[action + '.' if not action.endswith('.') else action for action in actions] for actions in all_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 0\n",
    "# print(all_actions[idx])\n",
    "print(all_actions[idx][0])\n",
    "words = all_actions[idx][0].split()\n",
    "print(words)\n",
    "\n",
    "indices = [[1, 2, 3], [9, 10]]\n",
    "\n",
    "for ind in reversed(indices):\n",
    "    print(ind[0])\n",
    "    words.insert(ind[0], '[unused3]')\n",
    "\n",
    "all_actions[idx][0] = ' '.join(words)\n",
    "print(all_actions[idx][0])\n",
    "\n",
    "# sentence = 'add  garram masala seeds and a bay leaf to the oil.'\n",
    "# print(sentence)\n",
    "# words = sentence.split()\n",
    "# print(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_actions)\n",
    "\n",
    "steps = all_actions[0]\n",
    "steps1 = [step.strip() for step in steps]\n",
    "\n",
    "# print(steps)\n",
    "# print(steps1)\n",
    "for s in steps1:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities, indices = parse(steps, max_step_length=20)\n",
    "entities.append([NULL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(steps))\n",
    "print(len(entities))\n",
    "print(len(indices))\n",
    "\n",
    "print(steps)\n",
    "print(entities)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities.append(NULL)\n",
    "ent_len = [len(e) for e in entities]\n",
    "# ent_len.append(1)    # For appened NULL (alternatively, do entities.append(null))\n",
    "print(ent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1 = []\n",
    "steps1.append('Grill [unused3] the tomatoes in [unused3] a pan.')\n",
    "steps1.append('Add [unused3] oil into [unused3] the pan.')\n",
    "steps1.append('Add [unused3] oil into [unused3] the pan.')\n",
    "steps1.append('Cook [unused3] the bacon.')\n",
    "steps1.append('Spread [unused3] some mayonnaise onto [unused3] the bread.')\n",
    "steps1.append('Place [unused3] a piece of [unused3] lettuce onto [unused3] it.')\n",
    "steps1.append('Place [unused3] the tomatoes over [unused3] it.')\n",
    "steps1.append('Sprinkle [unused3] some salt and pepper onto [unused3] it.')\n",
    "steps1.append('Place [unused3] the bacon at [unused3] the top.')\n",
    "steps1.append('Place the [unused3] piece of bread at the top.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL = '[unused1]'\n",
    "PAD = '[unused2]'\n",
    "ENTITY = '[unused3]'\n",
    "ACTION = '[SEP]'\n",
    "\n",
    "MAX_STEP_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LxmertModel, LxmertTokenizer\n",
    "\n",
    "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\", pad_token=PAD)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [NULL, PAD, ENTITY]})\n",
    "tokenizer.encode([NULL, PAD, ENTITY], add_special_tokens=True)\n",
    "\n",
    "tokens_steps1 = tokenizer(\n",
    "                    steps1,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False,\n",
    "                    add_special_tokens=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=MAX_STEP_LENGTH + 2,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_steps1['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps1_flat = tokens_steps1['input_ids'].flatten()\n",
    "# print(steps1_flat)\n",
    "steps1_flat = steps1_flat[steps1_flat != 101]\n",
    "# print(steps1_flat)\n",
    "steps1_flat = tokenizer.decode(steps1_flat) + ' ' + NULL\n",
    "print(steps1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = 'hello'\n",
    "qwer = 'world'\n",
    "print(type([asdf,qwer]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steps1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokens_steps1['input_ids'])\n",
    "\n",
    "steps1_flat = tokens_steps1['input_ids'].flatten()\n",
    "print(steps1_flat)\n",
    "steps1_flat = steps1_flat[steps1_flat != 101]\n",
    "print(steps1_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = read_json(os.path.join('/h/mkhan/ece496-capstone/datasets', 'annotations', 'ycii_annotations_trainval.json'))['database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = annotations['fn9anlEL4FI']['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for item in annotations:\n",
    "    if annotations[item]['subset']=='validation':\n",
    "        print(item)\n",
    "#         print(annotations[item])\n",
    "        print(annotations[item]['duration'])\n",
    "#         print(annotations[item]['annotations'])\n",
    "        segments = annotations[item]['annotations']\n",
    "        for segment in segments:\n",
    "            print(segment)\n",
    "#             start = segment['segment'][0]\n",
    "            end = segment['segment'][1]\n",
    "#             print(str(start) + \" \" + str(end))\n",
    "        \n",
    "        print(end)\n",
    "\n",
    "        i += 1\n",
    "        if i==3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
