{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzVJQqtFG9GY"
   },
   "outputs": [],
   "source": [
    "# Sample video is referred to on YouCookII dataset homepage (http://youcook2.eecs.umich.edu). Transcript taken from there and modified.\n",
    "#\n",
    "# Title: Super Quick BLT Sandwich | Breakfast Special | My Recipe Book By Tarika Singh.\n",
    "# Link: https://youtu.be/4eWzsx1vAi8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuHLbQpEF8dE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls ./graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI0CkHikJ418"
   },
   "outputs": [],
   "source": [
    "video = inference('./video.mp4', './transcript.vtt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oaRsC7TSqEB"
   },
   "outputs": [],
   "source": [
    "# Display details.\n",
    "display_video(video)\n",
    "\n",
    "# Generate json.\n",
    "video.generate_json('graph')\n",
    "\n",
    "# Visualize graph (PDF).\n",
    "file_to_viz('graph.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ACSjZzr7jkK"
   },
   "outputs": [],
   "source": [
    "!cat graph.json | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_RV9CcQqDQo"
   },
   "outputs": [],
   "source": [
    "!zip -r ./graph.zip ./graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3giNEgrC0HP"
   },
   "outputs": [],
   "source": [
    "def inference(video_path, transcript_path):\n",
    "  video = Video(video_path, transcript_path)\n",
    "  video.align()\n",
    "\n",
    "  rr = RR(video.steps)\n",
    "  rr.run()\n",
    "\n",
    "  # Generate frames.\n",
    "  video.generate_frames('graph')\n",
    "\n",
    "  #set bboxes for video entities\n",
    "  vg_inference(video)\n",
    "\n",
    "  return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VZt8r9YmN9u"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import make_grid  \n",
    "  \n",
    "def display_video(video):\n",
    "  # Merge all the frames into one image.\n",
    "  grid = make_grid(video.vframes_aligned)\n",
    "\n",
    "  # Plot the combined image.\n",
    "  plt.figure(figsize=(15, 15))\n",
    "  plt.imshow(grid.permute(1, 2, 0))\n",
    "\n",
    "  for step in video.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUxvQNfxiIAC"
   },
   "outputs": [],
   "source": [
    "import webvtt\n",
    "import json\n",
    "\n",
    "from torchvision.io import read_video\n",
    "\n",
    "def get_frame(start, end, vframes):\n",
    "  middle = datetime.today() + start + (end - start)\n",
    "  middle = int(middle.hour * 3600 + middle.minute * 60 + middle.second)\n",
    "  vframe = vframes[middle]\n",
    "  return vframe\n",
    "\n",
    "class Video:\n",
    "\n",
    "  def __init__(self, video_path, transcript_path):\n",
    "    # Get all of information from the video.\n",
    "    self.vframes, self.aframes, self.info = read_video(video_path, pts_unit='sec')\n",
    "\n",
    "    # Get the FPS. Note that sometimes the file may not contain metadata causing\n",
    "    # this to fail. Ensure that the video contains metadata!\n",
    "    self.fps = int(self.info.get('video_fps'))\n",
    "\n",
    "    if not self.fps:\n",
    "      raise Exception('Video {} does not contain required metadata.'.format(video_path))\n",
    "\n",
    "    # Change the axes from [T, H, W, C] -> [T, C, H, W].\n",
    "    self.vframes = self.vframes.permute(0, 3, 1, 2)\n",
    "\n",
    "    # Parse through the transcript.\n",
    "    self.captions = webvtt.read(transcript_path)\n",
    "    \n",
    "    # We haven't aligned the frames yet.\n",
    "    self.vframes_aligned = None\n",
    "\n",
    "  def downsample(self):\n",
    "    # Downsample by striding along the array.\n",
    "    self.vframes = self.vframes[::self.fps]\n",
    "\n",
    "  def align(self):\n",
    "    self.downsample()\n",
    "    self.steps = [Step(idx, caption, self.vframes) for idx, caption in enumerate(self.captions)]\n",
    "    self.vframes_aligned = [step.vframe for step in self.steps]\n",
    "\n",
    "  def generate_frames(self, path):\n",
    "    for step in self.steps:\n",
    "      step.generate_frames(path)\n",
    "\n",
    "  def generate_json(self, file):\n",
    "    info = [step.generate_json() for step in self.steps]\n",
    "    json.dump(info, open('{}.json'.format(file), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsdV4wwLpmMj"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "\n",
    "class Step:\n",
    "\n",
    "  time_format = '%H:%M:%S.%f'\n",
    "  print_format = '%H:%M:%S'\n",
    "  offset = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "  def __init__(self, idx, caption, vframes):\n",
    "    self.idx = idx\n",
    "    self.text = caption.text\n",
    "    self.start = datetime.strptime(caption.start, Step.time_format)\n",
    "    self.end = datetime.strptime(caption.end, Step.time_format)\n",
    "\n",
    "    self.set_frames(vframes)\n",
    "    self.set_frame()\n",
    "\n",
    "    # Set by the model.\n",
    "    self.DOBJ = []\n",
    "    self.PRED = None\n",
    "    self.PP = []\n",
    "\n",
    "    self.path = None\n",
    "\n",
    "  def generate_frames(self, path):\n",
    "    self.path = os.path.abspath('./{}/{}.png'.format(path, self.idx))\n",
    "    save_image(self.vframe.float()/255, self.path)\n",
    "\n",
    "  @staticmethod\n",
    "  def get_seconds(time):\n",
    "    return time.hour * 3600 + time.minute * 60 + time.second\n",
    "\n",
    "  def get_interval(self):\n",
    "    start_index = self.get_seconds(self.start)\n",
    "    end_index = self.get_seconds(self.start + (self.end - self.start))\n",
    "    return (start_index - 1), (end_index - 1)\n",
    "  \n",
    "  def set_frames(self, vframes):\n",
    "    start_index, end_index = self.get_interval()\n",
    "    self.vframes = vframes[start_index:(end_index + 1)]\n",
    "\n",
    "  def get_index(self):\n",
    "    index = int(len(self.vframes) / 2)\n",
    "    return index\n",
    "\n",
    "  def set_frame(self):\n",
    "    self.vframe = self.vframes[self.get_index()]\n",
    "\n",
    "  def __str__(self):\n",
    "    s = 'Action ID {}: {} ({} -> {})\\n'.format(self.idx, self.text, self.start.strftime(Step.print_format), self.end.strftime(Step.print_format))\n",
    "    s += 'Predicate: {}\\n'.format(self.PRED)\n",
    "\n",
    "    for DOBJ in self.DOBJ:\n",
    "      s += 'DOBJ: {} ({}), BB: {}\\n'.format(DOBJ.text, DOBJ.reference, DOBJ.bb)\n",
    "\n",
    "    for PP in self.PP:\n",
    "      s += 'PP: {} ({}), BB: {}\\n'.format(PP.text, PP.reference, PP.bb)\n",
    "\n",
    "    return s\n",
    "\n",
    "  def generate_json(self):\n",
    "    attr = dict()\n",
    "    attr['annot'] = self.text\n",
    "    attr['img'] = self.path\n",
    "    attr['pred'] = self.PRED\n",
    "\n",
    "    attr['entities'] = []\n",
    "    attr['bboxes'] = []\n",
    "    attr['ea'] = []\n",
    "    attr['eb'] = []\n",
    "\n",
    "    for idx, entity in enumerate(self.DOBJ + self.PP):\n",
    "      attr['entities'].append(entity.text)\n",
    "      attr['bboxes'].append(entity.bb)\n",
    "      attr['ea'].append(entity.reference)\n",
    "      attr['eb'].append(-1 if not entity.bb else idx)\n",
    "\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqA0TYAzqOP1"
   },
   "outputs": [],
   "source": [
    "class Object:\n",
    "\n",
    "  def __init__(self, step, text, bb=None, reference=-1):\n",
    "    self.step = step\n",
    "    self.text = text\n",
    "    self.reference = reference\n",
    "    self.bb = bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B43gPrIiAnhT"
   },
   "source": [
    "## Reference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3189,
     "status": "ok",
     "timestamp": 1606957138869,
     "user": {
      "displayName": "Sagar Patel",
      "photoUrl": "",
      "userId": "08375359492343280284"
     },
     "user_tz": 300
    },
    "id": "xlWRkde587SE",
    "outputId": "e2afe85e-1928-4582-87f4-fcd3552e078e"
   },
   "outputs": [],
   "source": [
    "# Note that this is a temporary solution where we use the exiting RR module to\n",
    "# perform reference resolution. We simply copy the references, PRED, DOBJ and PP\n",
    "# from the output. In the future, a separate class for the Parser should be made\n",
    "# which only outputs the DOBJ, PP and PRED. This should be integrated into the\n",
    "# design above (easy). The RR will be done by the BERT model and will no longer\n",
    "# need the token-based approach from Neuralcoref.\n",
    "\n",
    "from importlib import reload\n",
    "from ref_res_model import ReferenceResolver\n",
    "\n",
    "class RR:\n",
    "\n",
    "  rr = ReferenceResolver()\n",
    "\n",
    "  def __init__(self, steps):\n",
    "    self.steps = steps\n",
    "    \n",
    "  def run(self):\n",
    "    resolved_steps = RR.rr.parse_and_resolve_all_refs([step.text for step in self.steps])\n",
    "    \n",
    "    for step, resolved_step in zip(self.steps, resolved_steps):\n",
    "      step.PRED = resolved_step.pred\n",
    "      step.DOBJ = [Object(step, str(DOBJ.ent_text), reference = DOBJ.act_id_ref) for DOBJ in resolved_step.dobj_list]\n",
    "      step.PP = [Object(step, str(PP.ent_text), reference = PP.act_id_ref) for PP in resolved_step.pp_list]              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Db08S2IAr1G"
   },
   "source": [
    "## Visual Grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WphEruCGA6w6"
   },
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGKdrnOeA_UN"
   },
   "source": [
    "\n",
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAxteqNQAyes"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8643,
     "status": "ok",
     "timestamp": 1606957466042,
     "user": {
      "displayName": "Sagar Patel",
      "photoUrl": "",
      "userId": "08375359492343280284"
     },
     "user_tz": 300
    },
    "id": "W05L92ilAtV9",
    "outputId": "f60a359b-584a-40b2-eb4a-5f8e3bca506c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from ipywidgets import widgets, Layout\n",
    "from io import BytesIO\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.layers import nms\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
    "\n",
    "\n",
    "from mmf.datasets.processors.processors import VocabProcessor, VQAAnswerProcessor\n",
    "from mmf.models.pythia import Pythia\n",
    "from mmf.common.registry import registry\n",
    "from mmf.common.sample import Sample, SampleList\n",
    "from mmf.utils.env import setup_imports\n",
    "from mmf.utils.configuration import Configuration\n",
    "\n",
    "setup_imports()\n",
    "\n",
    "class MMFDemo:\n",
    "  TARGET_IMAGE_SIZE = [448, 448]\n",
    "  CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
    "  CHANNEL_STD = [0.229, 0.224, 0.225]\n",
    "  \n",
    "  def __init__(self):\n",
    "    self._init_processors()\n",
    "    self.visual_bert = registry.get_model_class(\n",
    "            \"visual_bert\"\n",
    "        ).from_pretrained(\n",
    "            \"visual_bert.pretrained.coco\"\n",
    "        )\n",
    "\n",
    "    # Add this option so that it only output hidden states\n",
    "    self.visual_bert.model.output_hidden_states = True\n",
    "\n",
    "    self.visual_bert.model.to(\"cuda\")\n",
    "    self.visual_bert.model.eval()\n",
    "\n",
    "    # Add this option so that losses are not pushed into output\n",
    "    self.visual_bert.training_head_type = \"finetuning\"\n",
    "\n",
    "    self.detection_model = self._build_detection_model()\n",
    "    \n",
    "  def _init_processors(self):\n",
    "    args = Namespace()\n",
    "    args.opts = [\n",
    "        \"config=projects/pythia/configs/vqa2/defaults.yaml\",\n",
    "        \"datasets=vqa2\",\n",
    "        \"model=visual_bert\",\n",
    "        \"evaluation.predict=True\"\n",
    "    ]\n",
    "    args.config_override = None\n",
    "\n",
    "    configuration = Configuration(args=args)\n",
    "    \n",
    "    config = self.config = configuration.config\n",
    "    vqa2_config = config.dataset_config.vqa2\n",
    "    text_processor_config = vqa2_config.processors.text_processor\n",
    "    \n",
    "    text_processor_config.params.vocab.vocab_file = \"../model_data/vocabulary_100k.txt\"\n",
    "\n",
    "    # Add preprocessor as that will needed when we are getting questions from user\n",
    "    self.text_processor = VocabProcessor(text_processor_config.params)\n",
    "\n",
    "    registry.register(\"coco_text_processor\", self.text_processor)\n",
    "  \n",
    "\n",
    "  def _multi_gpu_state_to_single(self, state_dict):\n",
    "    new_sd = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if not k.startswith('module.'):\n",
    "            raise TypeError(\"Not a multiple GPU state of dict\")\n",
    "        k1 = k[7:]\n",
    "        new_sd[k1] = v\n",
    "    return new_sd\n",
    "  \n",
    "  def predict(self, url, text):\n",
    "    with torch.no_grad():\n",
    "      detectron_features = self.get_detectron_features(url)\n",
    "\n",
    "      sample = Sample()\n",
    "\n",
    "      processed_text = self.text_processor({\"text\": text})\n",
    "      #sample.text = processed_text[\"text\"]\n",
    "      sample.text_len = len(processed_text[\"tokens\"])\n",
    "\n",
    "      encoded_input = tokenizer(text, return_tensors='pt')\n",
    "      sample.input_ids = encoded_input.input_ids\n",
    "      sample.input_mask = encoded_input.attention_mask\n",
    "      sample.segment_ids = encoded_input.token_type_ids\n",
    "\n",
    "      sample.image_feature_0 = detectron_features\n",
    "      sample.image_info_0 = Sample({\n",
    "          \"max_features\": torch.tensor(100, dtype=torch.long)\n",
    "      })\n",
    "\n",
    "      sample_list = SampleList([sample])\n",
    "      sample_list = sample_list.to(\"cuda\")\n",
    "\n",
    "      output = self.visual_bert(sample_list)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return output\n",
    "    \n",
    "  \n",
    "  def _build_detection_model(self):\n",
    "\n",
    "      cfg.merge_from_file('../model_data/detectron_model.yaml')\n",
    "      cfg.freeze()\n",
    "\n",
    "      model = build_detection_model(cfg)\n",
    "      checkpoint = torch.load('../model_data/detectron_model.pth', \n",
    "                              map_location=torch.device(\"cpu\"))\n",
    "\n",
    "      load_state_dict(model, checkpoint.pop(\"model\"))\n",
    "\n",
    "      model.to(\"cuda\")\n",
    "      model.eval()\n",
    "      return model\n",
    "  \n",
    "  def get_actual_image(self, image_path):\n",
    "      if image_path.startswith('http'):\n",
    "          path = requests.get(image_path, stream=True).raw\n",
    "      else:\n",
    "          path = image_path\n",
    "      \n",
    "      return path\n",
    "\n",
    "  def _image_transform(self, image_path):\n",
    "      path = self.get_actual_image(image_path)\n",
    "\n",
    "      img = Image.open(path)\n",
    "      im = np.array(img).astype(np.float32)\n",
    "      im = im[:, :, ::-1]\n",
    "      im -= np.array([102.9801, 115.9465, 122.7717])\n",
    "      im_shape = im.shape\n",
    "      im_size_min = np.min(im_shape[0:2])\n",
    "      im_size_max = np.max(im_shape[0:2])\n",
    "      im_scale = float(800) / float(im_size_min)\n",
    "      # Prevent the biggest axis from being more than max_size\n",
    "      if np.round(im_scale * im_size_max) > 1333:\n",
    "           im_scale = float(1333) / float(im_size_max)\n",
    "      im = cv2.resize(\n",
    "           im,\n",
    "           None,\n",
    "           None,\n",
    "           fx=im_scale,\n",
    "           fy=im_scale,\n",
    "           interpolation=cv2.INTER_LINEAR\n",
    "       )\n",
    "      img = torch.from_numpy(im).permute(2, 0, 1)\n",
    "      return img, im_scale\n",
    "\n",
    "\n",
    "  def _process_feature_extraction(self, output,\n",
    "                                 im_scales,\n",
    "                                 feat_name='fc6',\n",
    "                                 conf_thresh=0.2):\n",
    "      batch_size = len(output[0][\"proposals\"])\n",
    "      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
    "      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "      feats = output[0][feat_name].split(n_boxes_per_image)\n",
    "      cur_device = score_list[0].device\n",
    "\n",
    "      feat_list = []\n",
    "\n",
    "      for i in range(batch_size):\n",
    "          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "          scores = score_list[i]\n",
    "\n",
    "          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "\n",
    "          for cls_ind in range(1, scores.shape[1]):\n",
    "              cls_scores = scores[:, cls_ind]\n",
    "              keep = nms(dets, cls_scores, 0.5)\n",
    "              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
    "                                           cls_scores[keep],\n",
    "                                           max_conf[keep])\n",
    "\n",
    "          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
    "          feat_list.append(feats[i][keep_boxes])\n",
    "      return feat_list\n",
    "\n",
    "  def masked_unk_softmax(self, x, dim, mask_idx):\n",
    "      x1 = F.softmax(x, dim=dim)\n",
    "      x1[:, mask_idx] = 0\n",
    "      x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
    "      y = x1 / x1_sum\n",
    "      return y\n",
    "   \n",
    "    \n",
    "  def get_detectron_features(self, image_path):\n",
    "      im, im_scale = self._image_transform(image_path)\n",
    "      img_tensor, im_scales = [im], [im_scale]\n",
    "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "      current_img_list = current_img_list.to('cuda')\n",
    "      with torch.no_grad():\n",
    "          output = self.detection_model(current_img_list)\n",
    "      feat_list = self._process_feature_extraction(output, im_scales, \n",
    "                                                  'fc6', 0.2)\n",
    "      return feat_list[0]\n",
    "\n",
    "  def get_detectron_features_and_out(self, image_path):\n",
    "      im, im_scale = self._image_transform(image_path)\n",
    "      img_tensor, im_scales = [im], [im_scale]\n",
    "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "      current_img_list = current_img_list.to('cuda')\n",
    "      with torch.no_grad():\n",
    "          output = self.detection_model(current_img_list)\n",
    "      feat_list = self._process_feature_extraction(output, im_scales, \n",
    "                                                  'fc6', 0.2)\n",
    "      return feat_list[0], output[0][\"proposals\"], im\n",
    "    \n",
    "\n",
    "  def get_detectron_features_and_out(self, image_path):\n",
    "      im, im_scale = self._image_transform(image_path)\n",
    "      img_tensor, im_scales = [im], [im_scale]\n",
    "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "      current_img_list = current_img_list.to('cuda')\n",
    "      with torch.no_grad():\n",
    "          output = self.detection_model(current_img_list)\n",
    "      feat_list = self._process_feature_extraction(output, im_scales, \n",
    "                                                  'fc6', 0.2)\n",
    "      return feat_list[0], output[0][\"proposals\"], im\n",
    "  \n",
    "  \n",
    "  def _process_bbox_extraction(self, output,\n",
    "                                 im_scales,\n",
    "                                 conf_thresh=0.2):\n",
    "      batch_size = len(output[0][\"proposals\"])\n",
    "      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
    "      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "      cur_device = score_list[0].device\n",
    "\n",
    "      bbox_list = []\n",
    "\n",
    "      for i in range(batch_size):\n",
    "          bboxes = output[0][\"proposals\"][i].bbox\n",
    "          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "          scores = score_list[i]\n",
    "\n",
    "          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "\n",
    "          for cls_ind in range(1, scores.shape[1]):\n",
    "              cls_scores = scores[:, cls_ind]\n",
    "              keep = nms(dets, cls_scores, 0.5)\n",
    "              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
    "                                           cls_scores[keep],\n",
    "                                           max_conf[keep])\n",
    "\n",
    "          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
    "          bbox_list.append(bboxes[keep_boxes])\n",
    "      return bbox_list\n",
    "\n",
    "  def detectron_get_bbox(self, image_path):\n",
    "        im, im_scale = self._image_transform(image_path)\n",
    "        img_tensor, im_scales = [im], [im_scale]\n",
    "        current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "        current_img_list = current_img_list.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            output = self.detection_model(current_img_list)\n",
    "        bbox_list = self._process_bbox_extraction(output, im_scales, 0.2)\n",
    "        return bbox_list[0]\n",
    "\n",
    "    #returns list of bounding box coordinates, in order of input entity_list\n",
    "    #bb_embed_list assumed to be 100 by 768 (the embedding size)\n",
    "  def visual_ground(self, output, entity_list, bbox_list, full_sentence, step_sentence):\n",
    "        \n",
    "        bbox_for_entity = []\n",
    "\n",
    "        encoded_input = tokenizer(full_sentence, return_tensors='pt')\n",
    "        step_encoded_input = tokenizer(step_sentence, return_tensors='pt')\n",
    "\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "        step_tokens = tokenizer.convert_ids_to_tokens(step_encoded_input[\"input_ids\"][0])\n",
    "\n",
    "        #find start of step_token list in all_tokens\n",
    "        start_step_ind = len(all_tokens) - len(step_tokens) +1\n",
    "        step_tokens = all_tokens[start_step_ind:]\n",
    "\n",
    "        bb_embed_list = output['sequence_output'][0, len(encoded_input[\"input_ids\"][0]):].cpu().detach().numpy()\n",
    "        text_embed_list = output['sequence_output'][0, 0:len(encoded_input[\"input_ids\"][0])].cpu().detach().numpy()\n",
    "\n",
    "        #move through the entities, we will form a meta-embeding for each entity\n",
    "        # by averaging the embeddings given by BERT (since an entity could be a longer string of many sub-word/tokens)\n",
    "        for entity in entity_list:\n",
    "            #print(\"\\n{}\".format(entity))\n",
    "            #run through sub-words that are in the step, if its a part of the entity, add it\n",
    "            related_embeddings=  [text_embed_list[start_step_ind + i] for i in range(len(step_tokens)) if check_subword_in_word(step_tokens[i], entity)]\n",
    "            \n",
    "            #print(len(related_embeddings))\n",
    "            related_embeddings = np.array(related_embeddings)\n",
    "            #print(len(related_embeddings))\n",
    "            #print(np.array(related_embeddings).shape)\n",
    "\n",
    "            \n",
    "            #average over them to get embedding for entity\n",
    "            entity_embedding = np.mean(related_embeddings, axis = 0)\n",
    "\n",
    "            #print(entity_embedding)\n",
    "\n",
    "            #now we have the entity embedding, compare to all bounding boxes to get scores for alignment\n",
    "            #print(bb_embed_list.shape)\n",
    "            #print(entity_embedding.shape)\n",
    "            scores = bb_embed_list@entity_embedding\n",
    "            bbox_ind = np.argmax(scores)\n",
    "            #scores = scores/np.sum(scores)\n",
    "\n",
    "            bbox_for_entity.append(bbox_list[bbox_ind].cpu().detach().numpy())\n",
    "        \n",
    "        return bbox_for_entity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIiS9j3EIIbk"
   },
   "outputs": [],
   "source": [
    "#pass subword (from BERT tokenizer) string, and word string\n",
    "#true if subword came from word\n",
    "def check_subword_in_word(subword, string):\n",
    "    #entity may be string of multiple words\n",
    "    words = string.split()\n",
    "\n",
    "    if (subword[:2] == \"##\" and any([subword[2:] in word for word in words])) or (subword[-2:] == \"##\" and any([subword[-2:] in word for word in words])) or (subword in words):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81702,
     "status": "ok",
     "timestamp": 1606957547752,
     "user": {
      "displayName": "Sagar Patel",
      "photoUrl": "",
      "userId": "08375359492343280284"
     },
     "user_tz": 300
    },
    "id": "sahjgn3VJq_s",
    "outputId": "97fec026-cb13-4364-c2eb-a6c55eaaeb7a"
   },
   "outputs": [],
   "source": [
    "demo = MMFDemo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyqL3ZEhJuCF"
   },
   "outputs": [],
   "source": [
    " def get_im_resize_factor(img_path): \n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    im = np.array(img).astype(np.float32)\n",
    "    im_shape = im.shape\n",
    "    im_size_min = np.min(im_shape[0:2])\n",
    "    im_size_max = np.max(im_shape[0:2])\n",
    "    im_scale = float(800) / float(im_size_min)\n",
    "    \n",
    "    # Prevent the biggest axis from being more than max_size\n",
    "    if np.round(im_scale * im_size_max) > 1333:\n",
    "        im_scale = float(1333) / float(im_size_max)\n",
    "    #im = cv2.resize(\n",
    "    #    im,\n",
    "    #    None,\n",
    "    #    None,\n",
    "    #    fx=im_scale,\n",
    "    #    fy=im_scale,\n",
    "    #    interpolation=cv2.INTER_LINEAR\n",
    "    #)\n",
    "\n",
    "    return im_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqkpAL3hvTgM"
   },
   "outputs": [],
   "source": [
    "#takes in video, and sets bb values for entities\n",
    "def vg_inference(video):\n",
    "    #demo = MMFDemo()\n",
    "\n",
    "    step_text = [step.text for step in video.steps]\n",
    "\n",
    "    #assuming all of same size\n",
    "    im_scale = get_im_resize_factor(video.steps[0].path)\n",
    "\n",
    "    for step_num in range(len(video.steps)):\n",
    "        full_sentence = \" \".join(step_text[:step_num+1])\n",
    "        sentence = video.steps[step_num].text\n",
    "\n",
    "        #construct entity list for step\n",
    "        entity_list = []\n",
    "\n",
    "        #add dobjs\n",
    "        entity_list.extend([video.steps[step_num].DOBJ[i].text for i in range(len(video.steps[step_num].DOBJ))])\n",
    "        #add pps\n",
    "        entity_list.extend([video.steps[step_num].PP[i].text for i in range(len(video.steps[step_num].PP))])\n",
    "\n",
    "\n",
    "        img_path = video.steps[step_num].path\n",
    "        output = demo.predict(img_path, full_sentence)\n",
    "\n",
    "        bbox_list = demo.detectron_get_bbox(img_path)\n",
    "\n",
    "        #list of bbox coordinates for scaled image\n",
    "        entity_bb = demo.visual_ground(output = output, \n",
    "                                entity_list = entity_list, \n",
    "                                bbox_list = bbox_list, \n",
    "                                full_sentence = full_sentence, \n",
    "                                step_sentence = sentence)\n",
    "        \n",
    "        for bb_ind in range(len(entity_bb)):\n",
    "            entity_bb[bb_ind] = np.round(entity_bb[bb_ind]/im_scale).astype(int)\n",
    "\n",
    "\n",
    "        #set bbox coordinates for all entities\n",
    "        count = 0\n",
    "        #re-order and put in dict for step\n",
    "        for dobj in video.steps[step_num].DOBJ:\n",
    "            dobj.bb = {'left': int(entity_bb[count][0]), 'top': int(entity_bb[count][3]), 'bot': int(entity_bb[count][1]), 'right': int(entity_bb[count][2])}\n",
    "            count += 1\n",
    "\n",
    "        for pp in video.steps[step_num].PP:\n",
    "            pp.bb = {'left': int(entity_bb[count][0]), 'top': int(entity_bb[count][3]), 'bot': int(entity_bb[count][1]), 'right': int(entity_bb[count][2])}\n",
    "            count+=1\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwy1Ow-t5iv6"
   },
   "outputs": [],
   "source": [
    "from graphviz import Graph, Digraph, nohtml\n",
    "import imagesize\n",
    "import json\n",
    "\n",
    "def read_json(path='output.json'):\n",
    "    file = open(path)\n",
    "    line = file.read().replace('\\n', ' ')\n",
    "    file.close()\n",
    "    try:\n",
    "        parsed_json = json.loads(line)\n",
    "    except:\n",
    "        assert False, 'Invalid JSON'\n",
    "    return parsed_json\n",
    "\n",
    "def file_to_viz(path='output.json'):\n",
    "    json_to_viz(read_json(path))\n",
    "\n",
    "def insert_newlines(string, length=32, lower=True):\n",
    "    lines = []\n",
    "    for i in range(0, len(string), length):\n",
    "        lines.append(string[i:i+length])\n",
    "    if lower:\n",
    "        # return ('\\n'.join(lines)).lower()     # required with html\n",
    "        return ('\\\\n'.join(lines)).lower()    # required with nohtml\n",
    "    # return '\\n'.join(lines)                   # required with html\n",
    "    return '\\\\n'.join(lines)                  # required with nohtml\n",
    "\n",
    "def get_scaled_center(img_center_x, img_center_y, img_width, img_height, img_scale, bb_left, bb_bot, bb_right, bb_top):\n",
    "    bb_center_x = img_center_x - img_width/2 + img_scale * (bb_left + (bb_right - bb_left) / 2)\n",
    "    bb_center_y = img_center_y - img_height/2 + img_scale * (bb_bot + (bb_top - bb_bot) / 2)\n",
    "    pos=str(bb_center_x) + \",\" + str(bb_center_y) + \"!\"\n",
    "    return pos\n",
    "\n",
    "# Required\n",
    "\n",
    "def json_to_viz(parsed_json):\n",
    "    # graph dimensions (adjust these values)\n",
    "    g_left = 0                  # left boundary of page\n",
    "    # g_bot = 0                 # bottom boundary of page\n",
    "    g_horizontal_gap = 1\n",
    "    g_vertical_gap = 1\n",
    "\n",
    "    g_txt_width = 5\n",
    "    g_txt_height = 0.5\n",
    "    g_txt_border = 0.125        # padding around each side\n",
    "\n",
    "    g_img_width = 4\n",
    "    g_img_height = 3\n",
    "\n",
    "    # g_inner_txt_boundary_color = 'invis'  # required for html\n",
    "    g_inner_txt_boundary_color = 'gray'\n",
    "    g_outer_txt_boundary_color = 'dimgray'\n",
    "    g_txt_font_color = 'midnightblue'\n",
    "\n",
    "    g_bb_color = 'limegreen'\n",
    "\n",
    "    g_edge_to_bb_color = 'limegreen'\n",
    "    g_edge_to_step_color = 'darkorange'\n",
    "\n",
    "    # resulting attributes\n",
    "    g_item_height = g_vertical_gap + max(g_txt_height+2*g_txt_border, g_img_height)\n",
    "\n",
    "    g_txt_center_x = g_left + g_img_width + g_horizontal_gap + g_txt_border + g_txt_width/2\n",
    "    g_img_center_x = g_left + g_img_width/2\n",
    "\n",
    "    g_outer_txt_width = g_txt_width + 2*g_txt_border\n",
    "    g_outer_txt_height = g_txt_height + 2*g_txt_border\n",
    "\n",
    "    # create graph\n",
    "    g = Digraph('text_core', engine='neato')\n",
    "\n",
    "    # set graph and node attributes\n",
    "    g.graph_attr['splines'] = 'curved'\n",
    "\n",
    "    g.node_attr['shape']='rect'\n",
    "    g.node_attr['fixedsize']='true'\n",
    "    g.node_attr['fontsize'] = '12'\n",
    "    g.node_attr['fontcolor'] = str(g_txt_font_color)\n",
    "    g.node_attr['imagescale'] = 'true'\n",
    "\n",
    "    step_count = len(parsed_json)\n",
    "\n",
    "    # draw nodes for images and texts\n",
    "    for i in range(step_count):\n",
    "        # formulate nohtml for text node\n",
    "        text = \"<f0>\" + insert_newlines(parsed_json[i]['pred'])\n",
    "        entity_count = len(parsed_json[i]['entities'])\n",
    "        for j in range(entity_count):\n",
    "            text += \"|<f\" + str(j+1) + \">\"\n",
    "            text += insert_newlines(parsed_json[i]['entities'][j])\n",
    "\n",
    "        # # formulate html for text node\n",
    "        # text = '''<<TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"1\"><TR><TD PORT=\"f0\">''' + insert_newlines(parsed_json[i]['pred']) + '''</TD>'''\n",
    "        # entity_count = len(parsed_json[i]['entities'])\n",
    "        # for j in range(entity_count):\n",
    "        #     text += '''<TD BGCOLOR=\"#EEFFDD\" PORT=\"f''' + str(j+1) + '''\">''' + insert_newlines(parsed_json[i]['entities'][j]) + '''</TD>'''\n",
    "        # text += '''</TR></TABLE>>'''\n",
    "\n",
    "        # draw text node\n",
    "        g.node(\"text_\"+str(i),text,pos=str(g_txt_center_x)+\",\"+str(i*g_item_height)+\"!\",width=str(g_txt_width),height=str(g_txt_height),color=str(g_inner_txt_boundary_color))\n",
    "        \n",
    "        g.node(\"text_shell_\"+str(i),'',pos=str(g_txt_center_x)+\",\"+str(i*g_item_height)+\"!\",width=str(g_outer_txt_width),height=str(g_outer_txt_height),color=str(g_outer_txt_boundary_color))\n",
    "        g.node(\"text_\"+str(i),nohtml(text),shape='Mrecord',pos=str(g_txt_center_x)+\",\"+str(i*g_item_height)+\"!\",width=str(g_txt_width),height=str(g_txt_height),color=str(g_inner_txt_boundary_color))    # required with nohtml\n",
    "        \n",
    "        # draw image node\n",
    "        g.node(\"img_\"+str(i),pos=str(g_img_center_x)+\",\"+str(i*g_item_height)+\"!\",width=str(g_img_width),height=str(g_img_height),label='',image=parsed_json[i]['img'],color='invis')\n",
    "        \n",
    "        # draw text-image pairing edge (needed for connected graph to manage overlapping)\n",
    "        g.edge(\"text_\"+str(i), \"img_\"+str(i), color='invis')\n",
    "\n",
    "    # draw edges from entity to action step\n",
    "    for i in range(step_count):\n",
    "        entity_count = len(parsed_json[i]['ea'])\n",
    "        for j in range(entity_count):\n",
    "            action_id_ref = parsed_json[i]['ea'][j]\n",
    "            if (action_id_ref != -1):\n",
    "                g.edge(\"text_\"+str(i)+\":f\"+str(j+1)+\":s\",\"text_shell_\"+str(action_id_ref)+\":n\",color=str(g_edge_to_step_color))\n",
    "\n",
    "    # draw bounding boxes\n",
    "    for i in range(step_count):\n",
    "        bb_count = len(parsed_json[i]['bboxes'])\n",
    "        width, height = imagesize.get(parsed_json[i]['img'])\n",
    "        w_ratio = g_img_width/width\n",
    "        h_ratio = g_img_height/height\n",
    "        min_ratio = min(w_ratio, h_ratio)\n",
    "\n",
    "        is_width_bounded = (w_ratio < h_ratio)      # bounded due to width\n",
    "        w=h_ratio*width if not is_width_bounded else g_img_width\n",
    "        h=w_ratio*height if is_width_bounded else g_img_height\n",
    "\n",
    "        for j in range(bb_count):\n",
    "            bb = parsed_json[i]['bboxes'][j]\n",
    "            g.node(\"bb_\"+str(i)+\"_\"+str(j),label='',pos=get_scaled_center(g_img_center_x,i*g_item_height,w,h,min_ratio,bb['left'],bb['bot'],bb['right'],bb['top']),width=str((bb['right']-bb['left'])*min_ratio),height=str((bb['top']-bb['bot'])*min_ratio),color=str(g_bb_color))\n",
    "\n",
    "    # draw edges from entity to bounding boxes\n",
    "    for i in range(step_count):\n",
    "        edge_orientation = \":s\" if (i < step_count/2) else \":n\"\n",
    "        entity_count = len(parsed_json[i]['eb'])\n",
    "        for j in range(entity_count):\n",
    "            bb_id_ref = parsed_json[i]['eb'][j]\n",
    "            if (bb_id_ref != -1):\n",
    "                g.edge(\"text_\"+str(i)+\":f\"+str(j+1)+edge_orientation,\"bb_\"+str(i)+\"_\"+str(bb_id_ref),color=str(g_edge_to_bb_color))\n",
    "                # g.edge(\"text_\"+str(i)+\":f\"+str(j+1)+\":n\",\"bb_\"+str(i)+\"_\"+str(bb_id_ref),color=str(g_edge_to_bb_color),n='1',pos=\"e,152.13,411.67 91.566,463.4 108.12,449.26 127.94,432.34 144.37,418.3\")\n",
    "\n",
    "    g.render('visualizer-output/graph', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91uFuUom5jYK"
   },
   "source": [
    "BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBxTI1OIL2OO"
   },
   "outputs": [],
   "source": [
    "step_text = [step.text for step in video.steps]\n",
    "\n",
    "#assuming all of same size\n",
    "im_scale = get_im_resize_factor(video.steps[0].path)\n",
    "\n",
    "for step_num in range(len(video.steps)):\n",
    "    full_sentence = \" \".join(step_text[:step_num+1])\n",
    "    sentence = video.steps[step_num].text\n",
    "\n",
    "    #construct entity list for step\n",
    "    entity_list = []\n",
    "\n",
    "    #add dobjs\n",
    "    entity_list.extend([video.steps[step_num].DOBJ[i].text for i in range(len(video.steps[step_num].DOBJ))])\n",
    "    #add pps\n",
    "    entity_list.extend([video.steps[step_num].PP[i].text for i in range(len(video.steps[step_num].PP))])\n",
    "\n",
    "\n",
    "    img_path = video.steps[step_num].path\n",
    "    output = demo.predict(img_path, full_sentence)\n",
    "\n",
    "    bbox_list = demo.detectron_get_bbox(img_path)\n",
    "\n",
    "    #list of bbox coordinates for scaled image\n",
    "    entity_bb = demo.visual_ground(output = output, \n",
    "                              entity_list = entity_list, \n",
    "                              bbox_list = bbox_list, \n",
    "                              full_sentence = full_sentence, \n",
    "                              step_sentence = sentence)\n",
    "    \n",
    "    for bb_ind in range(len(entity_bb)):\n",
    "        entity_bb[bb_ind] = np.round(entity_bb[bb_ind]/im_scale).astype(int)\n",
    "\n",
    "\n",
    "    #set bbox coordinates for all entities\n",
    "    count = 0\n",
    "    #re-order and put in dict for step\n",
    "    for dobj in video.steps[step_num].DOBJ:\n",
    "        dobj.bb = {'left': int(entity_bb[count][0]), 'top': int(entity_bb[count][3]), 'bot': int(entity_bb[count][1]), 'right': int(entity_bb[count][2])}\n",
    "        count += 1\n",
    "\n",
    "    for pp in video.steps[step_num].PP:\n",
    "        pp.bb = {'left': int(entity_bb[count][0]), 'top': int(entity_bb[count][3]), 'bot': int(entity_bb[count][1]), 'right': int(entity_bb[count][2])}\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1606949250730,
     "user": {
      "displayName": "Mohsin Hasan",
      "photoUrl": "",
      "userId": "15888221200962668520"
     },
     "user_tz": 300
    },
    "id": "392NvTUXn4PZ",
    "outputId": "ea4780f0-6375-400d-ae07-3d4a59cfabfd"
   },
   "outputs": [],
   "source": [
    "entity_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NX2WwxOdMo16"
   },
   "outputs": [],
   "source": [
    "    step_num = -1\n",
    "    #construct entity list for step\n",
    "    entity_list = []\n",
    "\n",
    "    #add dobjs\n",
    "    entity_list.extend([video.steps[step_num].DOBJ[i].text for i in range(len(video.steps[step_num].DOBJ))])\n",
    "    #add pps\n",
    "    entity_list.extend([video.steps[step_num].PP[i].text for i in range(len(video.steps[step_num].PP))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1606945915113,
     "user": {
      "displayName": "Mohsin Hasan",
      "photoUrl": "",
      "userId": "15888221200962668520"
     },
     "user_tz": 300
    },
    "id": "_rNGCFiLOs_j",
    "outputId": "3d18e27c-2b76-45e2-ee22-57416f9e2527"
   },
   "outputs": [],
   "source": [
    "entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1606947988858,
     "user": {
      "displayName": "Mohsin Hasan",
      "photoUrl": "",
      "userId": "15888221200962668520"
     },
     "user_tz": 300
    },
    "id": "6dvnAw_qOyBv",
    "outputId": "3cf130b9-827e-4a4c-f1bd-cb4320dd3000"
   },
   "outputs": [],
   "source": [
    "entity_list[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvW1wlUgfsTx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Inference.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
